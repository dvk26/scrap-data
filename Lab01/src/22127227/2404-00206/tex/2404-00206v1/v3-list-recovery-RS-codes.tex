	
% Author: VSI

\documentclass[11pt,dvipsnames]{extarticle}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{wasysym}
\usepackage[shortlabels]{enumitem}
\usepackage{xcolor}
\usepackage{tikz,tikz-cd}
\usepackage{geometry}
\usepackage{physics}
\usepackage[12hr]{datetime}
\usepackage{textcomp}
\usepackage[backref=page]{hyperref}
\usepackage{inputenc}
%\usepackage[color,notref,notcite]{showkeys}
\usepackage[nameinlink, noabbrev, capitalize]{cleveref}
\usepackage{appendix}
\usepackage{mathtools}
\usepackage{xfrac}
\usepackage{nicefrac}
\usepackage{url}
\usepackage{environ}
\usepackage{nicematrix}
\usepackage[algoruled]{algorithm2e}
\usepackage[normalem]{ulem}
\usepackage{float}
\usepackage{booktabs,multirow}

\hypersetup{
	colorlinks=true, urlcolor=NavyBlue, linkcolor=Mahogany, citecolor=ForestGreen, pdfborder={0,0,0},
}

\newcommand{\mb}{\mathbb}
\newcommand{\ms}{\mathscr}
\newcommand{\mc}{\mathcal}
\newcommand{\mf}{\mathfrak}
\newcommand{\tbf}{\textbf}
\newcommand{\tsf}{\textsf}
\newcommand{\mbf}{\mathbf}
\newcommand{\msf}{\mathsf}
\newcommand{\n}{\enspace}
\newcommand{\tx}{\text}
\newcommand{\ol}{\overline}
\newcommand{\ul}{\underline}
\newcommand{\bfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\newcommand{\supp}{\tx{supp}}
\newcommand{\base}{\tx{base}}
\newcommand{\spn}{\tx{span}}
\newcommand{\sgn}{\tx{sgn}}
\newcommand{\vst}{\vspace{0.2cm}}
\newcommand{\vsf}{\vspace{0.5cm}}
\newcommand{\vso}{\vspace{1cm}}
\newcommand{\en}{\enspace}
\newcommand{\ti}{\textit}
\newcommand{\pr}{\mb{P}}
\newcommand{\ex}{\mb{E}}
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\wgt}{\tx{\normalfont wt}}
\newcommand{\poly}{\tx{poly}}
\newcommand{\out}{\tx{out}}
\newcommand{\inn}{\tx{in}}
\newcommand{\lcm}{\tx{\normalfont lcm}}
\newcommand{\cl}{\tx{\normalfont cl}}
\newcommand{\zcl}{\tx{\normalfont Z-cl}}
\newcommand{\hcl}{\textrm{h-cl}}
\newcommand{\symcl}{\textrm{sym-cl}}
\newcommand{\wtcl}{\mathrm{wt-cl}}
\newcommand{\LJ}{\tx{\normalfont LJ}}
\newcommand{\LM}{\tx{\normalfont LM}}
\newcommand{\SM}{\tx{\normalfont SM}}
\newcommand{\under}{\underset}
\newcommand{\F}{\mathbb{F}}
\newcommand{\set}[1]{\left\{{#1}\right\}}
\newcommand{\B}{\set{0,1}}
\newcommand{\ord}{\tx{\normalfont ord}}
\newcommand{\Hilbert}{\tx{\normalfont H}}
\newcommand{\iref}[2]{(\hyperref[#2]{\ref*{#1}.\ref*{#2}})}
\newcommand{\QED}{$\hfill\square$}
\newcommand{\mQED}{\hfill\tag*{$\square$}}
\newcommand{\modulo}[1]{\,(\tx{\normalfont mod }#1)}
\newcommand{\HFa}{\tx{\normalfont HF_a}}
\newcommand{\certdeg}{\mathrm{certdeg}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\green}[1]{\textcolor{ForestGreen}{#1}}
\newcommand{\magenta}[1]{\textcolor{magenta}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\mult}{\mathrm{mult}}
\newcommand{\EHC}{\msf{EHC}}
\newcommand{\HC}{\msf{HC}}
\newcommand{\wdeg}{\mathrm{wdeg}}
\newcommand{\diag}{\tx{\normalfont diag}}
\newcommand{\XOR}{\tx{\normalfont XOR}}
\newcommand{\coeff}{\tx{\normalfont coeff}}
\newcommand{\email}[1]{\href{mailto:#1}{\textcolor{NavyBlue}{\texttt{#1}}}}
\newcommand{\RS}{{\normalfont\tsf{RS}}}
\newcommand{\FRS}{\tsf{FRS}}
\newcommand{\CRS}{\tsf{C-RS}}
\newcommand{\DRS}{\tsf{D-RS}}
\newcommand{\RRS}{\tsf{R-RS}}
\newcommand{\CRRS}{\tsf{CR-RS}}
\newcommand{\RM}{\tsf{RM}}
\newcommand{\FRM}{\tsf{FRM}}
\newcommand{\MFRM}{\tx{\normalfont MFRM}}
\newcommand{\PFRM}{\tx{\normalfont PFRM}}
\newcommand{\CRM}{\tsf{C-RM}}
\newcommand{\DRM}{\tsf{D-RM}}
\newcommand{\RRM}{\tsf{R-RM}}
\newcommand{\CRRM}{\tsf{CR-RM}}
\newcommand{\MC}{\tsf{MC}}
\newcommand{\Hyp}{\tsf{Hyp}}
\newcommand{\CHyp}{\tsf{C-Hyp}}
\newcommand{\DHyp}{\tsf{D-Hyp}}
\newcommand{\RHyp}{\tsf{R-Hyp}}
\newcommand{\CRHyp}{\tsf{CR-Hyp}}
\newcommand{\RIM}{\msf{RIM}}
\newcommand{\PI}{\mbf{PI}}
\newcommand{\transpose}{\msf{T}}

\renewcommand{\epsilon}{\varepsilon}
\newcommand{\eps}{\epsilon}

\mathchardef\mhyphen="2D

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{fact}[theorem]{Fact}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{statement}[theorem]{Statement}
\newtheorem*{claim*}{Claim}
\newtheorem{result}[theorem]{Result}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{example}[theorem]{Example}
\newtheorem{counterexample}[theorem]{Counterexample}
\newtheorem{question}[theorem]{Question}
\newtheorem{openquestion}[theorem]{Open Question}
\newtheorem{condition}[theorem]{Condition}

\theoremstyle{definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{constraint}[theorem]{Constraint}
\crefname{constraint}{Constraint}{Constraints}
\crefname{assumption}{Assumption}{Assumptions}

\newcommand{\dean}[1]{{\color{orange} Dean: #1}}
\newcommand{\venki}[1]{{\color{Magenta} Venki: #1}}

\makeatletter
\NewEnviron{restatethm}[2]{%
	% #1 (mandatory): a label
	\protected@write\@auxout{}{%
		\string\@restatetheorem{#1}{\detokenize\expandafter{\BODY}}%
	}%
	\begin{theorem}[#2]\label{#1}\BODY\end{theorem}%
}
\newcommand{\@restatetheorem}[2]{%
	\expandafter\gdef\cNsame restatethm@#1\endcNsame{#2}%
}
\newcommand{\restatethmnow}[2]{%
	\begingroup
	\renewcommand{\thetheorem}{\ref{#1}}%
	\begin{theorem}[#2]\cNsame restatethm@#1\endcNsame\end{theorem}%
	\endgroup
}
\makeatother

\newtheoremstyle{TheoremNum}
{\topsep}{\topsep}              %%% space between body and thm
{\itshape}                      %%% Thm body font
{}                              %%% Indent amount (empty = no indent)
{\bfseries}                     %%% Thm head font
{.}                             %%% Punctuation after thm head
{ }                             %%% Space after thm head
{\thmname{#1}\thmnote{ \bfseries #3}}%%% Thm head spec
\theoremstyle{TheoremNum}
\newtheorem{reptheorem}{Theorem}

\makeatletter
\newcommand{\leqnomode}{\tagsleft@true\let\veqno\@@leqno}
\newcommand{\reqnomode}{\tagsleft@false\let\veqno\@@leqno}
\makeatother

\geometry{margin=2.5cm}
%\setlength{\parindent}{0pt}
%\setlength{\parskip}{7.5pt}
\setcounter{section}{0}
\setcounter{MaxMatrixCols}{30}

\title{Random Reed--Solomon Codes Are List Recoverable With Optimal List Size}
\author{Dean Doron\thanks{Supported in part by NSF-BSF grant \#2022644.}\\Ben-Gurion University\\ \texttt{deand@bgu.ac.il}
	\and
	S. Venkitesh\thanks{Supported in part by BSF grant 2021683, ISF grant 735/20, and by the European Union (ERC, ECCC, 101076663).}\\University of Haifa\\ \texttt{venkitesh.mail@gmail.com}}
\date{}
\begin{document}
	
	\maketitle
	
	
	\begin{abstract}
		We prove that Reed--Solomon (RS) codes with random evaluation points are list recoverable up to capacity with optimal output list size, for any input list size. Namely, given an input list size $\ell$, a designated rate $R$, and any $\eps > 0$, we show that a random RS code is list recoverable from $1-R-\eps$ fraction of errors with output list size $L = O(\ell/\eps)$, for field size $q=\exp(\ell,1/\eps) \cdot n^2$. In particular, this shows that random RS codes are list recoverable \emph{beyond} the ``list recovery Johnson bound’’. Such a result was not even known for arbitrary random linear codes.
		Our technique follows and extends the recent line of work on list decoding of random RS codes, specifically the works of Brakensiek, Gopi, and Makam~\cite{brakensiek-gopi-makam-2023-random-RS}, and of Guo and Zhang~\cite{guo-zhang-2023-random-RS}. 
	\end{abstract}
	\thispagestyle{empty}
	\newpage
	%\tableofcontents
	
	\section{Introduction}\label{sec:intro}
	
	
	An error-correcting code is \emph{list recoverable} if it does not intersect small combinatorial rectangles too often. Formally, we say that $C \subseteq \mathbb{F}_q^n$ is $(\rho,\ell,L)$ list recoverable, if for any $S_1,\ldots,S_n \subseteq \mathbb{F}_q^n$, each $|S_i| \le \ell$, it holds that
	\[
	\left| \left\{ c \in C : \Pr_{i \sim [n]}[c_i \notin S_i] \le \rho \right\} \right| \le L.
	\]
	Initially, list recovery was used as a building block for list and uniquely decodable codes\footnote{Indeed, the setting of $\ell=1$ corresponds to \emph{list decoding} from $\rho$ fraction of adversarial errors.} \cite{guruswami-indyk-2002-linear-time-codes,guruswami-indyk-2003-linear-time-decodable-codes,guruswami-indyk-2004-efficiently-decodable-codes-GV,guruswami-indyk-2005-linear-time-encodable-decodable-codes,kopparty-meir-ron-zewi-saraf-2016-high-rate-LRC-LTC,gopi-kopparty-oliviera-ron-zewi-2018-LTC-LCC-GV,hemenway-ron-zewi-wootters-2020-local-list-recovery-tensor}.  However, list recovery is increasingly gaining independent interest, in part due to its applications in pseudorandomness, algorithm design, hashing, and cryptography. 
	In algorithms, for example, list-recovery
	has applications in areas such as heavy hitters, compressed sensing, and combinatorial group testing (see, e.g.,   \cite{indyk-ngo-rudra-2010,ngo-porat-rudra-2011-list-disjunct-matrices,ngo-porat-rudra-2012-compressed-sensing,gilbert-ngo-porat-rudra-2013-sparse-recovery,larsen-nelson-nguyen-thorup-2019-heavy-hitters,gilbert-li-porat-strauss-2017-for-all-sparse-recovery,doron-wootters-2022-list-recovery-heavy-hitters}).
	In pseudorandomness, list recoverable codes are related to seeded condensers (see \cite{guruswami-umans-vadhan-2009-unbalanced-expanders,doron-moshkovitz-oh-zuckerman-2022-pseudorandomness-hardness}), and were also utilized in the computational setting for constructing pseudorandom generators \cite{doron-moshkovitz-oh-zuckerman-2022-pseudorandomness-hardness}. 
	
	Given designated $\ell \in \mathbb{N}$ and $\rho > 0$,
	a (plain) random code over $\F_q$, of rate approaching $1-\rho$, is $(\rho,\ell,L = O\big(\frac{\ell}{1-\rho})\big)$ list recoverable with high probability, as long as $q$ is polynomial in $\frac{\ell}{1-\rho}$.
	Current explicit constructions fall short of getting these parameters in several aspects, and interestingly, random \emph{linear} codes are not known to achieve such a small output list size. Concretely, the work of Rudra and Wootters \cite{rudra-wootters-2018-list-recovery} shows that random linear codes of rate approaching $1-\rho$ are list recoverable with $L = \ell^{\,O(1)}$ (where we suppress the dependence on the ``distance to capacity'' for now).
	
	In this work, we study the list recoverability of randomly punctured Reed--Solomon codes, and show that they attain better parameters than \cite{rudra-wootters-2018-list-recovery}. 
	
	\paragraph{List recovery of Reed--Solomon codes.} Reed--Solomon (RS) codes~\cite{reed-solomon-1960} are the prototypical family of algebraic error-correcting codes, having widespread presence both in theory and in practice.  Consider a finite field \(\mb{F}_q\) with \(q\) elements, the univariate polynomial ring \(\mb{F}_q[X]\) in an indeterminate \(X\), and integer parameters \(1\le k\le n\le q\).  For distinct elements \(\alpha_1,\ldots,\alpha_n\in\mb{F}_q\), the \tsf{degree-\(k\) Reed--Solomon (RS) code} is a subset of \(\mb{F}_q^n\) defined by
	\[
	\RS(\alpha_1,\ldots,\alpha_n;k)=\{(f(\alpha_1),\ldots,f(\alpha_n)):f(X)\in\mb{F}_q[X],\,\deg(f)<k\}.
	\]
	This code is clearly an \(\mb{F}_q\)-linear space, and has rate \(R=\frac{k}{n}\).  By simple root counting, this code has (minimum relative Hamming) distance \(1-\frac{k-1}{n}\).  Thus, by the Singleton bound~\cite{joshi-1958-MDS,komamiya-1953-MDS,singleton-1964-MDS}, RS codes attain the optimal tradeoff between rate and distance.
	
	It is clear, by definition, that RS codes can be encoded efficiently.  In fact, classical algorithms for fast arithmetic~\cite{fiduccia-1971-fast-MM,borodin-moenck-1974-fast-univariate-evaluation} imply that RS codes can be encoded in nearly linear time.   However, the \emph{decodability} properties of RS codes are far from being well-understood, despite several decades of study, as well as applications in areas like complexity theory and pseudorandomness (e.g., \cite{cai-pavan-sivakumar-1999-hardness-permanent,sudan-trevisan-vadhan-2001-PRG,lund-potukuchi-2020-list-recovery-RS}).  
	
	The most basic question in the context of decodability is the \emph{unique decoding problem}, which is the task of finding the unique codeword (if it exists) within half the distance from any given received word.  This was solved for RS codes in the decade following their introduction~\cite{peterson,massey,welch-1983}.  
	
	A more general question is the aforementioned \emph{list decoding problem}, which is the task of outputting a list of codewords (provided the output list has a reasonably small size) within a specified decoding radius $\rho$ from any given received word.  The work of Berlekamp and Welch~\cite{welch-1983}, followed by the breakthroughs of Sudan~\cite{sudan-1997-RS}, and Guruswami and Sudan~\cite{guruswami-sudan-2001}, showed that this task can be efficiently performed for RS codes up to the \emph{Johnson bound}, which means radius \(\rho = 1-\sqrt{R}\) for a code of rate \(R\).  Further results on list decoding RS codes have been \emph{combinatorial} and not \emph{algorithmic}, that is, they establish limits on decoding radii for RS codes for which we can guarantee a small output list size, without giving us any algorithm to efficiently list decode (see, for instance,~\cite{guruswami-rudra-2005-limits-RS-decoding,ben-sasson-kopparty-radhakrishnan-2010-subspace-polynomials,rudra-wootters-2014-list-decoding,ferber-kwan-sauermann-2022-list-decoding-RS,shangguan-tamo-2023-list-decoding-RS-beyond-Johnson-bound,goldberg-shangguan-tamo-2023-list-decoding-RS-beyond-Johnson-all-rates,brakensiek-gopi-makam-2023-random-RS,guo-zhang-2023-random-RS,alrabiah-guruswami-li-2023-random-RS}).  Very recently, the works of Guo and Zhang~\cite{guo-zhang-2023-random-RS}, and Alrabiah, Guruswami, and Li~\cite{alrabiah-guruswami-li-2023-random-RS,alrabiah-guruswami-li-2024-AG-codes} have established nearly tight bounds on the alphabet size for randomly punctured RS codes achieving list decoding capacity.  We will elaborate on some of these results soon. 
	
	In this work, we are interested in the even more general question of \emph{list recovery of RS codes}.  We will discuss some results on list recovery of RS codes in~\cref{sec:results}.  Henceforth, we will concern ourselves only with linear codes.
	
	\subsection{List recovery bounds}\label{sec:intro-LR}
	
	Given an input tuple \((S_1,\ldots,S_n)\) of subsets of \(\mb{F}_q\), and a vector \(w=(w_1,\ldots,w_n)\in\mb{F}_q\), the \emph{relative Hamming distance} of \(w\) from the tuple \((S_1,\ldots,S_n)\) is said to be at most \(\rho\in[0,1]\), if the number of coordinates \(i\in[n]\) such that \(w_i\not\in S_i\) is at most \(\rho n\). Using this terminology,  a code \(C\subseteq\mb{F}_q^n\) is \emph{\((\rho,\ell,L)\) list recoverable (LR)} if there is no input tuple \((S_1,\ldots,S_n)\) of subsets of \(\mb{F}_q\), each having size at most \(\ell\), such that there are \(L+1\) codewords in \(C\) at a relative Hamming distance at most \(\rho\) from \((S_1,\ldots,S_n)\).  This is a generalization of list decodability: A code \(C\subseteq\mb{F}_q^n\) is said to be \emph{\((\rho,L)\) list decodable (LD)} if it is \((\rho,1,L)\)-LR.
	
	%	We will also be interested in another variant of list recovery, which we call \emph{\(\mc{L}_1\)-list recovery}.  A code \(C\subseteq\mb{F}_q^n\) is said to be \emph{\((\rho,\ell,L)\mhyphen\mc{L}_1\)-list recoverable (LR)} if there is no input tuple \((S_1,\ldots,S_n)\) of subsets of \(\mb{F}_q\), with \(\sum_{i=1}^n|S_i|\le\ell\), such that there are \(L+1\) codewords in \(C\) at a relative Hamming distance at most \(\rho\) from \((S_1,\ldots,S_n)\).
	
	The Johnson bound for list recovery~\cite{gopi-kopparty-oliviera-ron-zewi-2018-LTC-LCC-GV} implies that any \(\mb{F}_q\)-linear code having rate \(R\) is \(({1-\sqrt{\ell R}},\ell,L=O(\ell))\)-LR.  Expressed differently in terms of the \emph{gap to capacity} \(\eps\in(0,1)\), the Johnson bound for list recovery implies that any \(\mb{F}_q\)-linear code having rate \(\sim\eps^2/\ell\) is \((1-\eps,\ell,O(\ell))\)-LR.  Recently, Goldberg, Shangguan, and Tamo~\cite{goldberg-shangguan-tamo-2024-list-recovery-singleton-bound} gave an analogue of the \emph{Singelton} bound for list recovery.
	
	\begin{theorem}[{\cite[Theorem 5.1]{goldberg-shangguan-tamo-2024-list-recovery-singleton-bound}}]\label{thm:singleton}
		Let \(\ell\in[q],\,L\in[\ell,\ell q]\).  If an \(\mb{F}_q\)-linear code having rate \(R\) is \((\rho,\ell,L)\)-LR, then
		\[
		\rho\le\bigg(1-\frac{\ell}{L+1}\bigg)(1-R).
		\]
		In other words, if an \(\mb{F}_q\)-linear code having rate \(R\) is \((\rho,\ell,L)\)-LR, then
		\[
		L=\Omega\bigg(\frac{\ell}{1-\rho}\bigg).
		\]
	\end{theorem}
	
	\noindent An immediate corollary of~\cref{thm:singleton} is of interest to us.
	\begin{corollary}[{\cite[Corollary 5.4]{goldberg-shangguan-tamo-2024-list-recovery-singleton-bound}}]\label{cor:singleton}
		Let \(\ell\in[q],\,L\in[\ell,\ell q], R \in (0,1)\), and \(\eps\in(0,1-R)\).  If an \(\mb{F}_q\)-linear code \(C\) having rate \(R\) is \((1-R-\eps,\ell,L)\)-LR, then \(L=\Omega(\ell/\eps)\).
	\end{corollary}
	
	In this work, we will show that for randomly chosen evaluation points \(\alpha_1,\ldots,\alpha_n\in\mb{F}_q\), the corresponding Reed--Solomon code (of any rate) achieves the optimal list size in~\cref{cor:singleton} with high probability, if the field size \(q\) is sufficiently larger than \(n\).  Henceforth, we will refer to such RS codes corresponding to random choices of evaluation points, as \emph{random RS codes}.
	
	\subsection{Our results, and related work}\label{sec:results}
	
	To begin with a field-dependent negative result for list recovery of RS codes, we do know of \emph{explicit} RS codes having rate \(\sim1/\ell\) that are not list recoverable beyond the Johnson bound~\cite{guruswami-rudra-2005-limits-RS-decoding}.\footnote{Precisely,~\cite{guruswami-rudra-2005-limits-RS-decoding} prove the following: For any fixed prime power \(q\ge3\), and for the choices \(n=q^m\) (with \(m\) growing), \(k=\frac{q^m-1}{q-1}+1\), \(\ell=\lceil n/k\rceil=q\), and \(S_1=\cdots=S_n=\mb{F}_q\), the output list of polynomials \(f(X)\in\mb{F}_{q^m}[X]\) such that \(\deg(f)<k\) and \(f(x)\in\mb{F}_q\) for all \(x\in\mb{F}_{q^m}\), has size exactly \(q^{2^m}\ge(q^m)^{\omega(1)}\).  In fact, the output list is the set \(\big\{\sum_{t=0}^{2^m-1}c_t(X+\gamma^t)^{k-1}:c_0,\ldots,c_{2^m-1}\in\mb{F}_q\big\}\), where \(\mb{F}_{q^m}=\mb{F}_q(\gamma)\).}  In the positive direction, we note two recent important results.  
	\begin{itemize}
		\item
		Guo, Li, Shangguan, Tamo, and Wootters~\cite{guo-li-shangguan-2022-list-recovery-RS-tree-packings} showed that random RS codes having rate \(\sim\eps/\big(\sqrt{\ell}\cdot\log(1/\eps)\big)\) are \((1-\eps,\ell,O(\ell/\eps))\)-LR, over finite fields \(\mb{F}_q\) with \(q\sim(\ell/\eps)^{O(n/\eps)}\).  
		\item 
		Goldberg, Shangguan, and Tamo~\cite{goldberg-shangguan-tamo-2024-list-recovery-singleton-bound} showed that random RS codes having rate \(\sim\eps/\ell\) are \((1-\eps,\ell,O(\ell/\eps))\)-LR, over finite fields \(\mb{F}_q\) with \(q\sim n^{O(1/\eps)}\).  
	\end{itemize}
	On the one hand, in~\cite{goldberg-shangguan-tamo-2024-list-recovery-singleton-bound}, the rate does not go beyond the \(1/\ell\) barrier of the Johnson bound.  On the other hand, in~\cite{guo-li-shangguan-2022-list-recovery-RS-tree-packings}, the rate does go beyond the \(1/\ell\) barrier (with a worse dependence on \(\eps\)), but only slightly; an \(\ell^{\,-\Omega(1)}\) dependence in the rate is quite undesirable.   In particular, these results still leave open the question of whether there exist RS codes, having rate \((\log\ell)^{-O(1)}\), or even constant rate, that are \((1-\eps,\ell,q^{O(1)})\)-LR.
	
	Our main result is that for \emph{any rate}, random RS codes achieve the Singleton bound for list recovery (that is, achieve list recovery capacity), with optimal output list size, over fields of size polynomial in the length of the code, and exponential in the input list size. 
	\begin{theorem}[Main result]\label{thm:main-RS}
		For any positive integers \(n,\ell\), any small enough \(\eps>0\), and any rate \(R\in(0,1-\eps)\), the following holds for any finite field \(\mb{F}_q\) with \(q\ge\ell^{\,\Theta(\ell^2/R\eps^3)} \cdot n^2\).  With probability at least \(1-\ell^{\,-n}\) over a uniform and independent choice of \(\alpha_1,\ldots,\alpha_n\sim\mb{F}_q\), the code \(\RS(\alpha_1,\ldots,\alpha_n;Rn)\) is \[(1-R-\eps,\ell,L=O(\ell/\eps))\] list recoverable.
		In particular, the assertion holds with \(q\ge n^{\Theta(1/R\eps^3)}\) for \(\ell = O\big(\sqrt{\log n/\log\log n}\big)\).
	\end{theorem}
	% \dean{So can't we just merge the two items? And then maybe stress that: (1) for any constant $R$, the constant in the $O()$ is independent of $R$, and: (2) when $R=o(1)$, we can improve the rate to exactly $1-\eps$.}
	
	% \dean{I prefer the following phrasing throughout: For any positive integers $n,\ell$, and any small enough constant $\eps > 0$, the following holds, for any finite field $\mb{F}_q$ with $q \ge ...$.}
	
	% \dean{For any $R \in (0,1)$ that satisfies $R=o_{n,\ell}(1)$, with probability at least $1-o(1)$ over a uniform and independent choice of $\alpha_1,\ldots,\alpha_n \sim \mb{F}_q$, the code ... is ...}
	
	% \dean{Above, the $o(1)$ in the success probability behaves like ...}
	
	% \venki{All done.  Removed informal version; there is only one version now for the main theorem.  Also, the statement is now for \emph{any rate} \(R\), including the case \(R=1-o(1)\).}
	
	%	By rerunning the same analysis as in the proof of~\cref{thm:main-RS} (with suitable minor modifications), we have the following result in the \(\mc{L}_1\)-model.
	%	\begin{theorem}\label{thm:main-RS-L1}
		%		For any positive integers \(n,\ell\), and any small enough \(\eps>0\), the following holds for any finite field \(\mb{F}_q\) with \(q\ge\binom{n+\ell}{\ell}^{(1/n)\cdot\Theta(\ell^2/\eps)}n^2\).  For any rate \(R\in(0,1-\eps)\), with probability at least \(1-\ell^{\,-\Omega(n)}\) over a uniform and independent choice of \(\alpha_1,\ldots,\alpha_n\sim\mb{F}_q\), the code \(\RS(\alpha_1,\ldots,\alpha_n;Rn)\) is \((1-R-\eps,\ell,O(\ell/\eps))\mhyphen\mc{L}_1\)-LR.  Further, the rate can be improved to exactly \(1-\eps\), when \(R=o_{n,\ell}(1)\).
		%		
		%		In particular, the assertion holds with the field size \(q\ge2^{\Theta(\ell^2/\eps)}n^2\) if \(\ell\le O(n)\).
		%	\end{theorem}
	
	\paragraph*{Comparison with previous list recovery results.}  Our results provide improvements upon the list recovery radius, as well as upon the output list size.
	\begin{itemize}
		\item  In the result of~\cite{goldberg-shangguan-tamo-2024-list-recovery-singleton-bound}, the rate approaches the Johnson bound with optimal dependence on the gap to capacity \(\eps\), and in the earlier result of~\cite{guo-li-shangguan-2022-list-recovery-RS-tree-packings} the rate breaks the \(1/\ell\) barrier of the Johnson bound, but with a worse dependence on \(\eps\) (by a factor of \(\log(1/\eps)\)).  We believe that our result  is the first where the rate breaks the \(1/\ell\) barrier of the Johnson bound while having optimal dependence on \(\eps\).  In fact, our result holds for any rate!  In particular,~\cref{thm:main-RS} strictly improves the result of~\cite{guo-li-shangguan-2022-list-recovery-RS-tree-packings} when \(\ell =  O(n)\), and strictly improves the result of~\cite{goldberg-shangguan-tamo-2024-list-recovery-singleton-bound} when \(\ell= O(\log n/\log\log n)\).
		
		\item  The best upper bound known on the output list size for constant-rate random linear codes is \((\ell/\eps)^{O((\log \ell)/\eps^4)}\), by Rudra and Wootters~\cite{rudra-wootters-2018-list-recovery}, over alphabets of size \(\ell^{\,O(1/\eps)}\).  We show that random RS codes beat this bound and give output list size \(O(\ell/\eps)\), which matches the bound for random unstructured codes.   This seems to be one of the relatively rare cases for linear codes in which structured randomness gives better results.  However, we do need the alphabet size to be exponential in \(\ell\).
	\end{itemize}
	Note that our results only imply that \emph{almost all} RS codes have the desired properties, as is the case with previous works, and the hunt for explicit constructions is still on.
	
	We summarize the previous state-of-the-art (including explicit list recoverable codes that are not RS codes), as well as our results (for the case when \(\eps\) is arbitrarily small, but fixed), in~\cref{tab:prev-results}.  
	
	\renewcommand{\arraystretch}{1.5}
	\begin{table}[p]
		\centering
		\begin{tabular}{| >{\centering\arraybackslash}p{2.1cm} | >{\centering\arraybackslash}p{2.75cm} | >{\centering\arraybackslash}p{1.5cm} | >{\centering\arraybackslash}p{2.75cm} | >{\centering\arraybackslash}p{1.4cm} | >{\centering\arraybackslash}p{3.5cm} |}
			\hline
			& Rate \(R\) & List recovery radius $\rho$ & Input list size \(\ell\) & Output list size \(L\) & Field size \(q\) \\
			\hline
			\hline
			&&&&&\\[-15pt]
			{\tiny Johnson bound~\cite{gopi-kopparty-oliviera-ron-zewi-2018-LTC-LCC-GV}} & \(\sim\dfrac{\eps^2}{\ell}\) & \(1-\eps\) & \(O(n)\) & \(O(\ell)\) & \(\Omega(n)\) \\[10pt]
			\hline
			{\tiny Random code~\cite{guruswami-2004-thesis}} & \(\Theta(1)\) & \(1-R-\eps\) & unrestricted & \(O(\ell/\eps)\) & \(\ell^{\,O(1/\eps)}\)\\
			\hline
			{\tiny Random pseudo-linear code \cite{guruswami-indyk-2001-expander-codes}} & \(\Theta(1)\) & \(1-R-\eps\) & unrestricted & {\(O\)\tiny\(\bigg(\dfrac{\ell\log\ell}{\eps^2}\bigg)\)} & \(\ell^{\,O(1/\eps)}\)\\
			\hline
			{\tiny Random linear code \cite{rudra-wootters-2018-list-recovery}} & \(\Theta(1)\) & \(1-R-\eps\) & unrestricted & {\tiny\(\bigg(\dfrac{\ell}{\eps}\bigg)^{O\big(\frac{\log\ell}{\eps^4}\big)}\)} & \(\ell^{\,O(1/\eps)}\)\\
			\hline
			{\tiny Random linear code \cite{guruswami-li-moshieff-resch-2022-list-decoding-list-recovery-random-linear-codes}} & \(\Theta(1)\) & 0 & unrestricted & \(\ell^{\,\Omega(1/\eps)}\) & \(\Omega(\ell)\)\\
			\hline
			{\tiny Folded RS code \cite{tamo-2023-tighter-list-size}} & \(\Theta(1)\) & \(1-R-\eps\) & unrestricted & {\tiny\(\bigg(\dfrac{\ell}{\eps}\bigg)^{\,O(\ell/\eps)}\)} & {\(\Omega(n)\)}\\
			\hline
			&&&&&\\[-15pt]
			{\tiny Folded RS code + expanders \cite{kopparty-ron-zewi-saraf-wootters-2023-list-decoding}} & \(\Theta(1)\) & \(1-R-\eps\) & unrestricted & \(O_{\eps,\ell}(1)\) & \((1+\ell)^{O(1/\eps^6)}\) \\
			\hline
			\hline
			\multicolumn{6}{|c|}{\emph{Previous state-of-the-art for list recovery of random RS codes}}\\
			\hline
			\hline
			&&&&&\\[-15pt]
			\cite{lund-potukuchi-2020-list-recovery-RS} & \(\sim\dfrac{1}{\sqrt{\ell}\cdot\log q}\) & \(1-\dfrac{1}{\sqrt{2}}\)& \(O(n)\) & \(O(\ell)\) & \(\Omega\big(\sqrt{\ell}\cdot n\log n\big)\) \\[10pt]
			\hline
			&&&&&\\[-15pt]
			\cite{guo-li-shangguan-2022-list-recovery-RS-tree-packings} & \(\sim\dfrac{\eps}{\sqrt{\ell}\cdot\log(1/\eps)}\) & \(1-\eps\) & \(O(n)\) & \(O(\ell/\eps)\) & \((\ell/\eps)^{O(n/\eps)}\) \\[10pt]
			\hline
			&&&&&\\[-15pt]
			\cite{goldberg-shangguan-tamo-2024-list-recovery-singleton-bound} & \(\sim\dfrac{\eps}{\ell}\) & \(1-\eps\) & \(O(n)\) & \(O(\ell/\eps)\) & \(n^{O(1/\eps)}\)\\[10pt]
			\hline
			\hline
			\multicolumn{6}{|c|}{\emph{Our results for list recovery of random RS codes}}\\
			\hline
			\hline
			\cref{thm:main-RS}  & unrestricted & \(1-R-\eps\) & unrestricted & \(O(\ell/\eps)\) & \(\ell^{\,\Theta_R(\ell^2/\eps^3)}n^2\) \\
			\hline
			&&&&&\\[-15pt]
			\cref{thm:main-RS}  & unrestricted & \(1-R-\eps\) & \(O\bigg(\sqrt{\dfrac{\log n}{\log\log n}}\,\bigg)\) & \(O(\ell/\eps)\) & \(n^{\Theta_R(1/\eps^3)}\) \\[10pt]
			\hline
			%			&&&&&\\[-15pt]
			%			\cref{thm:main-RS-L1}  & unrestricted & \(1-R-\eps\) & unrestricted & \(O(\ell/\eps)\) & \(\dbinom{n+\ell}{\ell}^{\frac{1}{n}\cdot\,\Theta(\ell^2/\eps)}n^2\) \\[10pt]
			%			\hline
			%			\cref{thm:main-RS-L1}  & \(O(1)\) & \(1-\eps\) & \(O(n^{1/3})\) & \(O(\ell/\eps)\) & \(n^{\Theta(1/\eps)}\) \\
			%			\hline
			%			\cref{thm:main-RS-L1}  & \(O(1)\) & \(1-\eps\) & \(O(n)\) & \(O(\ell/\eps)\) & \(2^{\Theta(\ell^2/\eps)}Rn^2\) \\
			%			\hline
			%			&&&&&\\[-15pt]
			%			\cref{thm:main-RS-L1}  & \(O(1)\) & \(1-\eps\) & \(\omega(n)\) & \(O(\ell/\eps)\) & \(\bigg(\dfrac{\ell}{n}\bigg)^{\Theta(\ell^2/\eps)}\) \\[10pt]
			%			\hline
		\end{tabular}
		\caption{Previous state-of-the-art, and our results for list recovery of random RS codes.  Here, \(\eps>0\) is arbitrarily small, but fixed, and \(n,\ell\) are increasing.} 
		\label{tab:prev-results}
	\end{table}
	
	
	
	\subsection{Proof outline}\label{sec:outline}
	
	We will build upon the proof ideas in two previous works on list decoding by Brakensiek, Gopi, and Makam~\cite{brakensiek-gopi-makam-2023-random-RS}, and Guo and Zhang~\cite{guo-zhang-2023-random-RS}.  These works, when put together, showed that random RS codes achieve \emph{list decoding capacity} with field size \(q\ge\Omega_\eps(n^2)\).  The field size was subsequently improved to \(q\ge\Omega_\eps(n)\) (which is optimal) by Alrabiah, Guruswami, and Li~\cite{alrabiah-guruswami-li-2023-random-RS}, but our analysis for list recovery does not admit such an improvement.
	
	We will first summarize the argument for list decoding due to~\cite{brakensiek-gopi-makam-2023-random-RS,guo-zhang-2023-random-RS}, and then outline our extension to list recovery, while highlighting the differences in our argument.  For the remainder of this subsection, for simplicity, we assume that that the rate \(R\) is a constant, and \(\eps>0\) is an arbitrarily small constant as well.
	
	
	\subsubsection*{The argument of~\cite{guo-zhang-2023-random-RS} for list decoding of random RS codes}\label{sec:GZ-LD}
	
	Let us begin with a quick summary of the argument in~\cite{guo-zhang-2023-random-RS}, for random RS codes achieving list decoding capacity.  Consider some distinct points \(\alpha_1,\ldots,\alpha_n\in\mb{F}_q\), and \(1\le k\le n\le q\).  Suppose that the RS code \(\RS(\alpha_1,\ldots,\alpha_n;k)\) is not \((\rho,L)\)-LD, witnessed by a \emph{bad} received word \(w\in\mb{F}_q^n\), and a \emph{bad} list of codewords \(\{c^{(1)},\ldots,c^{(L+1)}\}\subseteq\RS(\alpha_1,\ldots,\alpha_n;k)\), each of which is at a relative Hamming distance at most \(\rho\) from \(w\).\footnote{The works~\cite{brakensiek-gopi-makam-2023-random-RS,guo-zhang-2023-random-RS} prove the assertions for a slightly stronger notion of \emph{average radius list decodability (ARLD)}.  In fact, we also prove our assertions for a slightly stronger notion of \emph{average radius list recoverability (ARLR)}.  We ignore this point in the high-level outline.}  Consider the corresponding \emph{generic} RS code \(\RS(X_1,\ldots,X_n;k)\) over the generic field \(\mb{F}_q(X_1,\ldots,X_n)\), where \(X_1,\ldots,X_n\) are indeterminates.  The linear constraints between the codewords \(c^{(1)},\ldots,c^{(L+1)}\) for the RS code are then recorded as linear constraints between corresponding codewords for the generic RS code: For any \(i\in[n]\), and distinct \(j,j'\in[L+1]\), suppose \(c^{(j)}_i=w_i=c^{(j')}_i\).  So the observed linear relation between the codewords \(c^{(j)},c^{(j')}\) for the RS code is \(V_k(\alpha_i)\cdot(f^{(j)}-f^{(j')})=0\), where \(V_k(\alpha_i)=\begin{bmatrix}1&\alpha_i&\cdots&\alpha_i^{k-1}\end{bmatrix}\) is a Vandermonde row, and \(f^{(j)}\in\mb{F}_q^k\) is the column vector of coefficients of the polynomial corresponding to \(c^{(j)}\).  The recorded linear relation for corresponding codewords of the \emph{generic} RS code is then \(V_k(X_i)\cdot(f^{(j)}-f^{(j')})=0\).  These linear constraints (after removing some obvious symmetries) form a linear system
	\[
	\RIM(X_1,\ldots,X_n)\cdot\begin{bmatrix}
		f^{(1)}-f^{(L+1)}\\\vdots\\f^{(L)}-f^{(L+1)}
	\end{bmatrix}=0,
	\]
	where the resulting matrix \(\RIM(X_1,\ldots,X_n)\) is called the \emph{reduced intersection matrix (RIM)}.\footnote{The RIM is a \emph{lighter} version of an earlier variant called the \emph{intersection matrix}, as defined in the work of Shangguan and Tamo~\cite{shangguan-tamo-2023-list-decoding-RS-beyond-Johnson-bound}.}  The analysis in~\cite{brakensiek-gopi-makam-2023-random-RS} implies that \(\RIM(X_1,\ldots,X_n)\) has full \(\mb{F}_q(X_1,\ldots,X_n)\)-column rank.\footnote{Precisely speaking, the claim of full column rank is made for a large enough submatrix (formed by choosing columns carefully).  We omit this subtlety here.}
	
	Assuming, towards a contradiction, that we have a bad received word and a bad output list of codewords,  the argument proceeds by considering the probability of full column-rankness of \(\RIM(\alpha_1,\ldots,\alpha_n)\), and a union bound over all possibilities of \(\RIM(X_1,\ldots,X_n)\) (by considering all possibilities of the linear constraints between codewords that could be obtained).  If we simply apply the analysis in~\cite{guo-zhang-2023-random-RS} right away, we can see that the failure probability for each possible RIM is small, and then the total failure probability is also small, provided that the field size \(q\ge(n/\eps)^{\Theta(n/\eps)}\). Building upon this,~\cite{guo-zhang-2023-random-RS} additionally observed that that the analysis of the full column-rankness of \(\RIM(\alpha_1,\ldots,\alpha_n)\) can be improved by worsening the decoding radius by a small fixed amount \(\lambda>0\), which we call \emph{slackness}.  In this case, the failure probability for each possible RIM can be reduced to \(2^{-\Omega(n)}\), and then the total failure probability can be shown to be small, even for \(q=2^{\Theta(1/\eps^3)}n^2\).  The follow-up work~\cite{alrabiah-guruswami-li-2023-random-RS} tightened this analysis to give field size \(q=2^{\Theta(1/\eps^2)}n\), and showed that this is optimal.
	
	We extend these ideas to our context of list recovery, with suitable modifications.
	
	\subsubsection*{Our argument for list recovery of random RS codes}\label{sec:main-LR}
	
	We now give an outline of our argument in the context of list recovery.  We follow the high-level strategy of~\cite{guo-zhang-2023-random-RS}, but there are two departures from their analysis.
	
	Consider some distinct points \(\alpha_1,\ldots,\alpha_n\in\mb{F}_q\), and \(1\le k\le n\le q\).  Suppose the RS code \(\RS(\alpha_1,\ldots,\alpha_n;k)\) is not \((\rho,\ell,L)\)-LR, witnessed by a \emph{bad} input tuple \((S_1,\ldots,S_n)\) of subsets of \(\mb{F}_q\) each having size \(\ell\), and a \emph{bad} list of codewords \(\{c^{(1)},\ldots,c^{(L+1)}\}\subseteq\RS(\alpha_1,\ldots,\alpha_n;k)\) each of which is at a relative Hamming distance at most \(\rho\) from \((S_1,\ldots,S_n)\).
	
	The first departure from~\cite{guo-zhang-2023-random-RS} is that we consider an \emph{extended} generic RS code \[\RS\big((X_{i,t})_{[n]\times[\ell]};k\big)\] of length \(\ell n\) over the generic field \(\mb{F}_q\big((X_{i,t})_{[n]\times[\ell]}\big)\), where \(X_{i,t},\,i\in[n],\,t\in[\ell]\) are indeterminates.  Assume an arbitrary, but fixed, enumeration of the sets \(S_i=\{w_{i,1},\ldots,w_{i,\ell}\},\,i\in[n]\).\footnote{Strictly speaking, we only assume \(|S_i|\le\ell\) for all \(i\in[n]\), and not equality.  Our analysis in this generality remains the same, modulo some minor modifications.  So we assume equality in the high-level outline.}  The linear constraints between the codewords \(c^{(1)},\ldots,c^{(L+1)}\) for the RS code are then recorded as linear constraints between corresponding codewords for the extended generic RS code, described as follows.  For any \(i\in[n]\), distinct \(j,j'\in[L+1]\), and \(t\in[\ell]\), suppose \(c^{(j)}_i=w_{i,t}=c^{(j')}_i\).  So the observed linear relation between the codewords \(c^{(j)},c^{(j')}\) for the RS code is \(V_k(\alpha_i)\cdot(f^{(j)}-f^{(j')})=0\), but with the added information that the agreement is with evaluation \(w_{i,t}\).  So we record the linear relation for corresponding codewords of the extended generic RS code as \(V_k(X_{i,t})\cdot(f^{(j)}-f^{(j')})=0\).
	
	These recorded linear constraints (after removing some obvious symmetries) will again form a linear system
	\[
	\RIM\big((X_{i,t})_{[n]\times[\ell]}\big)\cdot\begin{bmatrix}
		f^{(1)}-f^{(L+1)}\\\vdots\\f^{(L)}-f^{(L+1)}
	\end{bmatrix}=0,
	\]
	where we now refer to the resulting matrix \(\RIM\big((X_{i,t})_{[n]\times[\ell]}\big)\) as the \emph{extended RIM}.  An analysis analogous to that by~\cite{brakensiek-gopi-makam-2023-random-RS} will now show that \(\RIM\big((X_{i,t})_{[n]\times[\ell]}\big)\) has full column rank.\footnote{Again, the claim of full column rank will, in fact, be made for a large enough submatrix (formed by choosing columns carefully).  We omit this subtlety here.}  The list recoverability of the random RS code \(\RS(\alpha_1,\ldots,\alpha_n;k)\) will then follow by an analysis of the full column-rankness of the extended RIM under \emph{correlated random substitutions}.  Precisely, we will analyze the full column-rankness of the matrix
	\[
	\RIM^*\coloneqq\RIM\big(X_{i,t}=\alpha_i:t\in[\ell],\,i\in[n]\big).
	\]
	
	Let us now elaborate a bit about our rank analysis under correlated random substitutions.  Intuitively, if \(\RIM^*\) does not have full column rank, then we can record a \emph{profile} of the fall in rank by measuring the rank of the intermediate matrices
	\[
	\RIM^*_i\coloneqq\RIM\big((X_{i',t}=\alpha_i:t\in[\ell],\,i'\le i),(X_{i',t}:t\in[\ell],\,i'>i)\big),\quad i\in[n].
	\]
	This profile is represented by a vector formed by the indices \(i\in[n]\) witnessing a strict fall in the rank of \(\RIM^*_i\) compared to the rank of \(\RIM^*_{i-1}\), where \(\RIM^*_0\coloneqq\RIM\big(X_{i,t}:t\in[\ell],\,i\in[n]\big)\).  Finally, we show that the probability of obtaining any given \emph{profile vector} is small, and further, there are not too many possibilities for the profile vector.  A probability union bound will then wrap up our analysis.
	
	We formalize the above intuition by algorithmically generating the profile vector, as in~\cite{guo-zhang-2023-random-RS}.  We introduce a similar \emph{slackness} \(\lambda>0\) as in~\cite{guo-zhang-2023-random-RS} that slightly worsens the list recovery radius, and begin by noting that \(\wt{\RIM}^*(0)\coloneqq\RIM\big(X_{i,t}:t\in[\ell],\,i\in[n]\big)\) has full column rank.  So we fix a maximal submatrix \(M\) of \(\wt{\RIM}^*(0)\) that is nonsingular.  We then run through \(i\in[n]\) and find the minimal index \(i^*\) such that the rank of \(M\) under the partial correlated substitution
	\[
	\big((X_{i',t}=\alpha_i:t\in[\ell],\,i'\le i^*),(X_{i',t}:t\in[\ell],\,i'>i^*)\big)
	\]
	has strictly fallen below maximum.  If such an \(i^*\) does not exist, then we have certified that \(\RIM^*\) has full column rank; otherwise, we set \(i_1=i^*\), remove all the rows in \(\wt{\RIM}^*(0)\) that depend on the indeterminates \(X_{i^*,t},\,t\in[\ell]\), and denote the resulting matrix by \(\wt{\RIM}^*(i_1)\).  We then repeat this process \(r\) times, where \(r\le n\) is a parameter that we optimize.
	
	The \emph{second departure} from~\cite{guo-zhang-2023-random-RS} is in the optimization of \(r\).  In~\cite{guo-zhang-2023-random-RS}, \(r\) is carefully chosen to be a specific function \(\phi(\lambda,k,L)\).  We introduce a second \emph{slackness} \(\mu\in(0,1)\), and the optimal \(r\) is only \(\mu\) times the corresponding choice in~\cite{guo-zhang-2023-random-RS}, that is, our choice of \(r\) is \(\mu\cdot\phi(\lambda,k,L)\).\footnote{Note that we also set \(L=O(\ell/\eps)\) in list recovery, whereas for list decoding, the choice is \(L=O(1/\eps)\).}   This second slackness is not required in the list decoding setting, but is essential in our list recovery setting, to ensure that we have enough number of rows remaining to continue the iterative process.  Finally, at the end, we either certify that \(\RIM^*\) has full column rank, or generate a profile vector \((i_1,\ldots,i_r)\) that records the fall in rank.  Prominently, in contrast to~\cite{guo-zhang-2023-random-RS}, we observe that the number of possibilities of the extended RIM giving rise to a given profile vector is not very small; in fact, it is exponential in \(\ell\).  So we will need the field size to satisfy \(q=\ell^{\,\Omega_R(\ell^2/\eps^3)}n^2\) for the final probability union bound to be \(\ell^{-\Omega(n)}\).
	
	\subsection{Open questions}\label{sec:questions}
	
	
	Firstly, we note that for list decoding random RS codes,~\cite{alrabiah-guruswami-li-2023-random-RS} generated the profile vectors a bit differently, which gave a better bound on their count, and this improved the field size \(q\) to linear in \(n\).  We have not attempted to extend this improvement from \(n^2\) to \(n\) to the list recovery setting, and so we do not have linear dependence of \(q\) on \(n\).  Nevertheless, we surmise that this is very much true. %(and, in fact, can be proved with minimal changes to our argument).
	
	%	\begin{conjecture}\label{conj:q-linear}
		%		The assertions of~{\normalfont\cref{thm:main-RS}} hold for finite fields \(\mb{F}_q\) with \(q\ge\ell^{\,\Theta_\eps(\ell^2)}n\).
		%	\end{conjecture}
	
	Perhaps more importantly, for list recovery in general, and particularly for list recovery of RS codes, we would like to obtain a complete understanding of the dependence of the field size \(q\) and output list size $L$ on the input list size \(\ell\), for \emph{all ranges} of $\ell$.  To this effect, the following are natural questions to ponder about.
	\begin{question}\label{ques:poly-l}
		Do the assertions of~{\normalfont\cref{thm:main-RS}} hold for finite fields \(\mb{F}_q\), where \(q\sim(\ell n)^{O_\eps(1)}\) or \(q\sim\ell^{\,O_\eps(1)}n\)?  If not, is an exponential dependence of \(q\) on \(\ell\) necessary?
	\end{question}
	\noindent 
	Also, while it may be the case that for $L = O(\ell/\eps)$, an exponential dependence on $\ell$ is necessary,\footnote{We note that an exponential dependence on $1/\eps$ is necessary, as can be seen from capacity theorems for list decoding (see, e.g.,~\cite{zyablov-pinsker-1981} for the list decoding capacity theorem, and also the extension to list recovery capacity in~\cite{rudra-wootters-2018-list-recovery}).} this may not be the case if we allow $L$ to be slightly larger, even polynomial in $\ell/\eps$, and the recovery radius $\rho$ to be smaller, say any $\Omega(1-R)$. 
	In particular, can we get the alphabet size bounds in~\cref{ques:poly-l} with a slightly worse $L$, and a slightly worse rate? (So concretely, since we do not insist on tight distance to capacity, the alphabet size bounds in~\cref{ques:poly-l} would have no dependence on $\eps$.)\footnote{The Reed--Solomon condenser in \cite{guruswami-umans-vadhan-2009-unbalanced-expanders} implies that for some vanishing rate, a polynomial dependence on $\ell$ is possible.}
	
	
	
	
	\subsection*{Organization of the paper}
	\addcontentsline{toc}{subsection}{\protect\numberline{}Organization of the paper}
	
	We begin by reviewing some preliminaries in~\cref{sec:prelims}.  In~\cref{sec:extended-RIM}, we define our main linear algebraic tool, the extended reduced intersection matrix, and discuss in detail some of the subtleties involved in the definition.  We prove our main result,~\cref{thm:main-RS}, in~\cref{sec:list-recovery-RS}, in several steps.  Firstly, in~\cref{sec:optimal-list-recovery}, we observe that~\cref{thm:main-RS} follows from a slightly more general statement (\cref{thm:big}) for a slightly stronger notion of \emph{average radius list recovery}.  We analyse the extended RIM and its column rank in~\cref{sec:RIM-LR,sec:rank-analysis}, and proceed to finish the proof of~\cref{thm:big}, modulo a few technical lemmas, in~\cref{sec:proof-big-theorem}.  We then complete our argument by proving the technical lemmas in~\cref{sec:intersection}.
	
	%	Finally, in~\cref{sec:L1}, we will prove the (\cref{cor:L1})
	
	\section{Preliminaries}\label{sec:prelims}
	
	In this section, let us review some basic notions and terminologies that we will require along the way.
	
	\subsection*{Linear codes}
	
	Fix a field \(\mb{F}\).  An \tsf{\(\mb{F}\)-linear code} of \tsf{length} \(n\) is any \(\mb{F}\)-linear subspace of \(\mb{F}^n\).  The \tsf{rate} of the code \(C\) is \(k/n\), if \(C\) is a \(k\)-dimensional subspace.  A \tsf{generating matrix} for \(C\) is any matrix \(G\in\mb{F}^{n\times k}\) such that \(C=\{Gx:x\in\mb{F}^k\}\) (and so \(G\) has rank \(k\)), and a \tsf{parity check matrix} for \(C\) is any matrix \(H\in\mb{F}^{n\times(n-k)}\) such that \(C=\{y\in\mb{F}^n:H^\transpose y=0\}\) (and so \(H\) has rank \(n-k\)).  The \tsf{dual} of \(C\) is defined by
	\[
	C^\perp=\{z\in\mb{F}^n:z\cdot y=0,\tx{ for all }y\in C\},
	\]
	where \(z\cdot y\coloneqq\sum_{i=1}^nz_iy_i\) is the usual \tsf{dot product} over $\mb{F}$.  It is well-known that if \(C\) has rate \(k/n\), then \(C^\perp\) has rate \(1-(k/n)\), and further, a matrix \(H\in\mb{F}^{n\times(n-k)}\) is a parity check matrix for \(C\) if and only if \(H\) is a generating matrix for \(C^\perp\).
	
	We will mostly be interested in two cases:\n(i) \(\mb{F}=\mb{F}_q\), a finite field, and\n(ii) \(\mb{F}=\mb{F}_q(X_1,\ldots,X_n)\), a function field in which \(X_1,\ldots,X_n\) are indeterminates.  We may sometimes distinguish linear codes in the latter case by referring to them as \tsf{generic codes}.
	
	For any \(a=(a_1,\ldots,a_n),b=(b_1,\ldots,b_n)\in\mb{F}^n\), the \tsf{(relative) Hamming distance} between \(a\) and \(b\) is defined by
	\[
	\msf{d}(a,b)=\frac{1}{n}|\{i\in[n]:a_i\ne b_i\}|.
	\]
	The \tsf{(minimum relative) distance} of an \(\mb{F}\)-linear code \(C\) is defined by \(\delta(C)=\min\{\msf{d}(a,b):a,b\in C,\,a\ne b\}\).  The Singleton bound~\cite{joshi-1958-MDS,komamiya-1953-MDS,singleton-1964-MDS} states that for any linear code \(C\) of length \(n\), rate \(R\), and distance \(\delta\), we have \(R+\delta\le 1+(1/n)\).  Codes that achieve this bound are called \tsf{maximum distance separable (MDS)} codes.  It is also known that if \(C\) is an MDS code, then \(C^\perp\) is also an MDS code.  Further, in this case, every generating matrix and every parity check matrix is an \tsf{MDS matrix}, which means every maximal minor is nonzero.
	
	\subsection*{List recovery}
	
	An \tsf{input list} is any tuple \(S=(S_1,\ldots,S_n)\) of subsets \(S_i\subseteq\mb{F}\).  Further, for any \(a\in\mb{F}_q^n\), the (relative) Hamming distance between \(a\) and \(S\) is defined by
	\[
	\msf{d}(a,S)=\frac{1}{n}|\{i\in[n]:a_i\not\in S_i\}|.
	\]
	We say an input list \(S=(S_1,\ldots,S_n)\) is \tsf{\(\ell\)-sized} if \(|S_i|\le\ell\) for all \(i\in[n]\).
	
	Consider an \(\mb{F}\)-linear code \(C\).  For \(\rho\in[0,1]\) and \(\ell,L\in\mb{Z}^+\), we say \(C\) is \tsf{\((\rho,\ell,L)\)-list recoverable (LR)} if there does not exist any \(\ell\)-sized sequence of input lists \(S\) that admits \(L+1\) distinct codewords \(c^{(1)},\ldots,c^{(L+1)}\in C\) such that
	\[
	\msf{d}(c^{(j)},S)\le\rho,\quad\tx{for all }j\in[L+1].
	\]
	As it turns out, it is more convenient to work with a slightly stronger version of list recovery.  The code \(C\) is said to be \tsf{\((\rho,\ell,L)\)-average radius list recoverable (ARLR)} if there does not exist any \(\ell\)-sized sequence of input lists \(S\) that admits \(L+1\) distinct codewords \(c^{(1)},\ldots,c^{(L+1)}\in C\) such that
	\[
	\frac{1}{L+1}\sum_{j=1}^{L+1}\msf{d}(c^{(j)},S)\le\rho.
	\]
	It is immediate that if \(C\) is \((\rho,\ell,L)\)-ARLR, then \(C\) is also \((\rho,\ell,L)\)-LR.  Also note that \(C\) is said to be \tsf{\((\rho,L)\)-list decodable (LD)} if \(C\) is \((\rho,1,L)\)-LR, and \(C\) is said to be \tsf{\((\rho,L)\)-average radius list decodable (ARLD)} if \(C\) is \((\rho,1,L)\)-ARLR.  Recently,~\cite{goldberg-shangguan-tamo-2024-list-recovery-singleton-bound} gave an analogue of Singleton bound for list recovery (see the above~\cref{thm:singleton}).
	
	\subsection*{Reed-Solomon codes}
	
	For any distinct \(\alpha_1,\ldots,\alpha_n\in\mb{F}\), and \(1\le k\le n\), the \tsf{degree-\(k\) Reed-Solomon (RS) code} is an \(\mb{F}\)-linear code defined by
	\[
	\RS(\alpha_1,\ldots,\alpha_n;k)=\{(f(\alpha_1),\ldots,f(\alpha_n)):f(X)\in\mb{F}[X],\,\deg(f)<k\}.
	\]
	It is easy to see that \(\RS(\alpha_1,\ldots,\alpha_n;k)\) has rate \(k/n\) and distance \(1-((k-1)/n)\).  Thus, \(\RS(\alpha_1,\ldots,\alpha_n;k)\) is an MDS code.
	
	For any \(\alpha\in\mb{F}\), denote the \tsf{Vandermonde row} by \(V_k(\alpha)\coloneqq\begin{bmatrix}
		1&\alpha&\cdots&\alpha^{k-1}
	\end{bmatrix}\), and further, denote the \tsf{Vandermonde matrix} by \[V_k(\alpha_1,\ldots,\alpha_n)\coloneqq\begin{bmatrix}
		V_k(\alpha_1)\\\vdots\\ V_k(\alpha_n)
	\end{bmatrix}.\]  It is clear, by definition, that \(V_k(\alpha_1,\ldots,\alpha_n)\) is a generating matrix of the code \(\RS(\alpha_1,\ldots,\alpha_n;k)\).  Further, it is known that a parity check matrix of \(\RS(\alpha_1,\ldots,\alpha_n;k)\) is
	\[
	H_k(\alpha_1,\ldots,\alpha_n)\coloneqq\diag\bigg(\prod_{j\in[n]\setminus\{i\}}\frac{1}{\alpha_i-\alpha_j}:i\in[n]\bigg)\cdot V_{n-k}(\alpha_1,\ldots,\alpha_n).
	\]
	Thus, \(V_k(\alpha_1,\ldots,\alpha_n)\) and \(H_k(\alpha_1,\ldots,\alpha_n)\) are MDS matrices.
	
	\subsection*{Additional notation}
	
	Given any sequence \((z_1,\ldots,z_n)\) of elements (from some universe), and a subset \(U\subseteq[n]\), we will denote the subsequence \(z_U\coloneqq(z_u:u\in U)\).  We will use the subsequence notation frequently to refer to `indexed objects'; for instance, we will consider the Vandermonde matrices \(V_k(\alpha_U)\) and \(V_k(X_U)\), and the RS codes \(\RS(\alpha_U;k)\) and \(\RS(X_U;k)\).
	
	For any polynomial \(f(X)\in\mb{F}[X],\,\deg(f)<k\), we denote (by abuse of notation) the column vector of the coefficients of \(f(X)\) by \(f\in\mb{F}^{k\times 1}\).  So every codeword in \(\RS(\alpha_{[n]})\) is \(V_k(\alpha_{[n]})\cdot f\), for some \(f(X)\in\mb{F}[X],\,\deg(f)<k\).
	
	
	\section{The \emph{Extended} Reduced Intersection Matrix}\label{sec:extended-RIM}
	
	In their analysis of list decoding,~\cite{guo-zhang-2023-random-RS} constructed a \emph{reduced intersection matrix (RIM)} (inspired by an earlier construction of~\cite{shangguan-tamo-2023-list-decoding-RS-beyond-Johnson-bound}), which recorded in a generic sense, the linear constraints between a given set of codewords with respect to a received word.  Our goal here is to introduce an analogue of the RIM that facilitates list recovery.  We call this the \emph{extended RIM}.  We will only work with RS codes, but this discussion can also be extended to other codes in general.
	
	Consider the finite field \(\mb{F}_q\).  We will first define our extended RIM, for a given input list, and for a set of codewords in an RS code.  In fact, we will give the definitions in a special case, with some assumptions, and later extend this definition to the general case.  At the end of this section, we will relax the setting a bit more, and see that the extended RIM does not really depend on the set of codewords, or the input list, but only on the resulting set systems.
	
	\subsection{Extended RIM for an input list, and a set of codewords}\label{sec:RIM-RS}
	
	Fix some distinct \(\alpha_1,\ldots,\alpha_n\in\mb{F}_q\), and consider the code \(\RS(\alpha_{[n]};k)\).   Let \(S=(S_1,\ldots,S_n)\) be an \(\ell\)-sized input list, and consider distinct codewords \(c^{(1)},\ldots,c^{(T)}\in\RS(\alpha_{[n]};k)\) with \(c^{(j)}=V_k(\alpha_{[n]})\cdot f^{(j)}\). 
	
	\subsubsection*{A special case}
	
	We will make some convenient assumptions along the way.
	\begin{assumption}\label{ass:size}
		Each subset \(S_i\) in the \(\ell\)-sized input list \(S=(S_1,\ldots,S_n)\) has size exactly \(\ell\).
	\end{assumption}
	Let us enumerate \(S_i=\{w_{i,1},\ldots,w_{i,\ell}\}\) for $i \in [n]$.  Fix some \(B\subseteq[n],\,J\subseteq[T]\).  For \(j\in J,\,(i,t)\in([n]\setminus B)\times[\ell]\), define
	\[
	I(c^{(j)})=\{(i',t')\in([n]\setminus B)\times[\ell]:c^{(j)}_{i'}=w_{i',t'}\},\quad\tx{and}\quad J_{i,t}=\{j'\in J:c^{(j')}_i=w_{i,t}\}.
	\]
	So we have \((i,t)\in I(c^{(j)})\) if and only if \(j\in J_{i,t}\).  Denote \(I_j=I(c^{(j)}),\,j\in J\), and let \(U=\bigcup_{j\in J}I_j\).  The above two definitions immediately capture a set of linear constraints among the codewords \(c^{(1)},\ldots,c^{(T)}\).
	\begin{constraint}\label{con:linear-relations}
		The following two sets of linear constraints are equivalent.
		\begin{enumerate}[{\normalfont(a)}]
			\item  For any \((i,t)\in U\), and distinct \(j,j'\in J_{i,t}\), we have
			\[
			c^{(j)}_i=w_{i,t}=c^{(j')}_i,\quad\tx{which implies,}\quad V_k(\alpha_i)\cdot f^{(j)}=V_k(\alpha_i)\cdot f^{(j')}.
			\]
			\item  For any distinct \(j,j'\in J\), and \((i,t)\in I_j\cap I_{j'}\), we have
			\[
			c^{(j)}_i=w_{i,t}=c^{(j')}_i,\quad\tx{which implies,}\quad V_k(\alpha_i)\cdot f^{(j)}=V_k(\alpha_i)\cdot f^{(j')}.
			\]
		\end{enumerate}
	\end{constraint}
	
	We will now construct our extended RIM using~\cref{con:linear-relations}.  Consider an indeterminate \(X_{i,t}\), for each \((i,t)\in U\).  The extended RIM will be a matrix over the field \(\mb{F}_q(X_U)\).  The rows of this matrix are constructed using the following linear constraints for the generic RS code \(\RS(X_U;k)\), which are \emph{generic analogues} of~\cref{con:linear-relations}.
	\begin{constraint}\label{con:generic-relations}
		The following two sets of linear constraints are equivalent.
		\begin{enumerate}[(a)]
			\item  For \((i,t)\in U\), and distinct \(j,j'\in J_{i,t}\), we have \(V_k(X_{i,t})\cdot(f^{(j)}-f^{(j')})=0\).
			\item  For distinct \(j,j'\in J\), and \((i,t)\in I_j\cap I_{j'}\), we have \(V_k(X_{i,t})\cdot(f^{(j)}-f^{(j')})=0\).
		\end{enumerate}
	\end{constraint}
	\noindent We see that substituting \(X_{i,t}=\alpha_i\) for all \((i,t)\in U\) in~\cref{con:generic-relations} immediately gives~\cref{con:linear-relations}.  Further, crucially, if any constraint in~\cref{con:linear-relations} is implied by an agreement between two codewords with value \(w_{i,t}\), then the corresponding constraint in~\cref{con:generic-relations} is considered for the indeterminate \(X_{i,t}\).  This is an additional feature that we will need in the list recovery setting vis-\`a-vis the list decoding setting considered in~\cite{guo-zhang-2023-random-RS}.  Also, denote this \emph{correlated substitution} by \[\ol{\alpha}_U=\big(\underbrace{\alpha_1,\ldots,\alpha_1}_{\ell\tx{ times}},\ldots,\underbrace{\alpha_n,\ldots,\alpha_n}_{\ell\tx{ times}}\big).\]
	
	% \dean{Reading up to here, it's not clear if it can be done w.l.o.g.\ since the reader doesn't know how it's going to be used. So we should make sure to say why it's w.l.o.g. in the relevant place in the proof} \dean{It depends on the underlying $J$. So you want to say that we'll work with $J$-s such that...?}
	
	% \venki{Removed the `w.l.o.g.' niceness conditions.  Adding a few assumptions along the way in this subsection, for the definition.  The extension of the definition to the general case is explained in the next subsection.}
	
	%	Henceforth, without loss of generality , we will assume throughout the following \emph{niceness} conditions. 
	%	\begin{condition}\label{con:J-it}
		%		For each \(i\in[n]\setminus B\), we have
		%		\begin{enumerate}[\normalfont(a)]
			%			\item  \(|J_{i,t}|\ge1\) for every \(t\in[\ell]\).
			%			\item  \(|J_{i,t}|\ge2\) for some \(t\in[\ell]\).
			%		\end{enumerate}
		%	\end{condition}
	%	\begin{remark}
		%		It is easy to see that Condition~\ref{con:J-it} (a) ensures that each input list \(S_i\) is nontrivial \dean{First, it depends on $J$. Are you talking about the case when $T=L+1$ and $J=[T]$? Second, ``trivial'' usually means the entire $\mathbb{F}_q$, and I don't think here it's the case. So explain.}, and further, Condition~\ref{con:J-it} (b) ensures that at least one of the \(J'_{i,t}\)-s is nonempty.  If Condition~\ref{con:J-it} is not satisfied for some \(i\in[n]\setminus B\), then the remedy is elementary. \dean{Again. we don't know yet if this remedy has any affect on the proof.}
		%		\begin{enumerate}[(a)]
			%			\item  If \(|J_{i,t}|=0\) for some \(t\in[\ell]\), then we replace \(S_i\) with \(S_i\setminus\{w_{i,t}\}\).
			%			\item  If \(|J_{i,t}|\le1\) for every \(t\in[\ell]\), then we replace \(B\) with \(B\sqcup\{i\}\).
			%		\end{enumerate}
		%	\end{remark}
	
	%	Let \(U=\bigcup_{j\in J}I_j\).  We will also need to work over the function field \(\mb{F}_q(X_U)\), and consider the generic RS code \(\RS(X_U;k)\). \dean{What does it mean? And what does it mean that we consider it? How is it manifested below?}
	
	We are now ready to define the extended RIM.  We will not have one row for each constraint in~\cref{con:generic-relations}; instead, we will construct a \emph{minimal}\footnote{The `minimal' set of linear constraints that we construct may not really be minimal in any formal sense.  Instead, what we mean is that we shrink the collection in~\cref{con:generic-relations} by removing all obvious redundancies by either deletion or a linear transformation.  There could still be some `hidden' redundancies.} set of constraints that generate all of them, and have one row for each minimal constraint.  Let \(T_J=\max(J)\le T\), and denote \(j_{i,t}=\min(J_{i,t})\) for all \((i,t)\in U\).  It is easy to see that the following constraints generate~\cref{con:generic-relations}.
	\leqnomode	
	\begin{align}
		\qquad V_k(X_{i,t})\cdot(f^{(j_{i,t})}-f^{(T_J)})-V_k(X_{i,t})\cdot(f^{(j)}-f^{(T_J)})&=0&&\tx{for all }j\in J_{i,t},\,j\not\in\{j_{i,t},T_J\},\tag{C1}\label{eq:generic-1}\\
		V_k(X_{i,t})\cdot(f^{(j_{i,t})}-f^{(T_J)})&=0&&\tx{if }T_J\in J_{i,t},\,T_J\ne j_{i,t}.\tag{C2}\label{eq:generic-2}
	\end{align}
	\reqnomode
	
	Next, denote
	\[
	J'_{i,t}=J_{i,t}\setminus\{j_{i,t}\},\quad\wh{J}_{i,t}=J_{i,t}\setminus\{j_{i,t},T_J\},\quad\tx{for all }(i,t)\in U.
	\]
	Now, for every \((i,t)\in U,\,j\in J'_{i,t}\), define a row vector of length \((|J|-1)k\) by 
	\begin{align*}
		R_{i,j,t}(X_{i,t})&=\begin{cases}
			\begin{bNiceMatrix}[last-row]
				0^k&\cdots&0^k&V_k(X_{i,t})&0^k&\cdots&0^k&-V_k(X_{i,t})&0^k&\cdots&0^k\\
				&&&\underset{j_{i,t}}{\uparrow}&&&&\underset{j}{\uparrow}&&&
			\end{bNiceMatrix}&\tx{if }j\ne T_J,\\[30pt]
			\begin{bNiceMatrix}[last-row]
				0^k&\cdots&0^k&V_k(X_{i,t})&0^k&\cdots&0^k&\phantom{iiiii}0^k\phantom{iiiii}\!\!&0^k&\cdots&0^k\\
				&&&\underset{j_{i,t}}{\uparrow}&&&&&&&
			\end{bNiceMatrix}&\tx{if }j=T_J.
		\end{cases}
	\end{align*}
	The \tsf{\((B,J)\)-extended RIM} is then defined by 
	\[
	\mf{M}_J^B(X_U)=\begin{bmatrix}
		R_{i,j,t}(X_{i,t})
	\end{bmatrix}_{(i,t)\in U,\,j\in J'_{i,t}}\in\mb{F}_q(X_U)^{\big(\sum_{(i,t)\in U}|J'_{i,t}|\big)\times (|J|-1)k}.
	\]
	We also denote \(\mf{M}_J(X_U)=\mf{M}_J^\emptyset(X_U)\).  Note that \(U\) depends on the choice of \(B\), and the family \(\{I_j:j\in J\}\).  In fact, it is easy to see that given the family \(\{I_1,\ldots,I_T\}\), the \((B,J)\)-extended RIM \(\mf{M}_J^B(X_U)\) is completely determined, for every \(B\subseteq[n],\,J\subseteq[T]\).
	
	\paragraph*{Number of rows and columns of the extended RIM.}  The number of columns of \(\mf{M}_J^B(X_U)\) is clearly \((|J|-1)k\).  Now, define
	\[
	\wgt(B,J)=\sum_{j\in J}|I_j|-\bigg|\bigcup_{j\in J}I_j\bigg|.
	\]
	\begin{assumption}\label{ass:J}
		For every \((i,t)\in U\), we have \(|J_{i,t}|\ge2\).
	\end{assumption}
	\noindent By the definition of the \((B,J)\)-extended RIM, the number of rows of \(\mf{M}_J^B(X_U)\) is equal to
	\begin{align}
		\sum_{(i,t)\in U}|J'_{i,t}|&=\sum_{(i,t)\in U}\big(|J_{i,t}|-1\big)\tag*{by~\cref{ass:J}}\\
		&=\sum_{j\in J}\sum_{(i,t)\in I_j}\tbf{1}(j>j_{i,t})\tag*{by double counting}\\
		&=\sum_{j\in J}|I_j|-\sum_{j\in J}\sum_{(i,t)\in I_j}\tbf{1}(j=j_{i,t})\notag\\
		&=\sum_{j\in J}|I_j|-\bigg|\bigcup_{j\in J}I_j\bigg|\notag\\
		&=\wgt(B,J).\notag
	\end{align}
	Thus, \(\wgt(B,J)\) counts the number of rows in \(\mf{M}_J^B(X_U)\).  This quantity was already defined in~\cite{shangguan-tamo-2023-list-decoding-RS-beyond-Johnson-bound} and~\cite{guo-zhang-2023-random-RS} in the context of list decoding.
	
	
	
	\subsubsection*{The general case}
	
	Here, we explain how to define the extended RIM without~\cref{ass:size,ass:J}.  For each of the assumptions, we will see what happens if the assumption is false, and then suggest minor modifications to the definition of the extended RIM. 
	\begin{enumerate}[label=(A\arabic*),ref=A\arabic*]
		\item\label{ass:extend-1}  Firstly, let us address~\cref{ass:J}.  Suppose this assumption does not hold, that is, there exists \((i,t)\in U\) such that \(|J_{i,t}|\le1\).  If \(|J_{i,t}|=1\), that is, \(J_{i,t}=\{j_0\}\) for some \(j_0\in J\), then this exactly means \(c^{(j_0)}\) is the only codeword among \(c^{(1)},\ldots,c^{(T)}\) that takes the value \(w_{i,t}\) at \(\alpha_i\).  If \(|J_{i,t}|=0\), that is, \(J_{i,t}=\emptyset\), then this exactly means there is no codeword among \(c^{(1)},\ldots,c^{(T)}\) that takes the value \(w_{i,t}\) at \(\alpha_i\).  In either cases, the value \(w_{i,t}\) does not contribute to a linear constraint of the form in~\cref{con:linear-relations}.  So, we replace the set \(S_i\) with \(S_i\setminus\{w_{i,t}\}\), which has size \(|S_i|-1\).  Further, in the case \(J_{i,t}=\{j_0\}\), we replace \(I_{j_0}\) with \(I_{j_0}\setminus\{(i,t)\}\), and \(U\) with \(U\setminus\{(i,t)\}\), and then omit the set \(J_{i,t}\) altogether. 
		
		
		\item\label{ass:extend-2}  Secondly, let us address~\cref{ass:size}.  Suppose this assumption does not hold.  In this case, in fact, the sets \(I_j\)-s, \(J_{i,t}\)-s and \(U\) can be defined very similarly, even though the \(S_i\)-s have different sizes.  For concreteness, suppose we have \(|S_i|=\ell_i\le\ell\), with each \(\ell_i\) not necessarily equal to \(\ell\).  Let us enumerate \(S_i=\{w_{i,1},\ldots,w_{i,\ell_i}\},\,i\in[n]\).  Fix some \(B\subseteq[n],\,J\subseteq[T]\), and let \(\Gamma=\bigcup_{i\in[n]\setminus B}(\{i\}\times[\ell_i])\).  For \(j\in J,\,(i,t)\in\Gamma\), define
		\[
		I(c^{(j)})=\{(i',t')\in\Gamma:c^{(j)}_{i'}=w_{i',t'}\},\quad\tx{and}\quad J_{i,t}=\{j'\in J:c^{(j)}_i=w_{i,t}\}.
		\]
		Then denote \(I_j=I(c^{(j)}),\,j\in J\), and let \(U=\bigcup_{j\in J}I_j\), as before. 
	\end{enumerate}
	
	So, now in the general case, first perform the modification in (\ref{ass:extend-1}) to each \((i,t)\in U\) that violates~\cref{ass:J}, to complete the definition of the \(I_j\)-s, \(J_{i,t}\)-s, and \(U\).  Further, if the input list \(S=(S_1,\ldots,S_n)\) violates~\cref{ass:size}, then define the \(I_j\)-s, \(J_{i,t}\)-s, and \(U\) as indicated in (\ref{ass:extend-2}).  Define \(\mf{M}_J^B(X_U)\) as before.  Note that, now we have \(|J_{i,t}|\ge2\) for all \((i,t)\in U\).  Thus, the number of rows of \(\mf{M}_J^B(X_U)\) will be equal to \(\wgt(B,J)=\sum_{j\in J}|I_j|-\big|\bigcup_{j\in J}I_j\big|\).
	
	Having completed defining the extended RIM, let us now make the following observation.
	\begin{observation}\label{obs:RIM-linear-basic}
		Consider the \((\emptyset,[T])\)-extended RIM \(\mf{M}_{[T]}(X_U)\).  Then,
		\begin{enumerate}[\normalfont(a)]
			\item  For each \(j\in[T]\), we have \(|I_j|=n(1-\msf{d}(c^{(j)},S))\).
			
			\item  The homogeneous linear system 
			\[
			\mf{M}_{[T]}(\ol{\alpha}_{U})\cdot\left[\begin{array}{@{}cc@{}}
				Y_{1,0}\!\!\!\!\\\vdots\\Y_{1,k-1}\!\!\!\!\\\hline\vdots\\\hline Y_{T-1,0}\!\!\!\!\\\vdots\\Y_{T-1,k-1}\!\!\!\!\end{array}\right]=0
			\]
			has a nontrivial solution \(Y_{j,[0,k-1]}=f^{(j)}-f^{(T)},\,j\in[T-1]\).
			
			\item  For every \((i,t)\in U\), we have \(|J_{i,t}|\ge2\).
		\end{enumerate}
	\end{observation}
	\begin{proof} \hfill
		\begin{enumerate}[(a)]
			\item  Since \(c^{(1)},\ldots,c^{(T)}\) are codewords (in \(\RS(\alpha_{[n]};k)\)), for any \(i\in[n],\,j\in[T]\), either \(c^{(j)}_i\) is a single value in \(S_i\), or \(c^{(j)}_i\) does not belong to \(S_i\).  So, for each \(j\in[T]\), we have
			\[
			|I_j|=\sum_{i\in[n]}\mbf{1}\big(c^{(j)}_i\in S_i\big)=n(1-\msf{d}(c^{(j)},S)).
			\]
			
			\item  Consider any \((i,t)\in[n]\times[\ell],\,j\in J'_{i,t}\).  We see that the constraint
			\[
			R_{i,j,t}(X_{i,t})\cdot\begin{bmatrix}
				f^{(1)}-f^{(T)}\\\vdots\\f^{(T-1)}-f^{(T)}
			\end{bmatrix}=0
			\]
			is a (\ref{eq:generic-1})-type constraint if \(j\ne T\), and a (\ref{eq:generic-2})-type constraint if \(j=T\).   Also, we have a row vector \(R_{i,j,t}(X_{i,t})\) for each (\ref{eq:generic-1})-type and (\ref{eq:generic-2})-type constraint.  Thus, the set of constraints
			\[
			\mf{M}_{[T]}(X_U)\cdot\begin{bmatrix}
				f^{(1)}-f^{(T)}\\\vdots\\f^{(T-1)}-f^{(T)}
			\end{bmatrix}=0
			\]
			is exactly the union of the sets of (\ref{eq:generic-1})-type and (\ref{eq:generic-2})-type constraints.  Each constraint in~\cref{con:generic-relations} is obtained as an appropriate linear combination of (\ref{eq:generic-1})-type and (\ref{eq:generic-2})-type constraints,  and further, after the correlated substitution
			\[
			\mf{M}_{[T]}(\ol{\alpha}_U)\cdot\begin{bmatrix}
				f^{(1)}-f^{(T)}\\\vdots\\f^{(T-1)}-f^{(T)}
			\end{bmatrix}=0,
			\]
			we get~\cref{con:linear-relations}.  Since \(c^{(1)},\ldots,c^{(T)}\) are distinct codewords in \(\RS(\alpha_{[n]};k)\), the vector \(\begin{bmatrix}
				f^{(1)}-f^{(T)}\\\vdots\\f^{(T-1)}-f^{(T)}
			\end{bmatrix}\) is, therefore, a nonzero solution of the given homogeneous linear system.
			
			\item  Immediate from the general definition of the extended RIM.\qedhere
		\end{enumerate}
	\end{proof}
	
	\subsection{Extended RIM without an input list, or a set of codewords}\label{sec:RIM-general}
	
	Now let us see that the extended RIM does not really depend on the input list \(S=(S_1,\ldots,S_n)\), or the codewords \(c^{(1)},\ldots,c^{(T)}\), but only on the resulting set systems.  We don't really need this generality in our work, but we nevertheless elaborate on this point to clearly highlight the exact dependencies of the extended RIM.
	To make things precise, we begin by defining a subset \(I\subseteq[n]\times[\ell]\) to be a \tsf{function}\footnote{We use the terminology `function' since by definition, \(I\) is the \emph{graph} of some function \(f \colon S\to[\ell]\), for some \(S\subseteq[n]\).} if we have \(t=t'\) whenever \((i,t),(i,t')\in I\).
	
	Now,let \(0\le\ell_1,\ldots,\ell_n\le\ell\), and \(T\ge1\).  Fix a subset \(B\subseteq[n],\,J\subseteq[T]\), and let \(\Gamma=\bigcup_{i\in[n]\setminus B}(\{i\}\times[\ell_i])\).  Let \(I_j\subseteq\Gamma,\,j\in J\) be functions, and \(U\coloneqq\bigcup_{j\in J}I_j\).  For every \((i,t)\in U\), define a subset \(J_{i,t}\subseteq J\) by the equivalence: \(j\in J_{i,t}\) if and only if \((i,t)\in I_j\).  By exactly following the description in~\cref{sec:RIM-RS}, this immediately gives us a set of corresponding constraints defined by~(\ref{eq:generic-1}) and~(\ref{eq:generic-2}), and therefore, completely defines the corresponding \((B,J)\)-extended RIM, that is, \(\RIM^B_J(X_U)\). 
	
	The only difference in this generality is that for distinct \(\alpha_1,\ldots,\alpha_n\in\mb{F}_q\), the correlated substitution into \(\RIM^B_J(X_U)\), while well-defined, does not mean anything; in particular, we cannot say anything about the rank of \(\RIM^B_J(\ol{\alpha}_U)\).  So~\cref{obs:RIM-linear-basic} changes to the following (with an identical proof).
	\begin{observation}\label{obs:RIM-linear-basic-general}
		Fix an integer \(T\ge1\), integers \(0\le\ell_1,\ldots,\ell_n\le\ell\), and let \(\Gamma=\bigcup_{i\in[n]}(\{i\}\times[\ell_i])\).  Let \(I_j\subseteq\Gamma,\,j\in[T]\) be functions, and \(U\coloneqq\bigcup_{j\in J}I_j\).  Consider \(\mf{M}_{[T]}(X_U)\), which is the corresponding \((\emptyset,[T])\)-extended RIM.  Let \(\alpha_1,\ldots,\alpha_n\in\mb{F}_q\) be distinct, and consider the code \(\RS(\alpha_{[n]};k)\).  Suppose there exist an input list \(S=(S_1,\ldots,S_n)\), and codewords \(c^{(1)},\ldots,c^{(T)}\in\RS(\alpha_{[n]};k)\) such that
		\begin{itemize}
			\item   \(|S_i|=\ell_i\) for all \(i\in[n]\), and
			\item   \(c^{(j)}=V_k(\alpha_{[n]})\cdot f^{(j)}\), and \(I_j=I(c^{(j)})\) for all \(j\in[T]\).
		\end{itemize}
		Then, we get the following.
		\begin{enumerate}[\normalfont(a)]
			\item  For each \(j\in[T]\), we have \(|I_j|=n(1-\msf{d}(c^{(j)},S))\).
			
			\item  The homogeneous linear system
			\[
			\mf{M}_{[T]}(\ol{\alpha}_{U})\cdot\left[\begin{array}{@{}cc@{}}
				Y_{1,0}\!\!\!\!\\\vdots\\Y_{1,k-1}\!\!\!\!\\\hline\vdots\\\hline Y_{T-1,0}\!\!\!\!\\\vdots\\Y_{T-1,k-1}\!\!\!\!
			\end{array}\right]=0
			\]
			has a nontrivial solution \(Y_{j,[0,k-1]}=f^{(j)}-f^{(T)},\,j\in[T-1]\).
		\end{enumerate}
	\end{observation}
	
	Note that~\cref{obs:RIM-linear-basic} and~\cref{obs:RIM-linear-basic-general} only pertain to \((\emptyset,[T])\)-extended RIMs.  We will be concerned with \((B,J)\)-extended RIMs, for \(B\ne\emptyset\), only in the analysis of rank of a corresponding \((\emptyset,[T])\)-extended RIM, in~\cref{sec:rank-analysis}.  Let us also note the relation between the number of rows in a \((\emptyset,J)\)-extended RIM  and a \((B,J)\)-extended RIM.
	\begin{observation}\label{obs:RIM-deleted-count}
		Consider integers \(n,\ell,T\ge1\), and functions \(I_1,\ldots,I_T\subseteq[n]\times[\ell]\).  Then, for every \(J\subseteq[T]\) and \(B\subseteq[n]\), we have
		\[
		0\le\wgt(\emptyset,J)-\wgt(B,J)\le|B||J|.
		\]
	\end{observation}
	\begin{proof}
		Fix any \(J\subseteq[T]\), and let \(U=\bigcup_{j\in J}I_j\).  Consider any \(B\subseteq[n]\), and let \(I'_j=I_j\setminus\{(i,t)\in U:i\in B\}\), for all \(j\in J\).  Since \(I_1,\ldots,I_T\) are functions, we have \(|I'_j|\ge|I_j|-|B|\), for all \(j\in J\).  Now denote \(U([n]\setminus B)\coloneqq\bigcup_{j\in J}I'_j=U\cap(([n]\setminus B)\times[\ell])\).  Note that these immediately define the \((\emptyset,J)\)-extended RIM \(\mf{M}_J(X_U)\) and the \((B,J)\)-extended RIM \(\mf{M}_J^B(X_{U([n]\setminus B)})\).  Firstly, \(\wgt(\emptyset,J)\) is the number of rows of \(\mf{M}_J(X_U)\), and \(\wgt(B,J)\) is the number of rows of \(\mf{M}_J^B(X_{U([n]\setminus B)})\).  So, we immediately have \(\wgt(B,J)\le\wgt(\emptyset,J)\).  Further, by definition, we get
		\begin{align}
			\wgt(\emptyset,J)-\wgt(B,J)&=\bigg(\sum_{j\in J}|I_j|-\bigg|\bigcup_{j\in J}I_j\bigg|\bigg)-\bigg(\sum_{j\in J}|I_j|-\bigg|\bigcup_{j\in J}I'_j\bigg|\bigg)\notag\\
			&=\sum_{j\in J}\big(|I_j|-|I'_j|\big)-\bigg(\bigg|\bigcup_{j\in J}I_j\bigg|-\bigg|\bigcup_{j\in J}I'_j\bigg|\bigg)\notag\\
			&\le\sum_{j\in J}\big(|I_j|-|I'_j|\big)\tag*{since \(I'_j\subseteq I_j\)}\\
			&\le|B||J|.\notag\qedhere
		\end{align}
	\end{proof}
	
	
	\section{Optimal List Recovery of Random Reed--Solomon Codes}\label{sec:list-recovery-RS}
	
	In this section, we will prove our main~\cref{thm:main-RS}, modulo some technical lemmas.  We will prove these lemmas later.
	
	\subsection{Optimal list recovery with high probability}\label{sec:optimal-list-recovery}
	
	We begin by recalling~\cref{thm:main-RS}, for convenience.
	
	
	
	\begin{reptheorem}[\ref{thm:main-RS}]
		For any positive integers \(n,\ell\), any small enough \(\eps>0\), and any rate \(R\in(0,1-\eps)\), the following holds for any finite field \(\mb{F}_q\) with \(q\ge\ell^{\,\Theta(\ell^2/R\eps^3)}n^2\).  With probability at least \(1-\ell^{\,-n}\) over a uniform and independent choice of \(\alpha_1,\ldots,\alpha_n\sim\mb{F}_q\), the code \(\RS(\alpha_1,\ldots,\alpha_n;Rn)\) is \[(1-R-\eps,\ell,L=O(\ell/\eps))\] list recoverable.
		In particular, the assertion holds with the field size \(q\ge n^{\Theta(1/R\eps^3)}\) whenever \(\ell = O\big(\sqrt{\log n/\log\log n}\big)\).
	\end{reptheorem}
	\noindent Note that the ``in particular'' case above follows immediately by setting the appropriate parameters.
	
	
	
	We will prove the following more general statement, for the slightly stronger notion of average radius list recovery.  This more general statement is also slightly more technical, as it explicitly involves the two slacknesses \(\lambda>0\) and \(\mu\in(0,1)\).
	\begin{theorem}\label{thm:big}
		Let \(n,\ell\ge1\) be positive integers.  Consider the finite field \(\mb{F}_q\), where \(q\) is sufficiently large.  Let \(\lambda>0,\mu\in(0,1)\), \(L\in\left[\left\lceil\frac{1+\lambda}{(1-\mu)\lambda}\right\rceil\ell-1,\ell q\right]\), and \(k\in[n]\).  If \(\rho\in(0,1)\) satisfies
		\[
		\rho\le\bigg(1-\frac{\ell}{L+1}\bigg)\bigg(1-\frac{(1+\lambda)k}{n}\bigg),
		\]
		then over a uniform and independent choice of \(\alpha_1,\ldots,\alpha_n\sim\mb{F}_q\), the code \(\RS(\alpha_{[n]};k)\) is not \((\rho,\ell,L)\)-ARLR with probability at most
		\[
		(\ell+1)^{(L+1)n}\bigg(\frac{Ln(k-1)}{q-n+1}\bigg)^{\frac{\lambda\mu k}{L+1}}.
		\]
	\end{theorem}
	We will prove~\cref{thm:big} in the following subsections.  Let us now quickly see how~\cref{thm:main-RS} follows from~\cref{thm:big}.  Note that, in fact, we will get the assertion of~\cref{thm:main-RS} for average radius list recovery, instead of list recovery. 
	
	\begin{proof}[Proof of~{\normalfont\cref{thm:main-RS}} (for average radius list recovery)]
		We obtain the required result by setting parameters appropriately in~\cref{thm:big}.
		
		Fix any constant \(\mu\in(1/2,1)\).  Let \(k=\lfloor Rn\rfloor\).  Consider any \(\eps\in(0,1-R)\), and let
		\begin{align}
			\lambda=\frac{\eps}{1-\epsilon},\quad\tx{and}\quad L=\left\lceil\frac{1+\lambda}{(1-\mu)\lambda}\right\rceil\ell-1=\left\lceil\frac{1}{(1-\mu)\epsilon}\right\rceil\ell-1.\label{eq:param}
		\end{align}
		This implies
		\begin{align}
			\bigg(1-\frac{\ell}{L+1}\bigg)\bigg(1-\frac{(1+\lambda)k}{n}\bigg)&\ge(1-(1-\mu)\eps)\bigg(1-\frac{R}{1-\eps}\bigg)\notag\\
			&=(1-\eps+\mu\eps)-\bigg(1+\frac{\mu\eps}{1-\eps}\bigg)R\notag\\
			&=(1-R-\eps)+\mu\eps\bigg(1-\frac{R}{1-\eps}\bigg)\notag\\
			&\ge1-R-\eps.\label{eq:radius}
		\end{align}
		Suppose that
		\begin{align}
			q\ge\ell^{\frac{3}{(1-\mu)^3}\frac{\ell^2}{R\eps^3}+\frac{2}{(1-\mu)^2}\frac{\ell}{R\eps^2}}\,n^2=\ell^{\Theta\big(\frac{\ell^2}{R\eps^3}\big)}\,n^2.\label{ineq:q}
		\end{align}
		Then by~(\ref{eq:param}),~(\ref{eq:radius}), and~\cref{thm:big}, for i.i.d.\ uniformly random \(\alpha_1,\ldots,\alpha_n\sim\mb{F}_q\), the code \(\RS(\alpha_{[n]};\lfloor Rn\rfloor)\) is not \((1-R-\eps,\ell,L)\)-ARLR with probability at most 
		\begin{align}
			(\ell+1)^{(L+1)n}\bigg(\frac{Ln(k-1)}{q-n+1}\bigg)^{\frac{\lambda\mu k}{L+1}}&\le\left((\ell+1)^{\left\lceil\frac{1}{(1-\mu)\eps}\right\rceil\ell+1}\left(\frac{\left\lceil\frac{1}{(1-\mu)\eps}\right\rceil\ell\cdot Rn^2}{q-n+1}\right)^{\eps^2\mu(1-\mu)\big(\frac{R}{1-\eps}\big)\cdot\frac{1}{\ell}}\right)^n\notag\\
			&\le\left((\ell+1)^{\left\lceil\frac{1}{(1-\mu)\eps}\right\rceil\ell+1}\left(\frac{\left\lceil\frac{1}{(1-\mu)\eps}\right\rceil\ell\cdot n^2}{q-n+1}\right)^{\frac{(1-\mu)^2R\eps^2}{\ell}}\right)^n\tag*{since \(\mu>1-\mu\), \(1-\eps<1\), and \(R<1\)}\\[10pt]
			&=\left(\left(\frac{(\ell+1)^{\frac{\ell}{(1-\mu)^2R\eps^2}\cdot\big(\left\lceil\frac{1}{(1-\mu)\eps}\right\rceil\ell+1\big)}\cdot \left\lceil\frac{1}{(1-\mu)\eps}\right\rceil\ell\cdot n^2}{q-n+1}\right)^{\frac{(1-\mu)^2R\eps^2}{\ell}}\right)^n\label{ineq:prob}
		\end{align}
		Now, again since \(\mu<1,\,\eps<1\), and \(\ell\ge1\), we get an upper bound
		\begin{align}
			(\ell+1)^{\frac{\ell}{(1-\mu)^2R\eps^2}\cdot\big(\left\lceil\frac{1}{(1-\mu)\eps}\right\rceil\ell+1\big)}\left\lceil\frac{1}{(1-\mu)\eps}\right\rceil\ell\n\le\n\ell^{\frac{2}{(1-\mu)^3}\frac{\ell^2}{R\eps^3}}\left\lceil\frac{1}{(1-\mu)\eps}\right\rceil\ell\n\le\n\ell^{\frac{3}{(1-\mu)^3}\frac{\ell^2}{R\eps^3}}.\label{ineq:num}
		\end{align}
		Further, by the lower bound~(\ref{ineq:q}) on \(q\), we get a lower bound
		\begin{align}
			q-n+1\ge\Big(\ell^{\frac{3}{(1-\mu)^3}\frac{\ell^2}{R\eps^3}+\frac{2}{(1-\mu)^2}\frac{\ell}{R\eps^2}}\,n^2\Big)-n+1\ge\ell^{\frac{3}{(1-\mu)^3}\frac{\ell^2}{R\eps^3}+\frac{1}{(1-\mu)^2}\frac{\ell}{R\eps^2}}\,n^2.\label{ineq:denom}
		\end{align}
		So, by~(\ref{ineq:prob}),~(\ref{ineq:num}), and~(\ref{ineq:denom}), we get
		\begin{align*}
			(\ell+1)^{(L+1)n}\bigg(\frac{Ln(k-1)}{q-n+1}\bigg)^{\frac{\lambda\mu k}{L+1}}&\le\left(\left(\frac{\ell^{\frac{3}{(1-\mu)^3}\frac{\ell^2}{R\eps^3}}n^2}{\ell^{\frac{3}{(1-\mu)^3}\frac{\ell^2}{R\eps^3}+\frac{1}{(1-\mu)^2}\frac{\ell}{R\eps^2}}\,n^2}\right)^{\frac{(1-\mu)^2R\eps^2}{\ell}}\right)^n\\
			&=\left(\left(\frac{1}{\ell^{\frac{1}{(1-\mu)^2}\frac{\ell}{R\eps^2}}}\right)^{\frac{(1-\mu)^2R\eps^2}{\ell}}\right)^n\\
			&=\bigg(\frac{1}{\ell}\bigg)^n=o(1).\tag*{\qedhere}
		\end{align*}
	\end{proof}
	\begin{remark}\label{rem:q-small}
		Note that in the case where \(R=o(1)\), the proof of~\cref{thm:main-RS} shows that the assertion is true when the field size \(q\ge\ell^{\,\Theta(\ell^2/R\eps^3)}Rn^2=\ell^{\,\Theta(\ell^2/R\eps^3)}n\cdot o(n)\), that is, \(q\) is smaller than quadratic in \(n\).  But we do not emphasize this point in the statement of~\cref{thm:main-RS}, since our priority is to understand the dependence of \(q\) on \(\ell\).
	\end{remark}
	
	We will now proceed to prove~\cref{thm:big}, modulo some technical lemmas which we shall prove later.  The proof proceeds by analyzing the extended RIM in the context of list recovery.
	
	\subsection{Extended RIM and list recovery}\label{sec:RIM-LR}
	
	We have already seen in~\cref{obs:RIM-linear-basic}(b), that any extended RIM, that is \emph{obtained from codewords}, will have nonzero kernel after the correlated substitution.  This is simply a consequence of the definition.  In the context of list recovery, in fact, we can say more; we can choose a special submatrix of the extended RIM, which is itself an extended RIM with additional properties.  The rest of the proof will only involve this \emph{smaller} extended RIM.
	
	The following lemma captures the structure of the smaller extended RIM.  This is an extension of~\cite[Lemma 3.5]{guo-zhang-2023-random-RS} to list recovery.
	\begin{lemma}\label{lem:function-family}
		Let \(\rho\in(0,1),\,\lambda>0,\,\ell\ge1,\,L\ge\ell\).  Fix distinct \(\alpha_1,\ldots,\alpha_n\in\mb{F}_q\).  Suppose the code \(\RS(\alpha_{[n]};k)\) is not \((\rho,\ell,L)\)-ARLR, where
		\begin{align}
			\rho\le\bigg(1-\frac{\ell}{L+1}\bigg)\bigg(1-\frac{(1+\lambda)k}{n}\bigg).\label{ineq:rho}
		\end{align}
		Then, there exists \(T\in[2,L+1]\), and functions \(I_j\subseteq[n]\times[\ell]\) for all \(j\in[T]\) such that the corresponding \((\emptyset,[T])\)-extended RIM satisfies the following.
		\begin{enumerate}[\normalfont(a)]
			\item  \(\ker(\mf{M}_{[T]}(\ol{\alpha}_U))\ne0\), where \(U=\bigcup_{j\in[T]}I_j\). 
			\item  \(\wgt(\emptyset,[T])\ge(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k\). 
			\item  \(\wgt(\emptyset,J)\le(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(|J|-1)k\), for all \(J\subseteq[T],\,J\ne\emptyset\).
		\end{enumerate}
	\end{lemma}
	\begin{proof}
		Since \(\RS(\alpha_{[n]};k)\) is not \((\rho,\ell,L)\)-ARLR, we have a \emph{bad} \(\ell\)-sized sequence of input lists \(S=(S_1,\ldots,S_n)\), and a \emph{bad} output list \(c^{(1)},\ldots,c^{(L+1)}\in\RS(\alpha_{[n]};k)\) such that
		\begin{align}
			\frac{1}{L+1}\sum_{j=1}^{L+1}\msf{d}(c^{(j)},S)\le\rho.\label{ineq:bad-list}
		\end{align}
		Define a family of subsets of \([L+1]\) by
		\[
		\ms{P}=\bigg\{\mc{T}\subseteq[L+1]:|\mc{T}|\ge2,\tx{ and }\wgt(\emptyset,\mc{T})\ge(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)(|\mc{T}|-1)k\bigg\}.
		\]
		We see that
		\begin{align*}
			\wgt(\emptyset,[L+1])&=\sum_{j\in[L+1]}|I_j|-\bigg|\bigcup_{j\in[L+1]}I_j\bigg|\\
			&\ge n(L+1)-n\sum_{j\in[L+1]}\msf{d}(c^{(j)},S)-n\ell\tag*{by~\cref{obs:RIM-linear-basic}(a)}\\
			&\ge n(L+1-\ell)-n(L+1)\rho\tag*{by~(\ref{ineq:bad-list})}\\
			&\ge n(L+1-\ell)-(L+1-\ell)(n-(1+\lambda)k)\tag*{by~(\ref{ineq:rho})} \\
			&=(1+\lambda)(L+1-\ell)k\\
			&=(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)Lk.
		\end{align*}
		
		
		\noindent Therefore, \([L+1]\in\ms{P}\), that is, \(\ms{P}\ne\emptyset\).  So we choose a minimal set \(\mc{T}\in\ms{P}\) with respect to inclusion, and let \(T=|\mc{T}|\ge2\).  Now, fix some bijection $\msf{r}$ mapping $[L+1]$ into $[L+1]$, such that
		\(\msf{r}([T])=\mc{T}\) and \(\msf{r}([T+1,L+1])=[L+1]\setminus\mc{T}\).  Further, denote
		\begin{align*}
			&\wt{c}^{\,(j)}=c^{(\msf{r}(j))},\quad\wt{I}_j=I_{\msf{r}(j)},&&\tx{for all }j\in[L+1],\\
			\tx{and}\quad&\wt{\wgt}(\emptyset,J)=\wgt(\emptyset,\msf{r}(J)),&&\tx{for all }J\subseteq[L+1].
		\end{align*}
		Then, we observe the following.
		\begin{enumerate}[label=(W\arabic*),ref=W\arabic*]
			\item\label{wt:obs-1}  By definition of \(\mc{T}\), we have
			\[
			\wt{\wgt}(\emptyset,[T])=\wgt(\emptyset,\mc{T})\ge(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)(T-1)k.
			\]
			\item\label{wt:obs-2}    For every \(J\subsetneq[T],\,|J|\ge2\), we obviously have \(\msf{r}(J)\subsetneq\mc{T},\,|\msf{r}(J)|=|J|\ge2\), and so by minimality in the definition of \(\mc{T}\), we have
			\[
			\wt{\wgt}(\emptyset,J)=\wgt(\emptyset,\msf{r}(J))<(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)(|J|-1)k.
			\]
			\item\label{wt:obs-3}  For every \(j\in[T]\), we have
			\[
			\wt{\wgt}(\emptyset,\{j\})=\wgt(\emptyset,\msf{r}(j))=|\wt{I}_j|-|\wt{I}_j|=0.
			\]
		\end{enumerate}
		
		Now we consider the codewords \(\wt{c}^{\,(1)},\ldots,\wt{c}^{\,(T)}\in\RS(\alpha_{[n]};k)\).  Consider the input list \(S=(S_1,\ldots,S_n)\), denote \(U=\bigcup_{j\in[T]}\wt{I}_j\), and consider the corresponding \((\emptyset,[T])\)-extended RIM \(\mf{M}_{[T]}(X_U)\).  By~\cref{obs:RIM-linear-basic}(b), we get (a).  By~(\ref{wt:obs-1}), we get (b).  By~(\ref{wt:obs-2}) and~(\ref{wt:obs-3}), we get (c).
	\end{proof}
	
	
	The property (a) guaranteed by~\cref{lem:function-family} asserts that under the full correlated substitution, the smaller extended RIM does not have full column rank.  However, we observe that prior to any substitution, this smaller extended RIM must have full column rank.  In fact, we make the stronger claim that the column rank remains full even after removal of a small fraction of rows.  The following lemma is an analogue of~\cite[Lemma 3.11]{guo-zhang-2023-random-RS}, and the only place in the whole argument where the two slacknesses \(\lambda>0\) and \(\mu\in(0,1)\) are used.  For any \(U\subseteq[n]\times[\ell]\), and \(I\subseteq[n]\), we denote \(U(I)=\{(i,t)\in U:i\in I\}\).
	\begin{lemma}\label{lem:formal-full-rank}
		Let \(\lambda>0,\mu\in(0,1),\,k\in[n],\,L\in\left[\left\lceil\frac{1+\lambda}{(1-\mu)\lambda}\right\rceil\ell-1,\ell q\right]\), and \(T\in[2,L+1]\).  Consider functions \(I_j\subseteq[n]\times[\ell],\,j\in[T]\), such that
		\begin{enumerate}[\normalfont(a)]
			\item  \(\wgt(\emptyset,[T])\ge(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k\).
			\item  \(\wgt(\emptyset,J)\le(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(|J|-1)k\), for all \(J\subseteq[T],\,J\ne\emptyset\).
		\end{enumerate}
		Let \(U=\bigcup_{j\in[T]}I_j\).  For any \(B\subseteq[n]\) with \(|B|\le\left\lfloor\frac{\lambda\mu k}{T}\right\rfloor\), we have \(\ker(\mf{M}_{[T]}^B(X_{U([n]\setminus B)}))=0\).
	\end{lemma}
	
	We will prove~\cref{lem:formal-full-rank} later in~\cref{sec:intersection}; for now, we will assume~\cref{lem:formal-full-rank} and continue towards proving~\cref{thm:big}.  The remainder of the argument involves analyzing the fall in column rank of the smaller extended RIM under the correlated substitutions.  Also, note that the assumption \(L\ge\left\lceil\frac{1+\lambda}{(1-\mu)\lambda}\right\rceil\ell\) is required for~\cref{lem:formal-full-rank}, and will not explicitly appear in the remainder of the argument.  But it will be assumed throughout, since we are assuming~\cref{lem:formal-full-rank}.
	
	\subsection{Analysis of rank of extended RIM under correlated substitutions}\label{sec:rank-analysis}
	
	Let \(\mf{M}_J^B(X_U)\) be some \((B,J)\)-extended RIM.  Fix an arbitrary total order \(\preceq\) on the rows of \(\mf{M}_J^B(X_U)\).  This immediately induces the lexicographic order (which we again denote by \(\preceq\)) on the collection of all maximal minor matrices of \(\mf{M}_J^B(X_U)\).\footnote{If the maximal minor matrices are of size \(h\times h\), and a maximal minor matrix corresponding to row indices \((u_1,\ldots,u_h)\) is denoted by \(M_{(u_1,\ldots,u_h)}\), then the \tsf{lexicographic order} is defined as follows: for distinct \((u_1,\ldots,u_h),(v_1,\ldots,v_h)\), we have \(M_{(u_1,\ldots,u_h)}\preceq M_{(v_1,\ldots,v_h)}\) if \(u_t<v_t\) where \(t=\min\{t'\in[h]:u_{t'}\ne v_{t'}\}\).  We choose the lexicographic order only for convenience.  In fact, we can choose any total order on the collection of all maximal minor matrices.}
	
	Recall that for any \(I\subseteq[n]\), we denote \(U(I)=\{(i,t)\in U:i \in I\}\).  For any \(i\in[n]\), we denote \(U([i-1])\) by \(U(\le i-1)\), and \(U([i+1,n])\) by \(U(\ge i+1)\).  Consider any distinct \(\alpha_1,\ldots,\alpha_n\in\mb{F}_q\), and for any \(I\subseteq[n]\), denote the partial assignment 
	\[
	(\ol{\alpha}_{U(I)},X_{U([n]\setminus I)})=\Big(\big(X_{i,t}=\alpha_i:i\in I,\,(i,t)\in U\big),\n X_{U([n]\setminus I)}\Big).
	\]
	Further, for any matrix \(M(X_U)\) with each entry in \(\mb{F}_q[X_U]\), and any \(i\in[n]\), we will be interested in taking the partially assigned matrix denoted by
	\begin{align*}
		&M(\ol{\alpha}_{U(\le i-1)},X_{U(i)},X_{U(\ge i+1)})\\
		&\quad\coloneqq M\Big(\big(X_{i',t}=\alpha_{i'}:i'\le i-1,\,(i',t)\in U\big),\n\big(X_{i,t}:(i,t)\in U\big),\n\big(X_{i',t}:i'\ge i+1,\,(i',t)\in U\big)\Big),\quad
	\end{align*}
	and considering its further partial assignment
	\begin{multline*}
		M(\ol{\alpha}_{U(\le i-1)},\ol{\alpha}_{U(i)},X_{U(\ge i+1)})\coloneqq \\
		M\Big(\big(X_{i',t}=\alpha_{i'}:i'\le i-1,\,(i',t)\in U\big),\n\big(X_{i,t}=\alpha_i:(i,t)\in U\big),\n\big(X_{i',t}:i'\ge i+1,\,(i',t)\in U\big)\Big).
	\end{multline*}
	Now, we say \(i\in[n]\) is a \tsf{faulty index} for \(\mf{M}_J^B(X_U)\) (with respect to \(\alpha_1,\ldots,\alpha_n\)) if we have
	\[
	M(\alpha_{U(\le i-1)},X_{U(i)},X_{U(\ge i+1)})\ne0\quad\tx{and}\quad M(\alpha_{U(\le i-1)},\alpha_{U(i)},X_{U(\ge i+1)})=0,
	\]
	where \(M(X_U)\) is the determinant of the \(\preceq\)-smallest maximal minor matrix of \(\mf{M}_J^B(X_U)\).  Note that this immediately implies that
	\begin{align*}
		M(\alpha_{U(\le i'-1)},X_{U(i')},X_{U(\ge i'+1)})\ne0&\quad\tx{for all }i'\le i,\\
		\tx{and}\quad M(\alpha_{U(\le i'-1)},\alpha_{U(i')},X_{U(\ge i'+1)})=0&\quad\tx{for all }i'\ge i+1.
	\end{align*}
	
	We can now describe how to generate a \emph{profile vector} for the fall in column rank of an extended RIM under correlated substitutions.  The profile vector is generated by Algorithm \ref{alg:algorithm} (which is similar to that given by~\cite{guo-zhang-2023-random-RS} for list decoding), and the following result captures the outputs of Algorithm \ref{alg:algorithm}.
	\begin{algorithm}
		\caption{\texttt{ColumnRankProfile}(\(I_{[T]},\alpha_{[n]},r\))}
		\label{alg:algorithm}
		\KwIn{Functions \(I_j\subseteq[n]\times[\ell],\,j\in[T]\), distinct elements \(\alpha_1,\dots,\alpha_n\in\mb{F}_q\), and \(r\in\mb{Z}^+\).}
		\KwOut{\tsf{Success}, or \tsf{Failure}, or a \emph{profile vector} \((i_1,\dots,i_r)\in [n]^r\).}  
		\BlankLine		
		Define \(U=\bigcup_{j\in[T]}I_j\).
		
		Let \(B\leftarrow\emptyset\).
		
		\For{\(j=1\) \KwTo \(r\)}{
			\uIf{$\rank(\mf{M}_{[T]}^B(X_{U([n]\setminus B)}))<(T-1)k$}{Return \tsf{Failure}. }
			\uElseIf{a faulty index \(i\in[n]\) of \(\mf{M}_{[T]}^B(X_{U([n]\setminus B)})\) exists}{
				\(i_j\leftarrow i\) and  \(B\leftarrow B\sqcup\{i\}\).
			}
			\Else{Return \tsf{Success}. }
		}
		Return \((i_1,\dots,i_r)\).
	\end{algorithm}
	\begin{proposition}\label{pro:alg-success}
		Let \(\lambda>0,\mu\in(0,1),\,k\in[n],\,L\in\left[\left\lceil\frac{1+\lambda}{(1-\mu)\lambda}\right\rceil\ell-1,\ell q\right]\), and \(T\in[2,L+1]\).  Consider functions \(I_j\subseteq[n]\times[\ell],\,j\in[T]\), such that
		\begin{enumerate}[\normalfont(a)]
			\item  \(\wgt(\emptyset,[T])\ge(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k\).
			\item  \(\wgt(\emptyset,J)\le(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(|J|-1)k\), for all \(J\subseteq[T],\,J\ne\emptyset\).
		\end{enumerate}
		Let \(U=\bigcup_{j\in[T]}I_j\).  Further, let \(r\le\left\lceil\frac{\lambda\mu k}{T}\right\rceil\).  Then, {\normalfont\texttt{ColumnRankProfile}}\((I_{[T]},\alpha_{[n]},r)\) (Algorithm~{\normalfont\ref{alg:algorithm}}) will give exactly one of the following two outputs.
		\begin{enumerate}[{\normalfont(a)}]
			\item  {\normalfont\tsf{Success}}.  In this case, \(\ker(\mf{M}_{[T]}(\ol{\alpha}_U))=0\).
			\item  A profile vector \((i_1,\ldots,i_r)\in[n]^r\).  In this case, \(i_1,\cdots,i_r\) are distinct, where for every \(j\in[r]\), the index \(i_j\) is the faulty index of \(\mf{M}_{[T]}^{B_j}(X_{U([n]\setminus B_j)})\) with \(B_j=\{i_1,\ldots,i_{j-1}\}\) being the instance of \(B\) in the \(j\)-th iteration.
		\end{enumerate}
	\end{proposition}
	\begin{proof}
		Firstly, for any \(j\in[r]\), in the \(j\)-th iteration, we have \(|B|\le j-1\le r-1\le\left\lfloor\frac{\lambda\mu k}{T}\right\rfloor\).  So by~\cref{lem:formal-full-rank}, we have \(\ker(\mf{M}_{[T]}^B(X_{U([n]\setminus B)}))=0\).  Thus Algorithm~\ref{alg:algorithm} cannot output \tsf{Failure}.
		
		Now, suppose that for some \(j\in[r]\), in the \(j\)-th iteration, Algorithm~\ref{alg:algorithm} outputs \tsf{Success}.  This means there is no faulty index for \(\mf{M}_{[T]}^B(X_{U([n]\setminus B)})\), and therefore, \(\ker(\mf{M}_{[T]}^B(\ol{\alpha}_{U([n]\setminus B)}))=0\).  Since, \(\mf{M}_{[T]}^B(\ol{\alpha}_{U([n]\setminus B)})\) is a submatrix of \(\mf{M}_{[T]}(\ol{\alpha}_U)\) obtained by removing rows, this implies \(\ker(\mf{M}_{[T]}(\ol{\alpha}_U))=0\).
		
		The remaining possibility is that Algorithm~\ref{alg:algorithm} outputs a profile vector \((i_1,\ldots,i_r)\in[n]^r\), where for every \(j\in[r]\), the index \(i_j\) is the faulty index of \(\mf{M}_{[T]}^{B_j}(X_{U([n]\setminus B_j)})\).  (Here, we denote \(B_j\) to be the instance of \(B\) in the \(j\)-th iteration.)  Let us denote \(B_j=\{i_1,\cdots,i_{j-1}\},\,j\in[r]\).  It is immediate that for any \(j\in[r]\), if \(i\in[n]\) is a faulty index of \(\mf{M}_{[T]}^{B_j}(X_{U([n]\setminus B_j)})\), then \(i\not\in B_j\).  Thus, \(i_1,\ldots,i_r\) are distinct.  This completes the proof.
	\end{proof}
	
	We now show that Algorithm~\ref{alg:algorithm} outputs any fixed profile vector with low probability.
	
	
	\begin{proposition}\label{pro:alg-random}
		Let \(\lambda>0,\mu\in(0,1),\,k\in[n],\,L\in\left[\left\lceil\frac{1+\lambda}{(1-\mu)\lambda}\right\rceil\ell-1,\ell q\right]\), and \(T\in[2,L+1]\).  Consider functions \(I_j\subseteq[n]\times[\ell],\,j\in[T]\), such that
		\begin{enumerate}[\normalfont(a)]
			\item  \(\wgt(\emptyset,[T])\ge(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k\).
			\item  \(\wgt(\emptyset,J)\le(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(|J|-1)k\), for all \(J\subseteq[T],\,J\ne\emptyset\).
		\end{enumerate}
		Let \(U=\bigcup_{j\in[T]}I_j\).  Further, let \(r\le\left\lceil\frac{\lambda\mu k}{T}\right\rceil\).  Then for any \((i_1,\ldots,i_r)\in[n]^r\) with distinct entries, we have
		\[
		\Pr_{\substack{\alpha_1,\ldots,\alpha_n\sim\mb{F}_q\\\tx{\normalfont distinct}}}\Big[(i_1,\ldots,i_r)={\normalfont\texttt{ColumnRankProfile}}(I_{[T]},\alpha_{[n]},r)\Big]\le\bigg(\frac{(T-1)(k-1)}{q-n+1}\bigg)^r.
		\]
	\end{proposition}
	\noindent Note the bound on \(r\), the length of the profile vectors, in the statement of~\cref{pro:alg-random}.  We assume this bound, since this is required in the rank analysis of the extended RIM in~\cref{lem:formal-full-rank}, and the proof of~\cref{pro:alg-random} requires~\cref{lem:formal-full-rank}. 
	\begin{proof}
		Firstly, note that the probability space is defined by the uniform distribution over all tuples \((\alpha_1,\ldots,\alpha_n)\in\mb{F}_q^n\) with distinct entries.  Now fix any \(j\in[r]\).  Let \(B_j=\{i_1,\ldots,i_{j-1}\}\).  Since \(|B_j|\le j-1\le r-1\le\left\lfloor\frac{\lambda\mu k}{T}\right\rfloor\), by~\cref{lem:formal-full-rank}, we get \(\ker(\mf{M}_{[T]}^{B_j}(X_{U([n]\setminus B_j)}))=0\).  So let \(M_j(X_{U([n]\setminus B_j)})\) be the determinant of the \(\preceq\)-smallest \((T-1)k\times(T-1)k\) nonsingular minor matrix of \(\mf{M}_{[T]}^{B_j}(X_{U([n]\setminus B_j)})\).  Define the event 
		\[
		E_j:\quad M_j(\alpha_{U(\le i_j-1)},X_{U(i_j)},X_{U(\ge i_j)})\ne0\quad\tx{and}\quad M_j(\alpha_{U(\le i_j-1)},\alpha_{U(i_j)},X_{U(\ge i_j)})=0.
		\]
		Without loss of generality, assume \(i_1<\cdots<i_r\) (otherwise, we relabel the indices).   We need to prove that
		\[
		\Pr[E_1\wedge\cdots\wedge E_r]\le\bigg(\frac{(T-1)(k-1)}{q-n+1}\bigg)^r.
		\]
		If the LHS above is zero, then we are done.  So we assume that the LHS is positive.  Denoting \(E_0 \equiv 1\), we can then write
		\[
		\Pr[E_1\wedge\cdots\wedge E_r]=\prod_{j=1}^r\frac{\Pr[E_1\wedge\cdots\wedge E_j]}{\Pr[E_1\wedge\cdots\wedge E_{j-1}]}.
		\]
		
		Fix any \(j\in[r]\).  Let
		\[
		S=\Big\{\beta=(\beta_1,\ldots,\beta_{i_j-1})\in\mb{F}_q^{i_j-1}:\Pr[(\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)})\wedge E_1\wedge\cdots\wedge E_{j-1}]>0\Big\}.
		\]
		Then, we get 
		\begin{align*}
			\frac{\Pr[E_1\wedge\cdots\wedge E_j]}{\Pr[E_1\wedge\cdots\wedge E_{j-1}]}&=\frac{\sum_{\beta\in S}\Pr[(\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)})\wedge E_1\wedge\cdots\wedge E_j]}{\sum_{\beta\in S}\Pr[(\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)})\wedge E_1\wedge\cdots\wedge E_{j-1}]}\\
			&=\frac{\sum_{\beta\in S}\Pr[(\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)})\wedge E_j]}{\sum_{\beta\in S}\Pr[\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)}]}\\
			&=\frac{\displaystyle\sum_{\beta\in S}\frac{\Pr[(\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)})\wedge E_j]}{\Pr[(\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)})]}\cdot\Pr[(\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)})]}{\sum_{\beta\in S}\Pr[\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)}]}\\
			&\le\frac{\Bigg({\displaystyle\max_{\beta\in S}\frac{\Pr[(\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)})\wedge E_j]}{\Pr[(\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)})]}}\Bigg)\cdot\sum_{\beta\in S}\Pr[(\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)})]}{\sum_{\beta\in S}\Pr[\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)}]}\\
			&=\max_{\beta\in S}\frac{\Pr[(\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)})\wedge E_j]}{\Pr[\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)}]}\\
			&=\max_{\beta\in S}\Pr[E_j\,\Big|\,\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)}].
		\end{align*}
		Now, consider any \(\beta\in S\).  It is enough to show that \[\Pr[E_j\,\Big|\,\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)}]\le\frac{(T-1)(k-1)}{q-n+1}.\]  Define \(Q(X_{U(i_j)},X_{U(>i_j)})=M_j(\beta_{U(\le i_j-1)},X_{U(i_j)},X_{U(>i_j)})\in\mb{F}_q[X_{U(i_j)},X_{U(>i_j)}]\). First, consider the following two cases.
		\begin{itemize}
			\item  If \(Q(X_{U(i_j)},X_{U(>i_j)})=0\), then it follows immediately that \(\Pr[E_j\,\Big|\,\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)}]=0\).
			\item  Consider a new indeterminate \(Y\), and let \(Q'(Y,X_{U(>i_j)})=Q(X_{U(i_j)}=Y,X_{U(>i_j)})\).  If \(Q(X_{U(i_j)},X_{U(>i_j)})\ne0\) but \(Q'(Y,X_{U(>i_j)})=0\), then this means \(M_j\big(\beta_{U(\le i_j-1)},\n X_{U(i_j)}=Y,\n X_{U(>i_j)}\big)=0\), and therefore, again \(\Pr[E_j\,\Big|\,\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)}]=0\).
		\end{itemize}
		So now, it is left to consider the case where \(Q'(Y,X_{U(>i_j)})\ne0\) as a polynomial in \(\mb{F}_q(X_{U(>i_j)})[Y]\).  Note that \(\deg_Y(Q')\le(T-1)(k-1)\).  Conditioned on \(\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)}\), the random variable \(\alpha_{i_j}\) is uniformly distributed on \(\mb{F}_q\setminus\{\beta_1,\ldots,\beta_{i_j-1}\}\).  Therefore,
		\[
		\Pr[E_j\,\Big|\,\alpha_{U(\le i_j-1)}=\beta_{U(\le i_j-1)}]=\Pr_{\alpha_{i_j}}[Q'(Y,X_{U(>i_j)})=0]\le\frac{(T-1)(k-1)}{q-i_j+1}\le\frac{(T-1)(k-1)}{q-n+1}.\qedhere
		\]
	\end{proof}
	
	\subsection{List recovery under random puncturing (proof of~\cref{thm:big})}\label{sec:proof-big-theorem}
	
	A corollary of~\cref{pro:alg-random} is that the extended RIM, under random correlated substitutions, has full column rank with high probability.
	\begin{corollary}\label{cor:ker-nonzero-random}
		Let \(\lambda>0,\mu\in(0,1),\,k\in[n],\,L\in\left[\left\lceil\frac{1+\lambda}{(1-\mu)\lambda}\right\rceil\ell-1,\ell q\right]\), and \(T\in[2,L+1]\).  Consider functions \(I_j\subseteq[n]\times[\ell],\,j\in[T]\), such that
		\begin{enumerate}[\normalfont(a)]
			\item  \(\wgt(\emptyset,[T])\ge(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k\).
			\item  \(\wgt(\emptyset,J)\le(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(|J|-1)k\), for all \(J\subseteq[T],\,J\ne\emptyset\).
		\end{enumerate}
		Let \(U=\bigcup_{j\in[T]}I_j\).  Then,
		\[
		\Pr_{\substack{\alpha_1,\ldots,\alpha_n\sim\mb{F}_q\\\tx{\normalfont distinct}}}\big[\ker(\mf{M}_{[T]}(\ol{\alpha}_U))\ne0\big]\le\bigg(\frac{(T-1)n(k-1)}{q-n+1}\bigg)^{\frac{\lambda\mu k}{T}}.
		\]
	\end{corollary}
	\begin{proof}
		By~\cref{pro:alg-success}(a), if Algorithm~\ref{alg:algorithm} returns \tsf{Success}, then \(\ker(\mf{M}_{[T]}(\ol{\alpha}_U))=0\).  Therefore, by~\cref{pro:alg-success}(b), we get
		\begin{align*}
			&\Pr_{\substack{\alpha_1,\ldots,\alpha_n\in\mb{F}_q\\\tx{\normalfont distinct}}}\big[\ker(\mf{M}_{[T]}(\ol{\alpha}_U))\ne0\big]\\[5pt]
			&=\sum_{\substack{(i_1,\ldots,i_r)\in[n]^r\\\tx{\normalfont profile vector}}}\Pr_{\substack{\alpha_1,\ldots,\alpha_n\sim\mb{F}_q\\\tx{\normalfont distinct}}}\Big[(i_1,\ldots,i_r)={\normalfont\texttt{ColumnRankProfile}}(I_{[T]},\alpha_{[n]},r)\Big]\\[5pt]
			&\le n^r\bigg(\frac{(T-1)(k-1)}{q-n+1}\bigg)^r=\bigg(\frac{(T-1)n(k-1)}{q-n+1}\bigg)^r,\qquad\tx{for any }r\le\left\lfloor\frac{\lambda\mu k}{T}\right\rfloor.
		\end{align*}
		The claim then follows by noting that the above probability is a decreasing function of \(r\).
	\end{proof}
	
	Everything is now in place to finally establish~\cref{thm:big}.  
	\begin{proof}[Proof of~{\normalfont\cref{thm:big}}]
		Recall that we are working over the finite field \(\mb{F}_q\), where \(q\) is sufficiently large, and we have parameters \(n\ge1,\,\ell\ge1,\,k\in[n],\,\lambda>0,\,\mu\in(0,1),\,L\in\left[\left\lceil\frac{1+\lambda}{(1-\mu)\lambda}\right\rceil\ell-1,\ell q\right]\), and \(\rho\in(0,1)\) such that
		\[
		\rho\le\bigg(1-\frac{\ell}{L+1}\bigg)\bigg(1-\frac{(1+\lambda)k}{n}\bigg).
		\]
		We need to bound the probability that \(\RS(\alpha_{[n]};k)\) is not \((\rho,\ell,L)\)-ARLR, over a uniform and independent choice of \(\alpha_1,\ldots,\alpha_n\sim\mb{F}_q\).
		
		Consider distinct \(\alpha_1,\ldots,\alpha_n\in\mb{F}_q\), and suppose \(\RS(\alpha_{[n]};k)\) is not \((\rho,\ell,L)\)-ARLR.  So by~\cref{lem:function-family}, there exists \(T\in[2,L+1]\), and functions \(I_j\subseteq[n]\times[\ell]\) for all \(j\in[T]\) such that the corresponding \((\emptyset,[T])\)-extended RIM satisfies the following.
		\begin{enumerate}[\normalfont(a)]
			\item  \(\ker(\mf{M}_{[T]}(\ol{\alpha}_U))\ne0\), where \(U=\bigcup_{j\in[T]}I_j\). 
			\item  \(\wgt(\emptyset,[T])\ge(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k\). 
			\item  \(\wgt(\emptyset,J)\le(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(|J|-1)k\), for all \(J\subseteq[T],\,J\ne\emptyset\).
		\end{enumerate}
		Then~\cref{cor:ker-nonzero-random} immediately gives
		\begin{align}
			\Pr_{\substack{\alpha_1,\ldots,\alpha_n\sim\mb{F}_q\\\tx{\normalfont distinct}}}\big[\ker(\mf{M}_{[T]}(\ol{\alpha}_U))\ne0\big]\le\bigg(\frac{(T-1)n(k-1)}{q-n+1}\bigg)^{\frac{\lambda\mu k}{T}}.\label{ineq:prob-ub}
		\end{align}
		Therefore, over a uniform and independent choice of \(\alpha_1,\ldots,\alpha_n\sim\mb{F}_q\), the required probability is at most the probability that the following events are simultaneously true.
		\begin{enumerate}[(P1)]
			\item  \(\alpha_1,\ldots,\alpha_n\in\mb{F}_q\) are distinct.
			\item  \(\ker(\mf{M}_{[T]}(\ol{\alpha}_U))\ne0\), where \(T\in[2,L+1]\), functions \(I_j\subseteq[n]\times[\ell],\,j\in[T]\) and \(\mf{M}_{[T]}(\ol{\alpha}_U)\) satisfy (a), (b), and (c). 
		\end{enumerate}
		
		Now, firstly, since \(q\) is sufficiently large, for any \(T\in[2,L+1]\), we have
		\[
		\frac{(T-1)n(k-1)}{q-n+1}\le\frac{Ln(k-1)}{q-n+1}<1,
		\]
		and therefore the upper bound expression in~(\ref{ineq:prob-ub}) can be further bounded above as
		\begin{align}
			\bigg(\frac{(T-1)n(k-1)}{q-n+1}\bigg)^{\frac{\lambda\mu k}{T}}&\le\bigg(\max_{T^*\in[2,L+1]}\frac{(T^*-1)n(k-1)}{q-n+1}\bigg)^{\big(\min_{T^*\in[2,L+1]}\frac{\lambda\mu k}{T^*}\big)} \nonumber \\ &=\bigg(\frac{Ln(k-1)}{q-n+1}\bigg)^{\frac{\lambda\mu k}{L+1}}.\label{ineq:monotone-T}
		\end{align}
		Further, the number of tuples \((T,I_1,\ldots,I_T)\) such that \(T\in[2,L+1]\), and \(I_j\subseteq[n]\times[\ell]\) are functions for all \(j\in[T]\), is equal to
		\begin{align}
			\sum_{T=2}^{L+1}(\ell+1)^{Tn}=\frac{(\ell+1)^{(L+2)n}-1}{(\ell+1)^n-1}-(\ell+1)^n-1\le(\ell+1)^{(L+1)n}.\label{ineq:count}
		\end{align}
		So, the required probability is at most
		\begin{align}
			\Pr[\tx{(P1)}\wedge\tx{(P2)}]&=\Pr[\tx{(P2)}\,|\,\tx{(P1)}]\cdot\Pr[\tx{(P1)}]\notag\\
			&\le\Pr[\tx{(P2)}\,|\,\tx{(P1)}]\notag\\
			&\le\sum_{\substack{\tx{tuples }(T,I_1,\ldots,I_T)\\T\in[2,L+1]\\I_j\subseteq[n]\times[\ell]\tx{ function}}}\Pr_{\substack{\alpha_1,\ldots,\alpha_n\sim\mb{F}_q\\\tx{\normalfont distinct}}}\big[\ker(\mf{M}_{[T]}(\ol{\alpha}_U))\ne0\big]\notag\\
			&=\sum_{\substack{\tx{tuples }(T,I_1,\ldots,I_T)\\T\in[2,L+1]\\I_j\subseteq[n]\times[\ell]\tx{ function}}}\bigg(\frac{(T-1)n(k-1)}{q-n+1}\bigg)^{\frac{\lambda\mu k}{T}}\tag*{by~(\ref{ineq:prob-ub})}\\
			&\le(\ell+1)^{(L+1)n}\bigg(\frac{Ln(k-1)}{q-n+1}\bigg)^{\frac{\lambda\mu k}{L+1}}.\tag*{by~(\ref{ineq:monotone-T}) and~(\ref{ineq:count})}
		\end{align}
		This completes the proof.
	\end{proof}
	
	
	\subsection{Full-rankness of extended RIM before correlated substitutions}\label{sec:intersection}
	
	Let us now complete the only pending argument, which is the proof of~\cref{lem:formal-full-rank}.  This argument is again similar to that in~\cite{guo-zhang-2023-random-RS}, but generalized to the extended RIM.
	
	We will need a few results from previous literature about intersection dimensions of column subspaces of a matrix.  For any matrix \(M=\begin{bmatrix}
		M_1&\cdots&M_K
	\end{bmatrix}\in\mb{F}^{N\times K}\) over a field \(\mb{F}\), and any subset \(A\subseteq[N]\), we denote \(M_A\coloneqq\begin{bmatrix}
		M_i
	\end{bmatrix}_{i\in A}\in\mb{F}^{N\times|A|}\), and \(\Im(M_A)\coloneqq\spn_{\mb{F}}\{M_i:i\in A\}\).
	\begin{theorem}[{\cite{brakensiek-gopi-makam-2022-MR-tensor,tian-2019-intersection-dimension}}]\label{thm:M-int-dim}
		Consider a field \(\mb{F}\), and let \(K\in[N]\).  For any matrix \(M\in\mb{F}^{N\times K}\), and subsets \(A_1,\ldots,A_T\subseteq[N]\), where \(T\ge2\), we have
		\[
		\dim\bigg(\bigcap_{j=1}^T\Im(M_{A_j})\bigg)=\sum_{j=1}^T\dim(\Im(M_{A_j}))-\rank\begin{bmatrix}
			M_{A_1}&-M_{A_2}&&&\\
			M_{A_1}&&-M_{A_3}&&\\
			\vdots&&&\ddots&\\
			M_{A_1}&&&&-M_{A_T}
		\end{bmatrix}.
		\]
	\end{theorem}
	\begin{theorem}[{\cite{brakensiek-gopi-makam-2023-random-RS}}]\label{thm:vandermonde-int-dim}
		Consider the field \(\mb{F}_q(X_{[N]})\), and let \(K\in[N]\).  For a generic Vandermonde matrix \(V_K(X_{[N]})\), and subsets \(A_1,\ldots,A_T\subseteq[N]\) with \(|A_j|\le K\), we have 
		\[
		\dim\bigg(\bigcap_{j=1}^T\Im((V_K)_{A_j})\bigg)=\max_{\substack{P_1\sqcup\cdots\sqcup P_\nu=[T]\\\tx{\normalfont partition}}}\bigg(\sum_{i=1}^\nu\bigg|\bigcap_{j\in P_i}A_j\bigg|-(\nu-1)k\bigg).
		\]
	\end{theorem}
	
	Note that the generic Vandermonde matrix \(V_K(X_{[N]})\) is the generator matrix of the generic RS code \(\RS(X_{[N]};K)\) over the field \(\mb{F}_q(X_{[N]})\).  Further, recall that a parity check matrix of \(\RS(X_{[N]};K)\) is
	\[
	H_K(X_{[N]})\coloneqq\diag\bigg(\prod_{a'\in[N]\setminus\{a\}}\frac{1}{X_a-X_{a'}}:a\in[N]\bigg)\cdot V_{N-K}(X_{[N]})\in\mb{F}_q(X_{[N]})^{N\times(N-K)}.
	\]
	Thus, \(H_K(X_{[N]})\cdot V_K(X_{[N]})=0\).  Moreover,~\cref{thm:vandermonde-int-dim} immediately implies the following.
	\begin{corollary}\label{cor:parity-int-dim}
		For sets \(A_1,\ldots,A_T\subseteq[N]\) with \(|A_j|\le N-K\), we have
		\[
		\dim\bigg(\bigcap_{j=1}^T\Im(H_K(X_{A_j}))\bigg)=\max_{\substack{P_1\sqcup\cdots\sqcup P_\nu=[T]\\\tx{\normalfont partition}}}\bigg(\sum_{i=1}^\nu\bigg|\bigcap_{j\in P_i}A_j\bigg|-(\nu-1)(N-K)\bigg).
		\]
	\end{corollary}
	
	Towards proving~\cref{lem:formal-full-rank}, we will need a technical lemma, which is an analogue of~\cite[Lemma 3.10]{guo-zhang-2023-random-RS}.  This is a sort of preprocessing step, where we show that the kernel of the extended RIM can be embedded into the kernel of a \emph{nicer} matrix.
	\begin{lemma}\label{lem:ker-embed}
		Consider functions \(I_j\subseteq[n]\times[\ell]\) for \(j\in[T]\), let \(U=\bigcup_{j\in[T]}I_j\), and consider the corresponding \((\emptyset,[T])\)-extended RIM \(\mf{M}_{[T]}(X_U)\).  Further, let \(A_j=U\setminus I_j\) for \(j\in[T]\), and let \(k\le|U|\).  Define a matrix
		\[
		M(X_U)=\begin{bmatrix}
			H_k(X_{A_1})&-H_k(X_{A_2})&&\\
			\vdots&&\ddots&\\
			H_k(X_{A_1})&&&-H_k(X_{A_T})
		\end{bmatrix}.
		\]
		Then, there exists a linear map \(\mb{F}_q(X_U)^{(T-1)k}\to\mb{F}_q(X_U)^{\sum_{j=1}^T|A_j|}\) that maps \(\ker(\mf{M}_{[T]}(X_U))\) injectively to \(\ker(M(X_U))\).
	\end{lemma}
	\begin{proof}
		Denote \(\mb{F}=\mb{F}_q(X_U)\).  Consider any \((i,t)\in U\).  Recall that we have the subset \(J_{i,t}\subseteq[T]\) defined by the equivalence: \(j\in J_{i,t}\) if and only if \((i,t)\in I_j\).  Further, we have \(j_{i,t}=\min(J_{i,t})\).  Define a linear map \(\phi_{i,t}\colon\mb{F}^{(T-1)k}\to\mb{F}\) by
		\[
		y\coloneqq(y_1,\ldots,y_{T-1})\quad\longmapsto\quad\phi_{i,t}(y)\coloneqq V_k(X_{i,t})\cdot y_{j_{i,t}}.\phantom{aaaaaaaaa}
		\]
		
		Now, for any \(A\subseteq U\), denote \(\phi_A=(\phi_{i,t}:(i,t)\in A)\).  Define a linear map \(\psi\colon\mb{F}^{(T-1)k}\to\mb{F}^{\sum_{j=1}^T|A_j|}\) by
		\[
		y\coloneqq(y_1,\ldots,y_{T-1})\quad\longmapsto\quad\psi(y)\coloneqq\begin{bmatrix}
			\phi_{A_1}(y)-V_k(X_{A_1})\cdot y_1\\
			\vdots\\
			\phi_{A_T}(y)-V_k(X_{A_T})\cdot y_T
		\end{bmatrix},
		\]
		where we denote \(y_T=0\).
		
		Consider any \(y\coloneqq(y_1,\ldots,y_{T-1})\in\ker(\mf{M}_{[T]}(X_U))\), and again, denote \(y_T=0\).  Then,
		\begin{align}
			&&\phi_{i,t}(y)-V_k(X_{i,t})\cdot y_j&=V_k(X_{i,t})\cdot(y_{j_{i,t}}-y_j)=0,&&\tx{for all }(i,t)\in I_j,\,j\in[T]\notag\\
			\implies&&\phi_{I_j}(y)-V_k(X_{I_j})\cdot y_j&=0,&&\tx{for all }j\in[T].\label{eq:diff-phi}
		\end{align}
		Note that \(H_k(X_U)\cdot V_k(X_U)=0\).  So, for any \(j\in[T]\), we get that
		\begin{align*}
			H_k(X_{A_j})\cdot\big(\phi_{A_j}-V_k(X_{A_j})\cdot y_j\big)&=H_k(X_U)\cdot\big(\phi_U(y)-V_k(X_U)\cdot y_j\big)-H_k(X_{I_j})\cdot\big(\phi_{I_j}(y)-V_k(X_{I_j})\cdot y_j\big)\\
			&=H_k(X_U)\cdot\phi_U(y).
		\end{align*}
		This implies
		\begin{align*}
			M(X_U)\cdot\psi(y)&=H_k(X_{A_1})\cdot\big(\phi_{A_1}(y)-V_k(X_{A_1})\cdot y_j\big)-H_k(X_{A_j})\cdot\big(\phi_{A_j}(y)-V_k(X_{A_j})\cdot y_j\big)\\
			&=H\cdot\phi(y)-H\cdot\phi(y)=0.
		\end{align*}
		Thus, \(\ker(\mf{M}_{[T]}(X_U))\subseteq\ker(M(X_U))\).
		
		Now, suppose \(\psi(y)=0\).  Also, note that we have \(y_T=0\).  So, for every \(j\in[T]\), we have \(\phi_{A_j}-V_k(X_{A_j})\cdot y_j=0\), and combined with~(\ref{eq:diff-phi}), this implies \(V_k(X_U)\cdot y_j=\phi_U(y)=V_k(X_U)\cdot y_T=0\).  Since \(V_k(X_U)\) has full column rank, we get \(y=0\).  This completes the proof.
	\end{proof}
	
	We are now ready to prove~\cref{lem:formal-full-rank}.
	\begin{proof}[Proof of~{\normalfont\cref{lem:formal-full-rank}}]
		Since \(L\ge\left\lceil\frac{1+\lambda}{(1-\mu)\lambda}\right\rceil\ell-1\ge\big(\frac{1+\lambda}{(1-\mu)\lambda}\big)(\ell-1)\), we immediately get the inequality
		\begin{align}
			(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)k-\lambda\mu k=k+\frac{(1-\mu)\lambda Lk-(1+\lambda)(\ell-1)k}{L}\ge k.\label{eq:L-lb}
		\end{align}
		For every \(j\in[T]\), let \(I'_j=I_j\setminus U(B)\). So, we have \(U'\coloneqq U([n]\setminus B)=\bigcup_{j\in[T]}I'_j\).  Note that \(I'_1,\ldots,I'_T\subseteq([n]\setminus B)\times[\ell]\) are functions, and along with \(U'\), they define the corresponding \((B,J)\)-extended RIM \(\mf{M}_{[T]}^B(X_{U'})\).  Then by definition, for any \(J\subseteq[T],\,J\ne\emptyset\), we have
		\[
		\wgt(B,J)=\sum_{j\in J}|I'_j|-\bigg|\bigcup_{j\in J}I'_j\bigg|.
		\]
		Also note that, by the given conditions, we have
		\begin{enumerate}[label=(T\arabic*),ref=T\arabic*]
			\item\label{ineq:T-1}  \(\wgt(\emptyset,[T])\ge(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k\).
			\item\label{ineq:T-2}  \(\wgt(\emptyset,J)\le(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(|J|-1)k\), for all \(J\subseteq[T],\,J\ne\emptyset\).
		\end{enumerate}
		The following observations are then immediate.
		\begin{enumerate}[label=(WB\arabic*),ref=WB\arabic*]
			\item\label{ineq:wt-1}  By~\cref{obs:RIM-deleted-count} and~(\ref{ineq:T-1}), we have
			\[
			\wgt(B,[T])\ge\wgt(\emptyset,[T])-|B|T\ge(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)(T-1)k-\lambda\mu k.
			\]
			\item\label{ineq:wt-2}  By~\cref{obs:RIM-deleted-count} and~(\ref{ineq:T-2}), for any \(J\subseteq[T],\,J\ne\emptyset\), we have
			\[
			\wgt(B,J)\le\wgt(\emptyset,J)\le(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)(|J|-1)k.
			\]
		\end{enumerate}
		\noindent So, for every \(j\in[T]\), we have 
		\begin{align}
			|I'_j|&=\sum_{m\in[T]}|I'_m|-\sum_{m\in[T]\setminus\{j\}}|I'_m|\notag\\
			&\ge\bigg(\sum_{m\in[T]}|I'_m|-\sum_{m\in[T]\setminus\{j\}}|I'_m|\bigg)-\bigg(\bigg|\bigcup_{m\in[T]}I'_m\bigg|-\bigg|\bigcup_{m\in[T]\setminus\{j\}}I'_m\bigg|\bigg)\notag\\
			&=\bigg(\sum_{m\in[T]}|I'_m|-\bigg|\bigcup_{m\in[T]}I'_m\bigg|\bigg)-\bigg(\sum_{m\in[T]\setminus\{j\}}|I'_m|-\bigg|\bigcup_{m\in[T]\setminus\{j\}}I'_m\bigg|\bigg)\notag\\
			&=\wgt(B,[T])-\wgt(B,[T]\setminus\{j\})\notag\\
			&\ge(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)k-\lambda\mu k\tag*{by~(\ref{ineq:wt-1}) and~(\ref{ineq:wt-2})}\\
			&\ge k.\tag*{by~(\ref{eq:L-lb})}
		\end{align}
		Now, for each \(j\in[T]\), let \(A'_j=U'\setminus I'_j\), which means \(|A'_j|\le|U'|-k\).  By~\cref{thm:M-int-dim} and~\cref{cor:parity-int-dim} (applied with \(A_j=A'_j\) for \(j\in[T]\), \(N=|U'|\), \(K=k\), and \(U'\) identified with \([N]\)), we have
		\begin{align}
			\rank(M(X_{U'}))=\sum_{j\in[T]}\dim(\Im(H_k(X_{A'_j})))-\max_{\substack{P_1\sqcup\cdots\sqcup P_\nu=[T]\\\tx{partition}}}\bigg(\sum_{r=1}^\nu\bigg|\bigcap_{j\in P_r}A'_j\bigg|-(\nu-1)( |U'|-k)\bigg).\label{eq:rank-M}
		\end{align}
		Consider any partition \(P_1\sqcup\cdots\sqcup P_\nu=[T]\).  If \(\nu=1\), then
		\[
		\bigg|\bigcap_{j\in[T]}A'_j\bigg|= |U'|-\bigg|\bigcup_{j\in[T]}I'_j\bigg|=0.
		\]
		Now suppose \(\nu\ge2\).  Then, we get 
		\begin{align}
			\sum_{r=1}^\nu\bigg|\bigcap_{j \in P_r}A'_j\bigg|&=\sum_{r=1}^\nu\bigg( |U'|-\bigg|\bigcup_{j\in P_r}I'_j\bigg|\bigg)\notag\\
			&=\nu|U'|+\sum_{r=1}^\nu\bigg(\wgt(B,P_r)-\sum_{j\in P_r}|I'_j|\bigg)\notag\\
			&\le\nu|U'|+(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)\sum_{r=1}^\nu(|P_r|-1)k-\sum_{r=1}^\nu\sum_{j\in P_r}|I'_j|\tag*{by~(\ref{ineq:wt-2})}\\
			&=\nu|U'|+(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)(T-\nu)k-\sum_{j\in[T]}|I'_j|\tag*{since \(P_1\sqcup\cdots\sqcup P_\nu=[T]\)}\\
			&=\nu|U'|+(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)(T-\nu)k-\bigg(\wgt(B,[T])+\bigg|\bigcup_{j\in[T]}I'_j\bigg|\bigg)\notag\\
			&=\nu|U'|+(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)(T-\nu)k-\wgt(B,[T])-|U'|\tag*{since \(U'=\bigcup_{j\in[T]}I'_j\)}\\
			&\le(\nu-1)|U'|+(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)(T-\nu)k-(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)(T-1)k+\lambda\mu k\tag*{by~(\ref{ineq:wt-1})}\\
			&\le(\nu-1)|U'|-(\nu-1)(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)k+\lambda\mu k\notag\\
			&\le(\nu-1)|U'|-(\nu-1)(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)k+(\nu-1)\lambda\mu k\tag*{since \(\nu\ge2\)}\\
			&\le(\nu-1)\bigg(|U'|-(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)k+\lambda\mu k\bigg)\notag\\
			&\le(\nu-1)(|U'|-k)\tag*{by~(\ref{eq:L-lb})}
		\end{align}
		This concludes that
		\[
		\max_{\substack{P_1\sqcup\cdots\sqcup P_\nu=[T]\\\tx{partition}}}\bigg(\sum_{r=1}^\nu\bigg|\bigcap_{j\in P_r}A'_j\bigg|-(\nu-1)( |U'|-k)\bigg)\le0,
		\]
		and therefore by~(\ref{eq:rank-M}), we get
		\[
		\rank(M(X_{U'}))\ge\sum_{j\in[T]}\dim(\Im(H_k(X_{A'_j})))-0=\sum_{j\in[T]}|A'_j|,
		\]
		since \(V_{|U'|-k}(X_{U'})\), and hence \(H_k(X_{U'})\) are MDS matrices.  But the number of columns of \(M(X_{U'})\) is equal to \(\sum_{j\in[T]}|A'_j|\).  This means \(M(X_{U'})\) has full column rank, that is, \(\ker(M(X_{U'}))=0\).  Then by~\cref{lem:ker-embed}, we conclude that \(\ker(\mf{M}_{[T]}(X_{U'}))=0\).
	\end{proof}
	
	
	%	\section{Optimal list recovery in the \(\mc{L}_1\)-model}\label{sec:L1}
	%	
	%	In this section, we will prove~\cref{thm:main-RS-L1} in the \(\mc{L}_1\)-model.  Let us recall the statement, for convenience.
	%	\begin{reptheorem}[\ref{thm:main-RS-L1}]
		%		Let \(n,\ell\to\infty\) be positive integer parameters.  Let \(\eps>0\) be fixed, but sufficiently small.  Let \(R\in(0,1)\) be a parameter, and consider the finite field \(\mb{F}_q\), with \(q\ge\binom{n+\ell}{\ell}^{(1/n)\cdot\Theta(\ell^2/\eps)}Rn^2\).
		%		\begin{enumerate}[{\normalfont(a)}]
			%			\item  {\normalfont\tsf{Low-rate RS codes:}}  If \(R=o(1)\), then for i.i.d. uniformly random \(\alpha_1,\ldots,\alpha_n\in\mb{F}_q\), the code \(\RS(\alpha_1,\ldots,\alpha_n;Rn)\) is \((1-\eps,\ell,O(\ell/\eps))\mhyphen\mc{L}_1\)-LR with probability \(1-o(1)\).
			%			
			%			\item  {\normalfont\tsf{Constant-rate RS codes:}}  If \(R=\Theta(1)\), then for i.i.d. uniformly random \(\alpha_1,\ldots,\alpha_n\in\mb{F}_q\), the code \(\RS(\alpha_1,\ldots,\alpha_n;Rn)\) is \((1-R-\eps,\ell,O(\ell/\eps))\mhyphen\mc{L}_1\)-LR with probability \(1-o(1)\).
			%		\end{enumerate}
		%		In particular, the assertions {\normalfont(a)} and {\normalfont(b)} hold with the field size
		%		\[
		%		q\ge\begin{cases}
			%			n^{\Theta(1/\eps)}&\tx{if }\ell\le O(n^{1/3}),\\
			%			2^{\Theta(\ell^2/\eps)}Rn^2&\tx{if }\omega(n^{1/3})\le\ell\le O(n),\\
			%			(\ell/n)^{\Theta(\ell^2/\eps)}&\tx{if }\ell\ge\omega(n).
			%		\end{cases}
		%		\]
		%	\end{reptheorem}
	%	\noindent Note that the particular cases follow immediately by setting appropriate absolute constants.
	%	
	%	We will need the following binomial estimates.
	%	\begin{observation}
		%		Let \(n,\ell\to\infty\) be positive integer parameters.  Then
		%		\[
		%		\binom{n+\ell}{\ell}=\begin{cases}
			%			2^{(1+o(1))\ell\log(1+\frac{n}{\ell})}&\tx{if }\ell=o(n),\\
			%			2^{(1+o(1))n\log(1+\frac{\ell}{n})}&\tx{if }\ell=\omega(n),\\
			%			2^{(1+o(1))nH\big(\frac{C}{C+1}\big)}&\tx{if }\ell=Cn.
			%		\end{cases}
		%		\]
		%	\end{observation}
	
	
	%	We are now nearly ready to prove our main theorem.  We begin with an estimate on the number of restricted families of functions.
	%	\begin{lemma}\label{lem:function-count}
		%		Fix \(\lambda>0,\mu\in(0,1)\).  Assume \(n\le q\) with \(n\to\infty\), and \(L\in\left[\big(\frac{1+\lambda}{(1-\mu)\lambda}\big)(\ell-1),\ell q\right]\) with \(\ell=\ell(n)\to\infty\).  Then the number of tuples \((T,I_1,\ldots,I_T)\), where \(T\in[2,L+1]\), and \(I_1,\ldots,I_T\subseteq[n]\times[\ell]\) are functions satisfying \(\wgt(\emptyset,[T])\ge(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k\), is at most
		%		\[
		%		(\ell+1)^{(L+1)n}-\bigg(\frac{\ell}{(1+\lambda)\frac{k}{n}}\bigg)^{(1+\lambda\mu)Lk-1}.
		%		\]
		%	\end{lemma}
	%	\begin{proof}
		%		Notice that we get an upper bound by simply counting the number of families of functions \(I_{[T]}\) satisfying \(\sum_{j=1}^T|I_j|\ge(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k\).  For any such family \(I_{[T]}\), define a vector \(v_j=(v_{j,1},\ldots,v_{j,n})\in[0,\ell]^n\) by
		%		\[
		%		v_{j,i}=\begin{cases}
			%			t\in[\ell]&\tx{if }(i,t)\in I_j,\\
			%			0&\tx{if }I_j\cap\{(i,t):t\in[\ell]\}=\emptyset,\quad\tx{for all }i\in[n].
			%		\end{cases}
		%		\]
		%		Further, we denote \(v=(v_1,\ldots,v_T)\in[0,\ell]^{Tn}\), which implies \(|\supp(v)|=\sum_{j=1}^T|I_j|\).  Therefore, we need to obtain an upper bound on the size of the collection
		%		\[
		%		V\coloneqq\bigg\{(v_1,\ldots,v_T)\in[0,\ell]^{Tn}:|\supp(v)|\ge(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)(T-1)k\bigg\}.
		%		\]
		%		Let
		%		\[
		%		V^c\coloneqq[0,\ell]^{Tn}\setminus V=\bigg\{(v_1,\ldots,v_T)\in[0,\ell]^{Tn}:|\supp(v)|<(1+\lambda)\bigg(\frac{L+1-\ell}{L}\bigg)(T-1)k\bigg\}.
		%		\]
		%		Then we get
		%		\begin{align*}
			%			|V^c|&=\sum_{u=0}^{(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k-1}\binom{Tn}{u}\ell^{\,u}\\
			%			&\ge\binom{Tn}{(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k-1}\cdot\ell^{\,(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k-1}\\
			%			&\ge\bigg(\frac{Tn}{(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k-1}\bigg)^{(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k-1}\cdot\ell^{\,(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k-1}\\
			%			&=\bigg(\frac{\ell}{(1+\lambda)\big(\frac{L+1-\ell}{L}\big)\big(\frac{T-1}{T}\big)\big(\frac{k}{n}\big)-1}\bigg)^{(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k-1}\\
			%			&\ge\bigg(\frac{\ell}{(1+\lambda)\frac{k}{n}}\bigg)^{(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k-1}
			%		\end{align*}
		%		So the total count is at most
		%		\begin{align*}
			%			&\phantom{=}\,\sum_{T=2}^{L+1}\bigg((\ell+1)^{Tn}-\bigg(\frac{\ell}{(1+\lambda)\frac{k}{n}}\bigg)^{(1+\lambda)\big(\frac{L+1-\ell}{L}\big)(T-1)k-1}\bigg)\\
			%			&=(\ell+1)^{2n}\bigg(\frac{(\ell+1)^{Ln}-1}{(\ell+1)^n-1}\bigg)-\bigg(\frac{\ell}{(1+\lambda)\frac{k}{n}}\bigg)^{(1+\lambda)\big(\frac{L+1-\ell}{L}\big)k-1}\left(\frac{\Big(\frac{\ell}{(1+\lambda)\frac{k}{n}}\Big)^{(1+\lambda)\big(\frac{L+1-\ell}{L}\big)Lk}-1}{\Big(\frac{\ell}{(1+\lambda)\frac{k}{n}}\Big)^{(1+\lambda)\big(\frac{L+1-\ell}{L}\big)k}-1}\right)\\
			%			&\le(\ell+1)^{(L+1)n}-\bigg(\frac{\ell}{(1+\lambda)\frac{k}{n}}\bigg)^{(1+\lambda\mu)Lk-1}.\qedhere
			%		\end{align*}
		%	\end{proof}
	
	\raggedright
	\bibliographystyle{alpha}
	\bibliography{references}
	
	%\appendix
	%\appendixpage
	%\addappheadtotoc
	
\end{document}
