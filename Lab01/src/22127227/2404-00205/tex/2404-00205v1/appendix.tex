\appendix

\section{Examples and More Analysis}
\label{sec:more-analysis}


\begin{table*}[ht]
\centering
\small
\begin{tabular}{p{0.08\linewidth} | p{0.9\linewidth}}
\toprule
Question & Would students at Marist have to petition to get a rowing team? \\
Abstract & Would students at School X have to petition to get a Sports Team Y? \\
Sim.Q 1 & Would students at Yale University have to petition to get a cheerleading squad? \\
Sim.Q 2 & Would students at NYU have to petition to get a Lacrosse Team? \\
Program 1 (20\% Acc) & \begin{python}
def students_preferences(school: str, sport: str) -> bool:
	question = f"Would students at {school} prefer to have a {sport} sports team?"
	preferences = ask_gpt(question, bool)
	return preferences
def school_budget(school: str) -> int:
	question = f"What is the budget of {school}?"
	budget = ask_gpt(question, int)
	return budget
def answer(school_x: str, sports_team_y: str):
	preferences = students_preferences(school_x, sports_team_y)
	if preferences:
		budget = school_budget(school_x)
		if budget >= 5000000:
			return "Must be yes"
	return "Must be no"
\end{python} \\
Program 2 (100\% Acc) & \begin{python}
def has_sports_team(school: str, sport: str) -> bool:
	question = f"Does {school} have a {sport} team?"
	team_exists = ask_gpt(question, bool)
	return team_exists
def popularity_of_sport(sport: str) -> str:
	question = f"How popular is the sport {sport}?"
	popularity = ask_gpt(question, str)
	return popularity
def answer(school_x: str, sports_team_y: str):
	if has_sports_team(school_x, sports_team_y):
		return "Must be no"
	school_popularity = popularity_of_sport(sports_team_y)
	if eq_override(school_popularity, 'very popular'):
		return "Must be yes"
	return "Must be no"
\end{python} \\
\bottomrule
\end{tabular}
\caption{An example showing how generated similar questions (with 7 other questions omitted) are helpful in identifying a better candidate program. The top program does not first check if the school already has the sports team, which would fail on the two similar questions as shown. The bottom program checks it achieves better accuracy on similar questions, and eventually gets the correct answer on the original question when executed with corresponding parameters.}
\label{tab:p8}
\end{table*}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{p{0.08\linewidth} | p{0.9\linewidth}}
\toprule
Question & Did any country in Portuguese Colonial War take a general neutral role in WWII? \\
Abstract & Did any country in War X take a general neutral role in War Y? \\
Program & \begin{python}
def participating_countries_in_war(war: str) -> list:
	question = f"Which countries were involved in {war}?"
	countries = ask_gpt(question, list)
	return countries
def answer(war_x: str, war_y: str):
	war_x_countries = participating_countries_in_war(war_x)
	war_y_countries = participating_countries_in_war(war_y)
	for country_x in war_x_countries:
		if country_x in war_y_countries:
			return "Must be yes"
	return "Must be no"
\end{python} \\
Self-refined & \begin{python}
def participating_countries_in_war(war: str) -> list:
	question = f"Which countries were involved in {war}?"
	countries = ask_gpt(question, list)
	return countries
def neutral_country_in_war(country: str, war: str) -> bool:
	question = f"Was {country} neutral in {war}?"
	neutral = ask_gpt(question, bool)
	return neutral
def answer(war_x: str, war_y: str):
	war_x_countries = participating_countries_in_war(war_x)
	war_y_countries = participating_countries_in_war(war_y)
	for country_x in war_x_countries:
		if (in_override(country_x, war_y_countries) and neutral_country_in_war(country_x, war_y)):
			return "Must be yes"
	return "Must be no"
\end{python} \\
\bottomrule
\end{tabular}
\caption{An example generated program and self-refined program. The original candidate's solution is incorrect because it does not check the countries' neutrality. The self-refined program fixes this mistake based on generated similar questions and their silver labels. }
\label{tab:p9}
\end{table*}


In this section, we show a few examples of generated programs in our symbolic solution space. In Table~\ref{tab:p8}, we show that generated similar questions are helpful in selecting better programs, which leads to better overall performances in the ``+Selection'' setting of Table~\ref{tab:perf-ours}. In Table~\ref{tab:p9}, we show the effectiveness of self-refinement.

\section{Prompts}
\label{sec:prompts}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{p{0.08\linewidth} | p{0.9\linewidth}}
\toprule
Prompt & Content \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}
system & Identify named entities, special nouns, or numerical values in the given question, then replace some of them with appropriate semantic types, so that the resulting abstract question is still answerable with the same general solution as the original question. Follow the the provided examples. \\
user & Does Rusev have to worry about human overpopulation in his homeland? \\
assistant & We first identify all named entities, special nouns and numerical values: Rusev, human overpopulation, his homeland. In order to keep the question answerable, we can only replace Rusev with Person X, because the original question's solution depends on knowing the issue in consideration is human overpopulation. Moreover, replacing "his homeland" will lead to too much ambiguity. As a result, we can replace "Rusev" to "Person X" (person$\_$x: str) So the question becomes Does Person X have to worry about human overpopulation in his homeland? With parameters person$\_$x="Rusev" \\
user & Can the Very Large Telescope observe the largest mountain on Earth? \\
assistant & We first identify all named entities, special nouns and numerical values: the Very Large Telescope, the largest mountain on Earth. We can replace these entities with fine-grained semantic types that keep the question answerable. The original question's general solution works as long as it knows that the Very Large Telescope is a Telescope on Earth, and the object in question is a geographical feature also on Earth.  As a result, we can replace "the Very Large Telescope" to "Telescope X" (telescope$\_$x: str) "the largest mountain on Earth" to "Geographical Feature Y" (geo$\_$feature$\_$y: str) So the question becomes Can Telescope X on Earth observe Geographical Feature Y on Earth? With parameters telescope$\_$x="Very Large Telescope", geo$\_$feature$\_$y="the largest mountain on Earth" \\
user & Will the Albany in Georgia reach a hundred thousand occupants before the one in New York? \\
assistant & We first identify all named entities, special nouns and numerical values: Albany in Georgia, a hundred thousand, the one in New York. The original question's solution can be generalized to checking if a city will reach a specific population before another city. This means that we can conceptualize the two cities in the original question with "City", and the number as Population. As a result, we can replace "Albany in Georgia" to "City X" (city$\_$x: str) "a hundred thousand occupants" to "Population Y" (population$\_$y: int) "the one in New York" to "City Z" (city$\_$z: str) So the question becomes Will City X reach Population Y before City Z? With parameters city$\_$x="Albany, Georgia", population$\_$y=100000, city$\_$z="Albany, New York" \\
user & Would the top of Mount Fuji stick out of the Sea of Japan? \\
assistant & We first identify all named entities, special nouns and numerical values: Mount Fuji, Sea of Japan. The generalized solution of the original question can be applied as long as we know Mount Fuji is a mountain (we cannot simply conceptulize it into geographical feature because the solution only works for features that point upwards such as a mountain), and Sea of Japan is a body of water (similarly, it needs to have depths.) As a result, we can replace "Mount Fuji" to "Mountain X" (mountain$\_$x: str) "the Sea of Japan" to "Body of Water Y" (body$\_$of$\_$water$\_$y: str) So the question becomes Would the top of Mountain X stick out of Body of Water Y? With parameters mountain$\_$x="Mount Fuji", body$\_$of$\_$water$\_$y="the Sea of Japan" \\
user & Did any country in Portuguese Colonial War take a general neutral role in WWII? \\
assistant & We first identify all named entities, special nouns and numerical values: Portuguese Colonial War, a general neutral role, WWII. The original solution checks for the involving countries in the first war, then checks if any of these countries took a general neutral role in the second war. We should not replace "a general nutral role" with its semantic type, because the solution would be vastly different for other roles. As a result, we can replace "Portuguese Colonial War" to "War X" (war$\_$x: str) "WWII" to "War Y" (war$\_$y: str) So the question becomes Did any country in War X take a general neutral role in War Y? With parameters war$\_$x="Portuguese Colonial War", war$\_$y="WWII" \\
user & Was Lil Jon's top ranked Billboard song a collaboration with a member of The Lox? \\
assistant & We first identify all named entities, special nouns and numerical values: Lil Jon, Billboard, The Lox. In order to keep the question answerable with the original problem-solving path, we should not replace 'Billboard', because the solution depends on knowing what kind of song the question is asking for. As a result, we can replace "Lil Jon" to "Artist X" (artist$\_$x: str) "The Lox" to "Group Y" (group$\_$y: str) So the question becomes Was Artist X's top ranked Billboard song a collaboration with a member of Group Y? With parameters artist$\_$x="Lil Jon", group$\_$y="The Lox" \\
\bottomrule
\end{tabular}
\caption{Prompts for question conceptualization.}
\label{tab:p1}
\end{table*}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{p{0.08\linewidth} | p{0.9\linewidth}}
\toprule
Prompt & Content \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}
system & Solve the given question. Follow the way in the given examples to answer the questions. At the end, you must say if you have to guess an answer, the answer is yes/no. \\
user & Can the Very Large Telescope observe the largest mountain on Earth? \\
assistant & The Very Large Scope is in Chile. The largest mountain on Earth is mount Everest, which is in Nepal. The earth curvature is in between the two entities and there is not a line of sight. As a result, if I have to guess an answer, the answer is no. \\
user & Did Bill Nye vote for Franklin Delano Roosevelt? \\
assistant & Bill Nye was born in 1955. Franklin Delano Roosevelt's last election was in 1944. Bill Nye was not born nor alive at the time Roosevelt was running. As a result, if I have to guess an answer, the answer is no. \\
user & Does table tennis use prime numbers? \\
assistant & Table tennis plays to 11 points each game. 11 is a prime number. As a result, if I have to guess an answer, the answer is yes. \\
user & Would an Evander Holyfield 2020 boxing return set age record? \\
assistant & Evander Holyfield will turn 58 years old at the end of 2020. Steve Ward holds the world's oldest boxer title at age 59. 58 is younger than 59 and not enough to set the record. As a result, if I have to guess an answer, the answer is no. \\
user & Would the top of Mount Kilimanjaro stick out of the Sea of Japan? \\
assistant & The height of Mount Kilimanjaro is about 5,895 meters, while the Sea of Japan has a maximum depth of 3,742 meters. So, the top of Mount Kilimanjaro is taller than the maximum depth of the Sea of Japan. If we were to put Mount Kilimanjaro in the Sea of Japan, it will stick out. As a result, if I have to guess an answer, the answer is yes. \\
\bottomrule
\end{tabular}
\caption{Prompts for chain-of-thought inference.}
\label{tab:p2}
\end{table*}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{p{0.08\linewidth} | p{0.9\linewidth}}
\toprule
Prompt & Content \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}
system & Answer the question in the expected type. Use your best educated guess or estimation if needed. Follow the provided examples and return a JSON dictionary with key 'answer'. \\
user & How many people today are related to Genghis Khan? (int) \\
assistant & {"answer": 35000000} \\
user & What is the profession of Michael Jackson? (str) \\
assistant & {"answer": singer} \\
user & Who has more than one Nobel Prize? (list) \\
assistant & {"answer": ["John Bardeen", "Frederick Sanger", "Linus Pauling", "Marie Curie"]} \\
user & Does anchors on KBS speak Arabic? (bool) \\
assistant & {"answer": false} \\
\bottomrule
\end{tabular}
\caption{Prompts for ask$\_$llm implementation.}
\label{tab:p3}
\end{table*}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{p{0.08\linewidth} | p{0.9\linewidth}}
\toprule
Prompt & Content \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}
system & Follow the examples, for the given conceptualized question, generate some possible entities based on the type, and the provided example. Try to be diverse and creative. \\
user & Question: Was a person sold License X for Artwork Y ripped off? Entity: License X Example: Creative Commons License \\
assistant & MIT License General Public License BSD License CopyLeft License Artistic License Exclusive License VARA License END \\
user & Question: Is Technology X able to make Object Y? Entity: Object Y Example: adenovirus \\
assistant & prosthetics plastic bottles coffee smartphones contact lenses furniture computer software food END \\
\bottomrule
\end{tabular}
\caption{Prompts for entity generation.}
\label{tab:p4}
\end{table*}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{p{0.08\linewidth} | p{0.9\linewidth}}
\toprule
Prompt & Content \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}
system & Follow the examples for the given conceptualized statement and generate the missing entities based on the type so that the given statement holds. Try to be diverse and creative. \\
user & Statement: If someone loves chocolate, they enjoy Compound Y. Entity: Company Y: str (e.g., capsaicin) \\
assistant & {"Compound Y": "Theobromine", "statement": "If someone loves chocolate, they enjoy Theobromine."} {"Compound Y": "Phenylethylamine", "statement": "If someone loves chocolate, they enjoy Phenylethylamine."} {"Compound Y": "Anandamide", "statement": "If someone loves chocolate, they enjoy Anandamide."} {"Compound Y": "Tryptophan", "statement": "If someone loves chocolate, they enjoy Tryptophan."} {"Compound Y": "Caffeine", "statement": "If someone loves chocolate, they enjoy Caffeine."} \\
user & Statement: Person X was present at the Year Y Met Gala. Entity: Person X: str (e.g., Bruce Lee), Year Y: int (e.g., 1964) \\
assistant & {"Person X": "Blake Lively", "Year Y": 2018, "statement": "Blake Lively was present at the 2018 Met Gala."} {"Person X": "Kendall Jenner", "Year Y": 2021, "statement": "Kendall Jenner was present at the 2021 Met Gala."} {"Person X": "Rihanna", "Year Y": 2017, "statement": "Rihanna was present at the 2017 Met Gala."} {"Person X": "Lady Gaga", "Year Y": 2019, "statement": "Lady Gaga was present at the 2019 Met Gala."} {"Person X": "Beyonce", "Year Y": 2015, "statement": "Beyonce was present at the 2015 Met Gala."} \\
\bottomrule
\end{tabular}
\caption{Prompts for positive/negative statement generation.}
\label{tab:p5}
\end{table*}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{p{0.08\linewidth} | p{0.9\linewidth}}
\toprule
Prompt & Content \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}
system & Based on the given question, a person wrote a Python program. They used the following helper functions: ask\_gpt(query: str, type): ask a simple question to the knowledgeable GPT model and returns the answer in the desired type. However, the program this person wrote failed on some test cases. Given the program, and the failed test cases with corresponding reasons, write a new program that address any potential issues. \\
user & Original Question: Did Coach X (coach\_x: str) turn her chair around for Contestant Y (contestant\_y: str) on TV Show Z (tv\_show\_z: str)? Program the person wrote: ''' We need to consider if Contestant Y appeared on TV Show Z and if Coach X turned their chair around for Contestant Y. '''  def participant(contestant: str, tv\_show: str) -> bool: 	question = f"Did {contestant} participate in {tv\_show}?" 	participation = ask\_gpt(question, bool) 	return participation def chair\_turn(coach: str, contestant: str, tv\_show: str) -> bool: 	question = f"Did {coach} turn their chair around for {contestant} on {tv\_show}?" 	chair\_turned = ask\_gpt(question, bool) 	return chair\_turned def answer(coach\_x: str, contestant\_y: str, tv\_show\_z: str): 	participated = participant(contestant\_y, tv\_show\_z) 	if not participated: 		return "Must be no" 	chair\_turned = chair\_turn(coach\_x, contestant\_y, tv\_show\_z) 	return "Must be yes" if chair\_turned else "Must be no" Failed case 1: answer(coach\_x="Adam Levine", contestant\_y="John Legend", tv\_show\_z="The Voice") Failed reason 1: The concrete question in this case is 'Did Adam Levine turn her chair around for John Legend on The Voice?' Adam Levine and John Legend have both been coaches on "The Voice," but the premise of the question contains a misunderstanding of the show\'s format. "Turning chairs" is something coaches do during blind auditions to signal they want an artist to join their team. Since John Legend and Adam Levine are both coaches, they would not be auditioning and therefore would not have a chair turn situation between them.  As a result, if I have to guess an answer, the answer is no. However, the program returned results 'Yes'. \\
assistant & def check\_contestant(contestant: str, tv\_show: str) -> bool: 	question = f"Did {contestant} participate in {tv\_show} as a contestant?" 	participation = ask\_gpt(question, bool) 	return participation def check\_coach(judge: str, tv\_show: str) -> bool: 	question = f"Did {judge} participate in {tv\_show} as a coach?" 	participation = ask\_gpt(question, bool) 	return participation def chair\_turn(coach: str, contestant: str, tv\_show: str) -> bool: 	question = f"Did {coach} turn their chair around for {contestant} on {tv\_show}?" 	chair\_turned = ask\_gpt(question, bool) 	return chair\_turned def answer(coach\_x: str, contestant\_y: str, tv\_show\_z: str): 	is\_coach = check\_coach(coach\_x, tv\_show\_z) 	is\_contestant = check\_contestant(contestant\_y, tv\_show\_z) 	if not is\_coach or not is\_contestant: 		return "Must be no" 	chair\_turned = chair\_turn(coach\_x, contestant\_y, tv\_show\_z) 	return "Must be yes" if chair\_turned else "Must be no" \\
\bottomrule
\end{tabular}
\caption{Prompts for self-refinement. Another example is omitted.}
\label{tab:p6}
\end{table*}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{p{0.08\linewidth} | p{0.9\linewidth}}
\toprule
Prompt & Content \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}
system & Based on the given question, write a Python program with some abstraction that solves the given question and all other similar questions that can be solved in a similar fashion. Think as comprehensively as possible, so that the program would work on any inputs. You can use the following helper function: ask\_llm(query: str, type) to ask a simple question to the knowledgeable GPT model and returns the answer in the desired type. \\
user & Is there a rapper whose real name is similar to Rapper X (rapper\_x: str)'s real name have more than N times (multiplier\_n: int) of Award Y (award\_y: str) than Rapper X (rapper\_x: str)? Multiple Choices: A) yes B) no
 \\
assistant  & \begin{python}
'''
We can first find a list of rappers with enough Award Y, then check if any of them share a similar name with Rapper X.
'''

def rapper_real_name(name: str) -> str:
	question = f"What is the real or legal name of rapper {name}?"
	real_name = ask_gpt(question, str)
	return real_name
def rapper_awards(name: str, award_name: str) -> int:
	question = f"How many {award_name} has the rapper {name} won?"
	num_awards = ask_gpt(question, int)
	return num_awards
def list_of_rappers_with_certain_awards(award_name: str, award_count: int) -> list:
	question = f"Give me a list of rappers who have won at least {str(award_count)} {award_name}."
	list_of_rappers = ask_gpt(question, list)
	return list_of_rappers
def name_similar(name_1: str, name_2: str) -> bool:
	question = f"Is the name '{name_1}' similar to the name '{name_2}'?"
	name_is_similar = ask_gpt(question, bool)
	return name_is_similar
def answer(rapper_x: str, award_y: str, multiplier_n: int):
	reference_person_awards_count = rapper_awards(rapper_x, award_y)
	target_award_count = int(multiplier_n * reference_person_awards_count)
	candidates = list_of_rappers_with_certain_awards(award_y, target_award_count)
	reference_person_real_name = rapper_real_name(rapper_x)
	for candidate_name in candidates:
		candidate_real_name = rapper_real_name(candidate_name)
		if name_similar(candidate_real_name, reference_person_real_name):
			return "Must be yes"
	return "Must be no"
#The program ends here.

\end{python} \\
\bottomrule
\end{tabular}
\caption{Prompts for program generation. Six other examples are omitted.}
\label{tab:p7}
\end{table*}

Table~\ref{tab:p1} lists the prompts used for question conceptualization, including the system message and few-shot examples. Table~\ref{tab:p2} lists the few-shot prompt used for chain-of-thought. All domains and tasks use the same set of examples because we only consider binary questions in this work. Table~\ref{tab:p3} lists the prompts to implement the information retrieval function ask\_llm($\cdot$), encouraging LLM to output answers in the expected type. Table~\ref{tab:p4} shows the prompt used for noun generation, which is the first step in the similar question generation process. Table~\ref{tab:p5} details the prompts to generate positive/negative statements, as described in \S\ref{sec:similar-question-generation}. Table~\ref{tab:p6} shows the prompts used during the self-refinement process, with another few-shot example omitted. Table~\ref{tab:p7} demonstrates how programs are generated, with only one few-shot example shown due to space limitations. 

