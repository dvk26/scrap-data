
\documentclass{article} % For LaTeX2e
\usepackage{colm2024_conference}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{dashrule}
\usepackage{arydshln}
\usepackage{graphicx}
\usepackage{pythonhighlight}
\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}

\usepackage{xcolor}
\newcommand{\hm}[1]{\textcolor{blue}{~[hm:#1]}}
\newcommand{\sihao}[1]{\textcolor{purple}{~[sc: #1]}}
\colmfinalcopy
\title{Conceptual and Unbiased Reasoning in Language Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{
% Ben Zhou$^1$\thanks{Most work done duing Ben Zhou and Sihao Chen's internship at Tencent AI Lab.} \ \ \ Hongming Zhang$^2$\ \ \ Sihao Chen$^1$\ \ \ Dian Yu,\ \ \ Hongwei Wang \\ \\ 
% \textbf{Baolin Peng,\ \ \ Dan Roth,\ \ \ Dong Yu}
% \\ \\ 
% $^1$University of Pennsylvania\ \ $^2$ Tencent AI Lab
% }


\author{Ben Zhou\textsuperscript{$\spadesuit$}\thanks{Most work done during Ben Zhou and Sihao Chen's internship at Tencent AI Lab.} \quad Hongming Zhang\textsuperscript{$\heartsuit$} \quad Sihao Chen\textsuperscript{$\spadesuit$} \\ \bf{Dian Yu\textsuperscript{$\heartsuit$} \quad Hongwei Wang\textsuperscript{$\heartsuit$} \quad Baolin Peng\textsuperscript{$\heartsuit$} \quad Dan Roth\textsuperscript{$\spadesuit$} \quad Dong Yu\textsuperscript{$\heartsuit$}} \\
\textsuperscript{$\spadesuit$}University of Pennsylvania\quad \textsuperscript{$\heartsuit$}Tencent AI Lab
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \colmfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Conceptual reasoning, the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on large language models' capability to perform conceptual reasoning. In this work, we bridge this gap and propose a novel conceptualization framework that forces models to perform conceptual reasoning on abstract questions and generate solutions in a verifiable symbolic space. Using this framework as an analytical tool, we show that existing large language models fall short on conceptual reasoning, dropping 9\% to 28\% on various benchmarks compared to direct inference methods. We then discuss how models can improve since high-level abstract reasoning is key to unbiased and generalizable decision-making. We propose two techniques to add trustworthy induction signals by generating familiar questions with similar underlying reasoning paths and asking models to perform self-refinement. Experiments show that our proposed techniques improve models' conceptual reasoning performance by 8\% to 11\%, achieving a more robust reasoning system that relies less on inductive biases.
\end{abstract}

\section{Introduction}

Language models have progressed rapidly in recent years after community research into the emergent abilities in models with billions of parameters and trillions of pre-training tokens. While such models achieve admirable performances on many reasoning benchmarks, people have been looking into their decision-making process and trying to understand why large language models (LLM) perform well, especially compared with smaller models. One of the popular theories regarding why LLMs work is that they can memorize a huge amount of facts from pre-training data and generalize to new queries through induction on semantics \citep{tang2023large, Li2023Deceiving} (i.e., associate new inputs with memorized facts and guess the most likely outputs). Induction-based inference in LLMs works well for most daily tasks since these tasks are usually familiar to the model and contain strong induction signals. However, heavily relying on induction-based inference limits the model from performing well on non-induction tasks that require complex reasoning and planning, such as temporal reasoning \citep{zhou2020temporal} and travel planning \citep{xie2024travelplanner}. Moreover, relying on induction signals will inevitably lead to biased reasoning, where models perform much worse on queries containing unfamiliar semantics \citep{Li2023Deceiving}. 

A significant gap between LLM's induction-based reasoning and human reasoning, which does not have the issues above, is the ability to perform conceptual and abstract reasoning. Humans develop the ability to reason on a conceptual level from a very young age \citep{markovits2015metacognition, walker2015learning}, which enables them to overcome the limitations of induction through analogy and planning. For example, when humans travel to a new city, they can devise a plan to find good restaurants based on analogical experiences from the places they are familiar with because they can associate the underlying problem-solving processes on a conceptual level. However, there have been limited attempts to study LLMs' abstract reasoning because 1) it is difficult to force LLMs to perform abstract reasoning, and 2) it is challenging to verify the correctness of models' abstract reasoning process.

This work proposes a novel conceptualization framework to investigate and improve LLM's abstract reasoning capability. The conceptualization framework is composed of two parts, which address the challenges mentioned above respectively: 1) a question abstraction process that removes as much induction signal from the original question by replacing specific special nouns (e.g., named entities, numerical values) with their fine-grained semantic types; 2) a symbolic program space where LLMs will generate their abstract reasoning solutions in so that we can verify their correctness by executing the program with the necessary information from the original question and check for denotation accuracy. We use a state-of-the-art language model to perform the question abstraction step so that all downstream models work with a consistent set of high-quality abstract questions. Then, all downstream models are given the exact instructions regarding the design of the symbolic program space and try to generate solutions based on only the conceptualized question. In \S\ref{sec:analytical-results}, we show that existing LLMs' performances are much lower in the abstract reasoning setup, dropping 9\% to 28\% across different models compared to chain-of-thought (CoT) \citep{wei2022chain} that entirely rely on induction, indicating that existing LLMs lack the core reasoning capability to generalize on a conceptual level. This also suggests that while some models have seemingly close performances on various datasets, their underlying reasoning capability varies greatly as some gaps are dramatically larger than others.

We then propose improvement techniques for LLM conceptual reasoning. Our main motivation is finding high-quality induction signals and using them as a reference to the conceptual reasoning process. This approximates the human analogy processes that symbolically associate familiar problem-solving processes from past experiences. Specifically, we find concrete questions that can be reduced to the same conceptualized question but with familiar nouns. These questions are often directly answerable because the supporting evidence commonly appears in the pre-training data, and the LLM answer is more likely to be correct. We propose two ways to use such similar questions. They can be used as a run-time exam to identify high-performing candidate solutions, and we can infer final predictions based only on them. In addition, we can ask LLMs to self-refine the candidate programs based on CoT solutions to the mistakes they make. Experiments show the effectiveness of the proposed methods, which improve 8\% to 11\% on a wide range of reasoning benchmarks. Our proposed conceptual reasoning method achieves relatively comparable performances with CoT in some scenarios while being more robust and less biased because it produces symbolic solutions without heavily relying on induction signals and makes predictions based on deterministic executions of such symbolic solutions.

\section{Related Works}
Many works have identified that large language models
% ' autoregressive inferences cannot handle more symbolic tasks such as temporal reasoning, symbolic reasoning, proofing, and safety alignment
cannot handle tasks that require intensive reasoning~\citep{tang2023large, saparov2022language, feng2022generic, Li2023Deceiving, xu2023cognitive}. Our work builds on these works and identifies high-level and conceptual reasoning as a major bottleneck shared among these observations, limiting models from system-2 reasoning  \citep{kahneman2011thinking, weston2023system, lin2024swiftsage}. This work is also related to decomposition-based reasoning \citep{min2019multi, zhou2022learning}, and program-based inference methods \citep{chen2022program, gao2023pal, lu2024chameleon}, where these methods aim to shift from model performing probabilistic induction to symbolic deduction, similar to our motivations. This work is marginally relevant to other LLM self-revision works \citep{chen2023teaching, madaan2024self}, but we consider different signals.
% More work on fast and slow action planning and execution \citep{lin2024swiftsage} are also relevant.

\section{Conceptualization Framework}

\begin{figure}[h]
\begin{center}
    \includegraphics[scale=0.4]{figs/fig-conceptualization.pdf}
    \caption{An overview of the conceptualization process. From an original question $Q$, GPT-4 generates an abstract question $Q_{abs}$ and corresponding parameters through a conceptualization process, and a downstream LLM will try to generate a symbolic solution based on only $Q_{abs}$, which is later executed with the actual parameters.}
    \label{fig:conceptualization-overview}
\end{center}
\end{figure}

This section introduces how the conceptualization framework is defined and implemented. For a natural question $Q$, we first employ a question conceptualization step that transforms $Q$ to $Q_{abs}$, an abstract version of the question. We then define a symbolic solution space using Python programs. At inference time, the downstream model generates a symbolic solution in our defined space based on only $Q_{abs}$, which is later deterministically executed with the concrete input parameters to generate a final prediction to the original question $Q$. This process is overviewed in Fig.~\ref{fig:conceptualization-overview}.

\subsection{Question Conceptualization}
\label{sec:conceptualization}
The goal of the question conceptualization step is to replace certain special nouns with their fine-grained semantic types so that humans and systems can develop a general solution that works for any specific nouns that fall under the semantic types. The primary challenge in this step is the tradeoff between solvability and generalizability. Even though all conceptualized questions are solvable with an unlimited program length (i.e., assuming a solution that considers all possible corner cases), it is often impractical and less solvable if we conceptualize too many concrete nouns in the question. On the other hand, if we remove too few nouns, models can still take the semantic shortcuts \citep{Li2023Deceiving} based on inductive biases. However, we emphasize that a balanced solution between these two tradeoffs is ideal but not required for our benchmark to be valid since models should, in theory, work on any level of conceptualization as long as the fine-grained semantic types are correct and the question is readable. We verify this later in \S\ref{sec:human-analysis}.

To balance this tradeoff, we use one of the most capable large language models, GPT-4, and prompt it with carefully designed few-shot examples that teach GPT-4 which nouns to replace and what fine-grained semantic types they should be replaced with. We use six demonstrations, selected manually from the StrategyQA \citep{geva2021did} dev set. We list the prompt in \S\ref{sec:prompts}. Given the original question $Q$, GPT-4 generates the abstract question $Q_{abs}$, a list of replaced noun phrases with their semantic types and corresponding Python types, and their concrete parameters representing the actual values from the original question $Q$. 

\subsection{Symbolic Program Space}
\label{sec:program-space}
We design a symbolic space based on Python programs to accommodate solutions for general binary (yes/no) questions. It includes several pre-defined helper functions to facilitate information retrieval and relational operations.

\subsubsection{Information Retrieval} 
We introduce a pre-defined helper function called $\mathrm{ask\_llm(query, type)}$ to handle all cases that require external knowledge. As the function name suggests, it accepts a string query that is often a short question, asks that question to the same downstream LLM that generates the program, and forces it to return the answer in the expected type. To ensure that the returned results are in the expected type, we employ a few-shot prompt that maps natural language queries to JSON structures, followed by a type casting. If the type casting fails, the retriever will retry ten times or raise an exception. We list this helper function with the few-shot examples in \S\ref{sec:prompts}. Because this design separates reasoning and knowledge, future works can adopt other retrieval methods with minimal effort. 

\subsubsection{Relational Operation Replacement} 
We implement a special post-processing step to handle mismatches in relational operators. For example, consider a program that needs to find the relationship between two people and check if they are parent-child. It will likely generate a query to the $\mathrm{ask\_llm(query, type)}$ helper function in the form of ``What is the relationship between person-1 and person-2?'', and the helper function may return multiple ways of describing the same relation, such as ``father-son'' or ``mother-daughter'', or ``descendent.'' Python built-in equal operator $==$ cannot handle such fuzzy matches, so the execution may not lead to the correct results.

\begin{table*}[ht]
\centering
\small
\begin{tabular}{lc}
\toprule
Operator & Query for Soft Operator \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}
a == b & Consider the implied value, is \textit{a} roughly the same as \textit{b}?\\
a $>$ b & Consider the implied value, is \textit{a} roughly larger than \textit{b}?\\
a $<$ b & Consider the implied value, is \textit{a} roughly smaller than \textit{b}?\\
a in b & Considered the implied value, is \textit{a} included or mentioned by the list \textit{b}? \\
\bottomrule
\end{tabular}
\caption{Queries to replace relational operators with soft operators. Other operations are compositions with the listed operations.}
\label{tab:soft-operator}
\end{table*}

To address this, we propose soft relational operators to replace built-in operators, including $>, >=, <, <=, ==, !=, in$, and $not\ in$. Each soft operator translates a compiler-level relational operation to a soft query to large language models. The specific queries that are used to replace the built-in operators are listed in Table~\ref{tab:soft-operator}. 
% These queries are fed into the $\mathrm{ask\_llm(query, type)}$ helper function with a boolean expected value. 
We implement actual helper functions that correspond to each operation. For example, we implement a helper function named $\mathrm{gt}(a, b)$ that calls $\mathrm{ask\_llm(\cdot)}$ with the queries for $>$ operations in Table~\ref{tab:soft-operator}. Then, we use Python's abstract syntax trees\footnote{\url{https://docs.python.org/3/library/ast.html}} and recursively find \textit{BoolOps} that fall under our interested relational operators and replace them with calls to our implemented helper functions. 

\subsection{Symbolic Program Execution}
As mentioned above, each question $Q$ is transformed to $Q_{abs}$ and a set of parameters with concrete values, as shown in Fig.~\ref{fig:conceptualization-overview}. In addition, each generated symbolic solution will have a function named $\mathrm{answer}(\cdot)$, which accepts the parameters as input values and returns a prediction string. At inference time, we prepend the actual implementations of our helper functions, including $\mathrm{ask\_llm(\cdot)}$ and all relational helper functions to the generated symbolic solution. We also append a function call to the $\mathrm{answer}(\cdot)$ function with the parameters from the original question and print the returned value from this function call. Finally, we execute this generated Python file and use the printed string as the final prediction of the model.

\subsection{Applications of the Conceptualization Framework}
We discuss two applications of the framework: evaluation and unbiased reasoning. 

\subsubsection{Analytical Evaluation}
\label{sec:analytical-evaluation}
Our proposed conceptualization framework can be an analytical tool to check if existing LLMs understand the problem-solving process or are simply taking semantic shortcuts \citep{Li2023Deceiving}. At inference time, a downstream model is provided with $Q_{abs}$, and few-shot demonstrations defining the available helper functions and program space. The model then generates K candidate programs, which are later executed with the corresponding input parameters and then used to majority vote a final answer based on the executed results. All downstream models are provided with the same $Q_{abs}$ and few-shot demonstrations and executed with the same input parameters.\footnote{$Q_{abs}$ and the input parameters are generated by GPT-4 as shown in Fig.~\ref{fig:conceptualization-overview}, and an expert writes the few-shot examples.} We detail the prompts used for generating candidate program solutions in \S\ref{sec:prompts}.

This inference process forces the model not to reason with memorized knowledge from particular nouns in the original question but to devise a solution based on the question's underlying reasoning path on an abstract level. Ideal reasoners should show relatively small gaps between performing conceptual reasoning and direct inferences (e.g., CoT).

\subsubsection{Unbiased Reasoning}
\label{sec:unbiased-reasoning}

\begin{figure}[h]
\begin{center}
    \includegraphics[scale=0.4]{figs/fig-reasoning.pdf}
    \caption{Comparison of a direct inference method (left) and conceptual inference (right). Results are taken from gpt-3-turbo.}
    \label{fig:reasoning-comparison}
\end{center}
\end{figure}

Taking one step further with the analytical settings, we can use the conceptualization framework to perform unbiased reasoning. This is based on observations that LLMs often make stubborn mistakes if certain word combinations strongly hint toward an incorrect answer. For example, as Fig.~\ref{fig:reasoning-comparison} shows, GPT-3.5 makes a stubborn mistake with direct inference on the left. This is because the model strongly memorizes prime numbers as a mathematical concept, so it cannot correctly associate that with sports. As a result, even though the model correctly identifies the need to find numbers in the sports' rules and scoring systems, it still cannot derive the correct answer. 

On the other hand, if the model follows our proposed conceptual inference process as shown on the right of Fig.~\ref{fig:reasoning-comparison}, it can correctly generate an unbiased solution since it cannot see the specific names of the sport and the numerical property, which can later be executed to find the correct answer. That being said, our proposed conceptualization framework can be applied for unbiased and neutral model reasoning, less affected by induction signals that might be irrelevant to problem-solving.

\section{Conceptual Reasoning with Analogy}
\label{sec:analogy}
As mentioned in \S\ref{sec:unbiased-reasoning}, the conceptualization framework can perform unbiased reasoning by removing most induction signals from the input question and forcing the model to perform abduction and deduction. This motivates us to discuss how we can improve the performance of conceptual reasoning and ideally reach the performances of direct inference methods, which have full access to induction signals such as entity names. 

This section proposes a method to inject \textbf{trustworthy} induction signals back to the reasoning process. Here, we emphasize that compared to a direct inference method that uses inductions from the original question, which are unreliable when the question differs much from pre-training distribution, our key motivation is to add induction signals familiar to the language model itself and, hence, more trustworthy. Specifically, for $Q_{abs}$, we generate a list of concrete questions with familiar nouns that can be conceptualized into the same abstract question. We use CoT to answer these questions and keep only those that can be confidently answered through self-consistency. % We view these similar questions as trustworthy induction signals that can contribute to the problem-solving of the original question since the underlying reasoning paths overlap. 
% \hm{reviewers might have concerns about the definition of trustworthy here. Maybe we can rephrase it slightly or explain more here. People might confuse this with research work on the trustworthy of LLMs.}
This approximates the human analogy process: when we encounter a new situation, we often resort to familiar situations from personal experiences and use what we did back then to solve the current task.

In \S\ref{sec:similar-question-generation}, we introduce how we find these trustworthy similar questions that language models can accurately answer. In \S\ref{sec:program-selection}, we describe the first method of injecting such induction signals into the conceptual inference process, namely using them to select better candidate programs. In \S\ref{sec:self-refinement}, we introduce a self-refinement method using these questions as feedback.

\subsection{Similar Question Acquisition}
\label{sec:similar-question-generation}

\subsubsection{Question Generation}
% The most straightforward way to generate similar questions is to directly give $Q_{abs}$ to the downstream LLM and ask it to generate a list of questions. However, this is less ideal because the generated questions often lack diversity. 
To acquire the maximum diversity and correctness, we design a novel pipeline process to generate similar questions based on an abstract question $Q_{abs}$, with the goal of generating questions with similar underlying reasoning paths and solvable by LLMs directly. This similar question generation pipeline is performed with the same downstream LLM that generates the programs so that we ensure that the reasoning process does not involve a more capable model such as GPT-4. 

We first generate a list of concrete nouns/entities for each semantic type. For example, consider \textit{Is City X on Coast Y?}, the abstract version of \textit{Is Miami on the American West Coast?} we ask the downstream LLM to generate possible City X's. It will generate a list of city names familiar to the LLMs, such as Hong Kong, New York, and London. Similarly, it will generate a list of Coast Y's similar to \textit{American West Coast}, such as the Persian Gulf Coast, Baltic Coast, and the Black Sea Coast. During this process, we provide it with the nouns in the original question to ensure that the generated nouns are roughly in the same scope as the original entity. This way, the model will not generate a small coast such as Rockport Coast, which is too small to accommodate any cities. 

For each of the generated concrete nouns, we prompt the language model to generate a statement corresponding to either a positive answer or a negative answer to the original question.\footnote{This is because we only consider binary questions in this work.} For example, given the newly generated entity ``Hong Kong'' for City X, we design a few-shot prompt for the language model to generate positive statements with concrete Coast Y, such as \textit{Hong Kong is on China's southern coast}, as well as negative statements such as \textit{Hong Kong is not on the French Riviera}. We then transform these statements back to a question in the form of the original question, which is \textit{Is Hong Kong on China's southern coast?} and \textit{Is Hong Kong on the French Riviera?}, respectively. At the same time, the model will also generate corresponding input parameters, such as \textit{city\_x=``Hong Kong'', coast\_y=``China's southern coast''}. Prompts used in this step are listed in \S\ref{sec:prompts}.

\subsubsection{Question Validation}

\begin{table}[ht]
\centering
\small
\begin{tabular}{lcc}
\toprule
Agreement & Accuracy & \% Remaining \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}
None & 81.1 & 100 \\
8/10 & 89.7 & 73 \\
10/10 & 97.2 & 47 \\
\bottomrule
\end{tabular}
\caption{Sanity-check experiment on StrategyQA dev set. This compares the performance and remaining instances if we only evaluate instances where a certain number of predictions out of 10 samples agree. Experiments shown use Llama-70B-instruct and CoT.}
\label{tab:sanity-check-agreement}
\end{table}


We employ a verification process to select those that can be confidently answered and are likely to be more trustworthy by finding silver labels for each generated question with CoT. We run the chain-of-thought inference ten times for each question and keep the questions where at least 8 out of the 10 samples agree with each other.
% \hm{Do we have a distribution of the correct rate of all instances? People might question/challenge the quality of GPT generation result.} 
% The remaining questions are answerable, and the language model's silver label is more likely to be correct. 
To motivate this, we conduct a sanity-check experiment on StrategyQA with Llama-70B-instruct.\footnote{\url{https://huggingface.co/upstage/Llama-2-70b-instruct}} As Fig.~\ref{tab:sanity-check-agreement} shows, there is a significant improvement in performance if we only evaluate instances where CoT predictions agree with each other over a certain amount of times. This suggests that the labels we acquire after enforcing such an agreement are more reliable. On the other hand, an 8/10 agreement provides a good tradeoff between accuracy and the number of remaining instances, which will save computational costs. 

\subsection{Program Selection}
\label{sec:program-selection}
This section describes a straightforward method to apply these generated questions and their silver labels to improve model conceptual reasoning, namely a selection method that finds candidate programs that are likely to be better than the others. Specifically, within each question, we execute the K candidate programs generated in \S\ref{sec:analytical-evaluation} on all generated similar questions and evaluate each program's accuracy on the corresponding silver labels. We then rank the programs based on their accuracies and predict the original question based on the top programs' execution results. We employ a majority-vote process if multiple programs have the same accuracy. If there is a tie in the majority vote, we perform a weighted vote with all programs using their accuracies as the weight. 

\subsection{Program Self-refinement}
\label{sec:self-refinement}

\begin{figure}[h]
\begin{center}
    \includegraphics[scale=0.33]{figs/fig-self-refine-2.pdf}
    \caption{An overview of the self-refinement process with a concrete example. The initial program does not consider that some animals can change their color for camouflage purposes, as pointed out in the generate similar question's CoT solution. Based on this information, the model can refine the initial program to include such consideration.}
    \label{fig:self-refine}
\end{center}
\end{figure}

Using similar questions as a program selection method does not fully utilize their potential. This is because the reasoning processes from chain-of-thought are often correct on the generated similar questions because of the strong induction signal. As a result, if a program fails on certain similar questions, the CoT outputs serve as great feedback for LLMs to self-refine the candidate program. As overviewed in Fig.~\ref{fig:self-refine}, we propose a self-refinement process to ask LLMs to generate a better program based on an existing program and natural language feedback from CoT on why the existing program fails on a similar question. Because of computational limitations, we only conduct this process on programs with top accuracies within each question. We join the new programs with the original programs and conduct the same inference process described in \S\ref{sec:program-selection}. 

\section{Experiments}

\begin{table*}[ht]
\centering
\begin{tabular}{lccccccc}
\toprule
System & BoolQ & CsQA & Hotpot & Strat.dev & Strat.test & All & Exe.\%\\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}
GPT-3.5 CoT & 72.6 & 60.3 & 85.3 & 75.0 & 63.3 & 71.9 & -\\
GPT-3.5 Prog. & 58.3 & 37.6 & 78.3 & 68.0 & 61.6 & 63.4 & 89 \\
\midrule
Llama CoT & 72.1 & 80.8 & 85.1 & 81.1 & 69.5 & 77.6 & -\\
Llama Prog. & 58.9 & 53.4 & 65.6 & 61.0 & 56.8 & 59.7 & 81 \\
\midrule
Mixtral CoT & 72.6 & 67.9 & 84.9 & 74.1 & 67.2 & 74.0 & - \\
Mixtral Prog. & 41.1 & 10.7 & 55.3 & 53.9 & 52.9 & 46.1 & 73\\
\bottomrule
\end{tabular}
\caption{Performance comparisons between CoT (uses original questions) and Prog(ram) (uses conceptualized questions and the proposed program space in \S\ref{sec:program-space}). Exe.\% is the program execution success rate. Unsuccessful executions are similar to CoT outputs that do not contain an answer, so they do not affect the fairness of comparison.}
\label{tab:perf-conceptualize}
\end{table*}

\begin{table*}[ht]
\centering
\begin{tabular}{lcccccc}
\toprule
System & BoolQ & CsQA & Hotpot & Strat.dev & Strat.test & Avg.$\Delta$  \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}
Llama Program & 58.9 & 53.4 & 65.6 & 61.0 & 56.8 & - \\ 
+ Selection & 65.0 & 76.1 & 72.1 & 68.4 & 60.0 & +9.2 \\
+ Self-refine & 68.0 & 78.6 & 73.9 & 71.1 & 61.0 & +11.4\\
\midrule
Mixtral Program & 41.1 & 10.7 & 55.3 & 53.9 & 52.9 & - \\ 
+ Selection & 53.3 & 30.8 & 60.1 & 56.1 & 52.1 & +7.7 \\
+ Self-refine & 55.3 & 38.0 & 64.0 & 58.8 & 54.8 & +11.4 \\
\bottomrule
\end{tabular}
\caption{System performances on reasoning datasets with proposed improvements. Selection refers to the selection method proposed in \S\ref{sec:program-selection}, and Self-refine refers to the self-refinement technique proposed in \S\ref{sec:self-refinement}. Program is the same as in Table~\ref{tab:perf-conceptualize}.}
\label{tab:perf-ours}
\end{table*}

In this section, we conduct experiments on both the analytical setting as described in \S\ref{sec:analytical-evaluation} and the reasoning setting as described in \S\ref{sec:unbiased-reasoning} with improvements described in \S\ref{sec:analogy}.

\subsection{Datasets, Models and Settings}
We consider four reasoning benchmarks that will benefit from symbolic solutions: BoolQ \citep{clark2019boolq}, CommonsenseQA \citep{talmor2018commonsenseqa}, HotpotQA \citep{yang2018hotpotqa}, and StrategyQA \citep{geva2021did}. We only use yes/no questions from HotpotQA. We convert multiple-choice questions in CommonsenseQA to binary questions by prompting Llama to transform the original question with its correct answer to a binary question with a gold answer of `yes', as detailed in \ref{sec:prompts}. We randomly sample 197, 234, and 456 questions from BoolQ, CommonsenseQA, and HotpotQA. StrategyQA has 228 questions in the dev set and 482 in the test set that can be successfully conceptualized.

For models, we consider three language models that have a relatively close performance on many popular benchmarks, namely GPT-3-Turbo~\citep{brown2020language}, Llama-70B-instruct~\citep{touvron2023llama} , and Mixtral-8x7B~\citep{jiang2024mixtral}.\footnote{We use gpt-3.5-turbo-0301 for GPT-3, \textit{upstage/Llama-2-70b-instruct} for llama, and \textit{mistralai/Mixtral-8x7B-Instruct-v0.1} for Mixtral. The last two models are taken from Huggingface and inferred with vLLM \citep{kwon2023efficient}.} 

All CoT inferences use the same set of few-shot prompts selected from StrategyQA. All experiments use a self-consistency K=10. We count draw cases (i.e., \#\textit{yes}=\#\textit{no}) as `unknown'. Temperature is 0.7, except for similar question generation, which uses 1.0 for diversity. All manual prompts and other parameters are tuned on samples from the StrategyQA dev set.

\subsection{Analytical Experiments}
\label{sec:analytical-results}
Table~\ref{tab:perf-conceptualize} shows our experiment results on three large language models' conceptual reasoning performance on four benchmarks. We see that all models drop performances using our conceptualization framework for analytical evaluation. Among these models, GPT-3.5 is the most consistent model, dropping 8.5\% on average, while Mixtral has a dramatic drop of 28\%, partially due to the lower execution success rate from poorly generated programs. These results show that although some language models claim to have on-par performances on many existing benchmarks, their core reasoning capabilities to not rely on induction are very different. This suggests that some models may show artificial high performances through imitation and memorization rather than actual generalization to reasoning tasks.

\subsection{Improvement Experiments}
Table~\ref{tab:perf-ours} shows the effectiveness of our proposed techniques in improving LLMs' conceptual reasoning capability. Our proposed analogical inference from similar questions improves 9.2\% with Llama and 7.7\% with Mixtral. Self-refinement further improves both models by over 11\%. Such improvements suggest that existing LLMs cannot efficiently make implicit analogies and find these trustworthy induction signals by themselves out of the box. We also observe that Llama achieves comparable performances on BoolQ and CommonsenseQA as CoT after self-refinement, indicating the potential to achieve high-performing, controllable, and unbiased reasoning. We include additional analysis in \S\ref{sec:more-analysis}.

\subsection{Human Analysis}
\label{sec:human-analysis}

\begin{table}[ht]
\centering
\small
\begin{tabular}{lcc}
\toprule
 & Correctness & Solvability \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}
StrategyQA / CommonsenseQA & 96 / 92 & 88 / 80 \\
% CommonsenseQA & 92 & 80 \\
\bottomrule
\end{tabular}
\caption{Human analysis on correctness (the percentage of questions that are correctly conceptualized by GPT-4), and solvability (the percentage of questions humans can solve with the first few interpretations of the abstract question).}
\label{tab:human}
\end{table}

We use two human analysis metrics to demonstrate that our conceptualized questions are reasonable for models to solve. Correctness is the percentage of questions where the semantic abstraction (i.e., the fine-grained semantic types) is correct. Solvability, on the other hand, measures the percentage of solvable questions with only the abstract form of the question. In \S\ref{sec:conceptualization}, we argued that as long as the correctness of semantic abstraction is guaranteed, models should, in theory, solve all questions, making solvability equal to correctness. 
% This is because models can generate a long solution considering all the corner cases and different angles. 
However, in our analysis, a question is considered solvable if the human expert can generate a solution without considering too many corner cases.
% that considers the specific case of the original question in their first thought, so abstract questions that are only solvable with hard corner cases are not counted. 

One of the authors conducted such analysis with 50 questions sampled from StrategyQA and CommonsenseQA, representing two distinct domains: factual and commonsense. Table~\ref{tab:human} shows the results of our human analysis. Our conceptualization correctness is high, ranging from 92\% to 96\%. Solvability is slightly lower, but it only represents the percentage of questions that can be solved within the initial thoughts of our human expert. 
% As argued above, machines should have a solvability score equal to correctness with longer solutions that cover more corner cases. 
Both metrics are lower on CommonsenseQA because these questions contain fewer named entities.

% \section{Analysis}
% \subsection{Error Analysis}
% \subsection{Effectiveness of Self-refinement}

\section{Conclusion}
We propose a novel conceptualization framework to investigate LLMs' ability to perform conceptual and high-level reasoning without the help of inductive biases from special nouns in the original question. Experiments show that existing LLMs perform much worse in conceptual reasoning than direct inference methods, dropping 9\% to 28\% across various models and benchmarks. To improve models' conceptual reasoning capability for unbiased inference, we propose two techniques that add trustworthy induction signals to the conceptual reasoning process, improving over 11\%. From an analytical perspective, our work suggests that models cannot sufficiently perform conceptual reasoning and efficiently make analogies to similar situations out of the box. To this end, our proposed improvement techniques that explicitly add back trustworthy analogical signals are shown to be effective, motivating future research on generalizable and unbiased reasoning and planning with minimum reliance on induction.

\bibliography{colm2024_conference}
\bibliographystyle{colm2024_conference}

\input{appendix.tex}

\end{document}
