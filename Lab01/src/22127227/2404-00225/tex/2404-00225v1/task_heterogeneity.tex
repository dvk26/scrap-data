\vspace{-1mm}
\section{Contrastive Learning for Task Heterogeneity}
\label{task_heterogeneity}
An ultimate goal of CL is to train foundation models to extract useful representations without human annotations.
The foundation models are usually trained through a pre-training and fine-tuning paradigm. 
% There are two types of tasks involved in CL: pre-training tasks and downstream tasks.
During pre-training, various characteristics of the data are injected into the models by pre-training tasks.
After pre-training, the models are fine-tuned to learn task-specific patterns of the downstream tasks.
In this section, we discuss the heterogeneous pre-training tasks and downstream tasks of CL.

\subsection{Pre-training Tasks}
Different pre-training tasks can guide models to capture different aspects of the data.
In general, there are four types of pre-training tasks, including \emph{pretext tasks} \cite{DBLP:conf/iclr/HjelmFLGBTB19,chen2020simple,perozzi2014deepwalk,DBLP:conf/icml/RadfordKHRGASAM21,franceschi2019unsupervised, DBLP:conf/cikm/ZhouZF0H22}, \emph{supervised tasks} \cite{abs-2004-11362,park2022fair,yang2022unified}, \emph{preference tasks} \cite{DBLP:journals/corr/abs-2310-13639, dennler2024using, shen2024improving, DBLP:conf/cikm/DuanFZLW23} and \emph{auxiliary tasks} \cite{pan2022contrastive,yang2022knowledge,ayush2021geography,chen2022learning,liu2023timesurl}.


\subsubsection{Pretext Tasks}
% The major advantage of pretext tasks is that they do not require the expansive and time-consuming human labeling.
The pretext tasks are the pre-training tasks without expensive and time-consuming human labels, and their objective is to discriminate positive and negative instance pairs, which are determined either by \emph{heuristics} \cite{DBLP:conf/iclr/HjelmFLGBTB19,chen2020simple,perozzi2014deepwalk,DBLP:conf/icml/RadfordKHRGASAM21,franceschi2019unsupervised} or \emph{extra models} \cite{li2020prototypical,jing2022x,DBLP:conf/nips/Tian0PKSI20,zhang2022fairness}.


\textbf{Heuristics Based Pair Construction}
% The naive pseudo labels are usually obtained by naive relations between two instances.
The heuristics-based methods construct contrastive pairs based on either simple relationships between instances or heuristically designed data augmentations.
For example, DIM \cite{DBLP:conf/iclr/HjelmFLGBTB19} treats a pair of local and global embeddings from the same image as positive and treats local and global embeddings from different images as negative.
SimCLR \cite{chen2020simple} treats two different augmented views of an image as a positive pair and treats two randomly sampled images as a negative pair.
% BiNE \cite{gao2018bine} treats the connected (user, item) pairs of the bipartite graphs as positive and treats randomly sampled pairs as negative.
DeepWalk \cite{perozzi2014deepwalk} leverages random walks to determine positive node pairs in a graph.
CLIP \cite{DBLP:conf/icml/RadfordKHRGASAM21} treats ground-truth image and text pairs as positive and other image and text pairs as negative.
SRL \cite{franceschi2019unsupervised} regards two adjacent sub-sequences in the same time series as a positive pair and two sub-sequences from different time series as a negative pair.



\textbf{Model Based Pair Construction}
Model-based methods leverage extra models, e.g., \emph{clustering}, \emph{view generation} and \emph{image editing} models, to generate contrastive pairs.
% The negative sampling process of CL inevitably include semantic errors \cite{li2020prototypical}, to address this issue, clustering is used to produce cluster pseudo labels.
For \emph{clustering},
ProtoNCE \cite{li2020prototypical} leverages external clustering methods, e.g., K-Means, to obtain semantic clusters, and uses the cluster centers to reduce the semantic errors of random negative sampling.
X-GOAL \cite{jing2022x} extends ProtoNCE to graphs.
% Sensitive attributes, such as gender and color, exist in real-world datasets.
% Fairness pretext tasks aim to exclude the information of the sensitive attributes in embeddings.
For \emph{view generation}, InfoMin \cite{DBLP:conf/nips/Tian0PKSI20} leverages flow-based models \cite{dinh2016density} to generate augmented views for an input image, and treats these generated views as positive pairs.
AutoGCL \cite{yin2022autogcl} and InfoTS \cite{luo2023time} extend InfoMin to graphs and time series.
For \emph{image editing},
FairCL \cite{zhang2022fairness} trains an image editor \cite{he2019attgan} to generate images with different sensitive labels, e.g., gender. 
The images generated from the same input image but having different sensitive labels are regarded as positive pairs.

\subsubsection{Supervised Tasks}
The data for supervised pre-training tasks is manually labeled before pre-training the models, which incorporates human knowledge.
SupCon \cite{abs-2004-11362} proposes to maximize the similarity of a pair of instances that share the same label.  
Sel-CL \cite{li2022selective} proposes to filter out noisy labels by selecting confident examples based on their representation similarity with their labels.
% FairCL \cite{zhang2022fairness} 
% Sensitive attributes, such as gender and color, exist in real-world datasets.
% Fairness pretext tasks aim to exclude the information of the sensitive attributes in embeddings.
FSCL \cite{park2022fair} introduces a Fair Supervised Contrastive Loss (FSCL) for visual representation learning based on SupCon, which defines the positive and negative pairs based on both class labels, e.g., attractiveness, and sensitive attribute labels, e.g., gender.
UniCL \cite{yang2022unified} unifies (image, label) and (image, text) pairs by expanding the label into a textual description, and then leverages image-to-text and text-to-image contrastive losses to pre-train the model. HeroCon~\cite{DBLP:conf/kdd/ZhengXZH22} proposes the weighted supervised contrastive loss to weight the importance of positive and negative pairs based on the similarity of different label vectors in the multi-label setting.

\subsubsection{Preference Tasks}
In recent years, Human-In-The-Loop (HITL) machine learning has become popular, which induces human prior knowledge into models by including humans in the training process. Different from supervised tasks, where humans first label the data and then the data is used to train the model, in HITL machine learning, humans iteratively evaluate the quality of the prediction made by the model and provide feedback to the model to adjust its learned knowledge. Specifically, ~\cite{DBLP:journals/corr/abs-2310-13639} derives contrastive preference loss for learning optimal behavior from human feedback using the regret-based model of human preferences. ~\cite{dennler2024using} proposes to combine CL loss to model exploratory actions and learn user preferences by utilizing the data collected from an interactive signal design process, where the data collection process can be regarded as the functionality of HITL. ~\cite{shen2024improving} introduces the contrastive rewards to penalize uncertainty and improve robustness based on human feedback. 
% ~\cite{DBLP:conf/prcv/BaiJLW23} proposes a preference CL loss to encode users' interests by contrasting preferences of user-items pairs.
% ~\cite{DBLP:conf/cikm/DuanFZLW23} designs a CL loss to model the long-term and short-term preferences and characterize the users' preferences. 

% \lc{I will add the related work soon.}
% preference-based learning. \url{https://icml.cc/virtual/2023/workshop/21495}

\subsubsection{Auxiliary Tasks}
The auxiliary tasks leverage external or metadata information to improve CL. 
For example,
Knowledge-CLIP \cite{pan2022contrastive} uses knowledge graph to guide CLIP \cite{DBLP:conf/icml/RadfordKHRGASAM21} to encode more precise semantics by tasks related to knowledge graphs e.g., link prediction.
KGCL \cite{yang2022knowledge} introduces a Knowledge Graph CL framework (KGCL) for the recommendation, which leverages knowledge graphs to provide side information of items via a knowledge-aware co-CL task.
GeoCL \cite{ayush2021geography} induces geo-location information into image embeddings by classifying geo-labels.
% MICoL \cite{zhang2022metadata} uses document metadata to build a meta-graph, and two documents are treated as a positive pair if they are reachable in the meta-graph. 
MUSER \cite{chen2022learning} uses text metadata, e.g., lyrics and album description, to learn better music sequence representations by aligning text tokens with music tokens as CLIP \cite{DBLP:conf/icml/RadfordKHRGASAM21}.
TimesURL \cite{liu2023timesurl} uses a reconstruction error to preserve important temporal variation information.
Additionally, other methods directly use downstream tasks as auxiliary tasks \cite{yu2022coca,DBLP:conf/cvpr/0010KBLY21,DBLP:conf/acl/PanWWL20,su2022contrastive,jing2023sterling}, which can also be regarded as \emph{multi-task learning} methods (see Sec. \ref{sec:connection}).

\subsection{Downstream Tasks}
\label{downstream_tasks}
The effectiveness of the CL methods is usually measured by their performance on a variety of downstream tasks.
In this subsection, we first briefly review representative downstream tasks and then discuss how to connect downstream tasks with CL strategies.

\subsubsection{Typical Downstream Tasks}
We briefly review the typical downstream tasks for different fields.

\textbf{Computer Vision.}
% \textit{Computer Vision}
Typical tasks include image classification~\cite{chen2020simple, he2020momentum, DBLP:journals/corr/abs-2003-04297, DBLP:conf/nips/GrillSATRBDPGAP20, DBLP:conf/iccv/CuiZ00J21, DBLP:conf/nips/Tian0PKSI20, DBLP:conf/cvpr/ColeYWAB22,rawat2017deep,zhou2021soda},
image clustering \cite{DBLP:conf/aaai/Li0LPZ021, tsai2020mice, li2020prototypical, wang2021dnb},
objective detection~\cite{DBLP:conf/cvpr/0019CZLCZ22, DBLP:conf/cvpr/ZhangC022, DBLP:conf/cvpr/SunLCYZ21, DBLP:conf/cvpr/BadamdorjR0C22,xie2021detco,wang2021dense}, image generation~\cite{DBLP:conf/cvpr/ZhanYWZLZ22, DBLP:conf/cvpr/DengYCWT20, DBLP:conf/cvpr/0010KBLY21,kang2020contragan}, style transfer~\cite{DBLP:conf/cvpr/HangXYL22, DBLP:conf/nips/ChenZWZZLXL21, DBLP:conf/iccv/YangHY23,park2020contrastive}, etc.

\textbf{Natural Language Processing.} Typical tasks include machine translation~\cite{DBLP:conf/emnlp/0003KLZKJ23, DBLP:conf/emnlp/IndurthiCAT23,  DBLP:conf/acl/OuyangY023, DBLP:conf/acl/Li0CKV22,pan2021contrastive,zhang2022frequency}, text classification~\cite{DBLP:conf/emnlp/Lin022, DBLP:conf/acl/YangYNGX23, DBLP:conf/acl/YuZQSWGWYN23,chen2022contrastnet,pan2022improved,choi2022c2l,wang2022contrastive,wang2021cross}, 
topic modeling \cite{li2022uctopic, nguyen2021contrastive,han2023unified,wang2020coarse,shi2021simple,vayansky2020review,jing2018cross},
text summarization~\cite{DBLP:conf/iconip/0003LZG023, DBLP:conf/ijcnn/ZhangSCXZL22, DBLP:conf/acl/WangCZQL21, DBLP:conf/cvpr/00040QBSW23,jing2021multiplex,xu2022sequence,liu2021simcls}, and information extraction~\cite{DBLP:conf/acl/GuoDCLZQ0YWP23, DBLP:conf/acl/LeeLZDPSZSGWALQ23, DBLP:conf/emnlp/LiZDM0Q23, ye2021contrastive}.

\textbf{Graph Learning.}
Typical tasks include node classification \cite{peng2020graph, wang2021self, jing2021hdmi,DBLP:conf/icml/YouCSW21,DBLP:conf/www/0001XYLWW21}, node clustering \cite{zhang2020commdgi, li2022graph, jing2022x, wang2022clusterscl,liu2023simple,zhao2021graph}, graph classification \cite{velickovic2019deep, sun2019infograph,xu2021infogcl,yin2023coco,luo2022dualgraph}, link prediction \cite{cao2021bipartite,shiao2022link,jing2022coin,he2017neural,kumar2020link}, recommendation \cite{xie2022contrastive,yu2022graph,wei2021contrastive,chen2023heterogeneous,jing2023sterling,yang2023generative,jiang2023adaptive}, knowledge graph reasoning \cite{wang2022simkgc, xu2023temporal, bordes2013translating, sun2018rotate, yan2021dynamic,ji2021survey} and anomaly detection \cite{liu2021anomaly,chen2022gccad,ma2021comprehensive}.

\textbf{Time Series Analysis}
Typical tasks include classification \cite{tonekaboni2020unsupervised,DBLP:conf/ijcai/Eldele0C000G21,zhang2022self,franceschi2019unsupervised,ismail2019deep}, forecasting \cite{DBLP:conf/aaai/YueWDYHTX22,woo2021cost, luo2023time,yang2022unsupervised,lim2021time,jing2022retrieval, zhou2024one,miller2024survey,jing2021network}, anomaly detection \cite{wang2023deep,kim2023contrastive, deldari2021time, yang2023dcdetector, blazquez2021review, ren2019time} and imputation \cite{liu2023timesurl,choi2023conditional,fang2020time, luo2018multivariate,cao2018brits, tashiro2021csdi,jing2024casper}.



\subsubsection{Connecting Downstream Tasks with CL strategies}\label{sec:connection}
Different CL strategies, e.g., different views and pre-training tasks, usually have disparate impact on downstream tasks \cite{DBLP:conf/nips/Tian0PKSI20,DBLP:conf/icml/YouCSW21,luo2023time,zheng2023simts,yeh2023toward}.
Therefore, a fundamental challenge to train foundation models lies in how to construct suitable CL strategies for the desired downstream tasks.
Given a downstream task and a set of available CL strategies, \emph{Automated Machine Learning (AutoML)} \cite{cubuk2018autoaugment,zoph2020learning,DBLP:conf/nips/Tian0PKSI20,yin2022autogcl,luo2023time,tang2023spatio,feng2022adversarial,jin2021automated,jing2024automated} and \emph{prompt learning} \cite{DBLP:conf/mm/ZouHS23,DBLP:conf/wsdm/Xu0QLXHH23, DBLP:conf/acl/AbaskohiRY23, DBLP:conf/acl/WangCZY23, DBLP:conf/acl/ZhengSYLPZXZ23,DBLP:conf/acl/ParanjapeMGHZ21} methods could be used to discover the optimal CL strategies.
Given the optimal CL strategies, one could either use these strategies to pre-train the model and then fine-tune on the downstream tasks, or train the model via \emph{multi-task learning} \cite{yu2022graph,yu2022coca,wang2023characterizing,DBLP:conf/cvpr/0010KBLY21,su2022contrastive,jing2023sterling} by combining the CL strategies with downstream tasks.
Additionally, some works also try to \emph{reformulate} \cite{abs-2004-11362,DBLP:conf/aaai/Li0LPZ021,do2021clustering,sun2018rotate,he2017neural,xie2022contrastive,qiu2021neural,reiss2023mean,eysenbach2022contrastive} the downstream tasks as CL tasks since they are inherently related.

\textbf{Automated Machine Learning.}
Being geared towards automating the procedure of machine learning, AutoML has gained a lot of attention in recent years \cite{he2021automl}.
AutoML methods formulate the problem of searching for the optimal CL strategies as a bi-level optimization problem \cite{cubuk2018autoaugment,DBLP:conf/icml/YouCSW21,jin2021automated}:
\begin{equation}\label{eq:automl}
\begin{split}
    {s}^* &= \arg\max \mathcal{R}(f_{\theta^*}, s)\\
    s.t. \,\theta^* &= \arg\min\mathcal{L}(f_\theta, s)
\end{split}
\end{equation}
where the lower-level problem is to minimize the loss $\mathcal{L}$ (e.g., cross-entropy loss) of the downstream task (e.g., classification) or a surrogate task (e.g., minimization of mutual information \cite{DBLP:conf/nips/Tian0PKSI20}) for the given model $f_\theta$ and the CL strategy $s$;
the upper-level problem is to maximize the validation reward $\mathcal{R}$ (e.g., accuracy) for the pair of the trained model and CL strategy $(f_{\theta^*}, s)$.


Existing AutoML methods can be categorized from two perspectives: 
\emph{search space} and \emph{search algorithms}.
In terms of the search space, existing methods mainly focus on \emph{data augmentations} \cite{cubuk2018autoaugment,li2020differentiable,DBLP:conf/icml/YouCSW21,suresh2021adversarial,reed2021selfaugment}, \emph{view constructions} \cite{DBLP:conf/nips/Tian0PKSI20,yin2022autogcl,luo2023time,tang2023spatio,feng2022adversarial}, \emph{pretext tasks} \cite{jin2021automated} and \emph{overall CL strategies} \cite{jing2024automated}. 
For \emph{data augmentations}, JOAO \cite{DBLP:conf/icml/YouCSW21} could automatically select the most challenging data augmentation pairs for graph data based on the current contrastive loss.
For \emph{view constructions}, InfoMin \cite{DBLP:conf/nips/Tian0PKSI20} argues that good contrastive views should retain information relevant to downstream tasks while minimizing irrelevant nuisances, which constructs the optimal views by minimizing the mutual information between different views.
InfoTS \cite{luo2023time} proposes an information theory-based criteria to evaluate fidelity and variety of the data augmentations for time series, and selects the optimal data augmentations by maximizing the scores derived from the criteria.
For \emph{pretext tasks},
AutoSSL \cite{jin2021automated} automatically searches for the optimal combination of pretext tasks for node clustering and node classification for graphs.
For \emph{overall CL strategies}, AutoCL \cite{jing2024automated} searches for all aspects of CL, including data augmentations, embedding augmentations, contrastive pair construction and loss functions for time series.

In terms of the search algorithms, existing methods are mainly based on \emph{reinforcement learning} \cite{cubuk2018autoaugment,zoph2020learning,jing2024automated}, 
\emph{adversarial learning} \cite{DBLP:conf/nips/Tian0PKSI20,DBLP:conf/icml/YouCSW21,yin2022autogcl,luo2023time,feng2024ariel,suresh2021adversarial,tang2023spatio},
\emph{evolution strategy} \cite{jin2021automated} and \emph{Bayesian optimization}~\cite{reed2021selfaugment}. 
For \emph{reinforcement learning}, AutoCL \cite{jing2024automated} uses a controller network to sample CL strategies and uses the model's performance on the validation set to design reward, where the controller is optimized by maximizing the reward $\mathcal{R}$ via reinforcement learning. 
For \emph{adversarial leanring}
AD-GCL \cite{suresh2021adversarial} measures the similarity of the node embeddings of two different graph views via mutual information, which formulates the lower-level and upper-level problems in Equation \eqref{eq:automl} as maximizing and minimizing the mutual information respectively.
ARIEL \cite{feng2022adversarial} uses the same contrastive loss for both lower-level loss $\mathcal{L}$ and the upper-level reward $\mathcal{R}$, and uses the adversarial attack to maximize $\mathcal{R}$.
For \emph{evolution strategy},
AutoSSL-ES \cite{jin2021automated} has adopted an evolution strategy \cite{hansen2003reducing} to obtain the optimal combination of CL strategies for graphs by optimizing the pseudo-homophily objective.
For \emph{Bayesian optimization},
SelfAugment \cite{reed2021selfaugment} leverages image rotation prediction as the lower-level task, and uses Bayesian optimization \cite{lim2019fast} as the search algorithm to obtain the optimal data augmentations.

\textbf{Prompt Learning.}
% \hh{missing?} \lc{I will add the related work soon.}
The contrastive-based prompt learning methods aim to combine CL with prompt learning for various purposes, such as maximizing the consistency of different representations~\cite{DBLP:conf/mm/ZouHS23}, enabling fine-tuning in few-shot or zero-shot setting~\cite{DBLP:conf/wsdm/Xu0QLXHH23, DBLP:conf/acl/AbaskohiRY23, DBLP:conf/acl/WangCZY23, DBLP:conf/acl/ZhengSYLPZXZ23}, and commonsense reasoning~\cite{DBLP:conf/acl/ParanjapeMGHZ21}. Specifically,~\cite{DBLP:conf/mm/ZouHS23} designs a multimodal prompt transformer to perform cross-modal information fusion and apply CL to maximize the consistency among the fused representation and the representation for each modality for the emotion recognition task. \cite{DBLP:conf/wsdm/Xu0QLXHH23, DBLP:conf/acl/AbaskohiRY23, DBLP:conf/acl/WangCZY23, DBLP:conf/acl/ZhengSYLPZXZ23} combine CL with prompt learning to fine-tune the model in the few-shot or zero-shot setting.
\cite{DBLP:conf/nips/0003KKW23} devises visual prompt-based CL and guided-attention-based prompt ensemble algorithms to task-learn specific state representations from multiple prompted embeddings. \cite{DBLP:conf/acl/ParanjapeMGHZ21} develops a list of contrastive generation prompts for the commonsense reasoning task. 


\textbf{Multi-Task Learning.}
When we have prior knowledge about what characteristics of the data can be brought by CL strategies, in addition to the pre-training and fine-tuning paradigm, another simple yet effective way to connect downstream tasks and CL strategies is to combine the pre-training tasks and downstream tasks as a multi-task learning task.
For example, 
CoCa \cite{yu2022coca} combines a contrastive loss with a caption generation loss to train image-text foundation models, where contrastive loss is used to learn global representations and captioning is used to learn fine-grained region-level features.
XMC-GAN \cite{DBLP:conf/cvpr/0010KBLY21} leverages contrastive losses for various pairs, such as (image, sentence) and (generated image, real image), to improve the alignment between them for the text-to-image generation task.
mRASP2 \cite{DBLP:conf/acl/PanWWL20} combines a contrastive loss with cross-entropy for multilingual machine translation, where the contrastive loss is adopted to minimize the representation gap of similar sentences and maximize that of irrelevant sentences.
SimCTG \cite{su2022contrastive} leverages a contrastive loss to encourage language models to learn discriminative and isotropic token representations for neural text generation.
SimGCL \cite{yu2022graph} discovers that InfoNCE loss helps models learn more evenly distributed user and item embeddings, which could mitigate the popularity bias. 
% SimGCL combines InfoNCE with a recommendation loss \cite{rendle2009bpr} to train the recommendation models.
STERLING \cite{jing2023sterling} combines the BYOL loss \cite{DBLP:conf/nips/GrillSATRBDPGAP20} with a co-clustering loss for co-clustering on bipartite graphs.





\textbf{Task Reformulation.}
Certain downstream tasks are inherently related to CL, such as \emph{classification}, \emph{clustering}, \emph{link prediction}, \emph{recommendation}, \emph{anomaly detection} and \emph{reinforcement learning}.
Therefore, the loss functions of these downstream tasks can be reformulated as a contrastive loss.
For example, in terms of \emph{classification}, the previously mentioned SupCon \cite{abs-2004-11362} integrates image class labels into self-supervised contrastive losses and proposes a Supervised Contrastive (SupCon) loss.
CLIP \cite{DBLP:conf/icml/RadfordKHRGASAM21} reformulates the image classification task as an image-text alignment contrastive task.
For \emph{clustering}, CC \cite{DBLP:conf/aaai/Li0LPZ021} introduces a CL-based clustering objective function, called contrastive clustering, by regarding the embedding vector of an instance as the soft cluster labels.
CRLC \cite{do2021clustering} reformulates the objective of clustering as a probability contrastive loss, which trains the parametric clustering classifier by contrasting positive and negative cluster probability pairs.
For \emph{link prediction}, since it is inherently a contrastive task: determining whether a pair of nodes is positive or not, most of the existing methods directly adopt CL losses as the objective functions to train the models \cite{ji2021survey}. For example, RotatE \cite{sun2018rotate} trains knowledge graph link prediction models by the negative sampling loss \cite{mikolov2013distributed}.
For \emph{recommendation}, it can be regarded as a link prediction task with ranking, and thus the training objectives are usually variants of contrastive losses \cite{yan2024reconciling,he2017neural}.
For example, CL4SRec \cite{xie2022contrastive} formulates the objective function of recommendation as a variant of InfoNCE \cite{oord2018representation}.
For \emph{anomaly detection}, 
NeuTraL \cite{qiu2021neural} directly adopts a contrastive loss as the loss function as well as the anomaly score.
MSC \cite{reiss2023mean} introduces a Mean-Shifted Contrastive (MSC) loss for one-class classification, which computes the angles between the image embeddings and the center of the normal embeddings.
For \emph{reinforcement learning}, Contrastive RL \cite{eysenbach2022contrastive} uses CL to directly perform goal-conditioned reinforcement learning by leveraging CL to estimate the Q-function for a certain policy and reward functions.

% In terms of classification, SupCon \cite{abs-2004-11362} integrates image class labels into self-supervised contrastive losses and proposes a Supervised Contrastive (SupCon) loss.

% In terms of clustering, CC \cite{DBLP:conf/aaai/Li0LPZ021} introduces a CL based clustering objective function, called contrastive clustering, by regarding the embedding vector of an instance as the soft cluster labels.
% CRLC \cite{do2021clustering} reformulates the objective of clustering as a probability contrastive loss, which trains the parametric clustering classifier by contrasting positive and negative cluster probability pairs.

% In terms of link prediction and recommendation, the loss functions of these two tasks are usually formulated as contrastive losses.
% For example, RotatE \cite{sun2018rotate} trains knowledge graph link prediction models by the negative sampling loss \cite{mikolov2013distributed}.
% NCF \cite{he2017neural} uses the cross-entropy loss to train recommendation systems, where an (user, item) pair is treated as a positive pair, i.e., labeled as True, if there's an interaction between them, otherwise, it is treated as a negative pair.
% CL4SRec \cite{xie2022contrastive} formulates the objective function of recommendation as a variant of InfoNCE \cite{oord2018representation}.



% In terms of anomaly detection, 
% NeuTraL \cite{qiu2021neural} directly adopts a contrastive loss as the loss function as well as the anomaly score.
% MSC \cite{reiss2023mean} introduces a Mean-Shifted Contrastive (MSC) loss for one-class classification, which computes the angles between the image embeddings and the center of the normal embeddings.
% DAD \cite{kopuklu2021driver} maximizes the similarity between the emebddings of normal driving behaviors and minimizing the similarity of normal and abnormal behaviors for driver anomaly detection.

% In terms of the RL,
% traditional RL algorithms are usually equipped with auxiliary perception losses to learn the characteristics of the environment
% \cite{eysenbach2022contrastive}.
% Recently, Contrastive RL \cite{eysenbach2022contrastive} uses CL to directly perform goal-conditioned RL by leveraging CL to estimate the Q-function for a certain policy and reward functions.