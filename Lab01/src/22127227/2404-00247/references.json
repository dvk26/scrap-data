{
    "2411-07934": {
        "paper_title": "Doubly Mild Generalization for Offline Reinforcement Learning",
        "authors": [
            "Yixiu Mao",
            "Qi Wang",
            "Yun Qu",
            "Yuhang Jiang",
            "Xiangyang Ji"
        ],
        "submission_date": "2024-11-12",
        "semantic_scholar_id": "c594010e0dbe6fac94caebb0578e3f22d1fe95bd"
    },
    "2308-02765": {
        "paper_title": "Surrogate Empowered Sim2Real Transfer of Deep Reinforcement Learning for ORC Superheat Control",
        "authors": [
            "Runze Lin",
            "Yangyang Luo",
            "Xialai Wu",
            "Junghui Chen",
            "Biao Huang",
            "Lei Xie",
            "Hongye Su"
        ],
        "submission_date": "2023-08-05",
        "semantic_scholar_id": "562559e63e554884873a00ce585cb1ecf45b32c6"
    },
    "2305-03360": {
        "paper_title": "A Survey on Offline Model-Based Reinforcement Learning",
        "authors": [
            "Haoyang He"
        ],
        "submission_date": "2023-05-05",
        "semantic_scholar_id": "dcd1770f1f107057917144e852a9950bd9dd5d32"
    },
    "2212-02179": {
        "paper_title": "Physics-Informed Model-Based Reinforcement Learning",
        "authors": [
            "Adithya Ramesh",
            "Balaraman Ravindran"
        ],
        "submission_date": "2022-12-05",
        "semantic_scholar_id": "4c62574c2900a8201b5201aab062fe5ee9625297"
    },
    "2203-09661": {
        "paper_title": "Meta-reinforcement learning for the tuning of PI controllers: An offline approach",
        "authors": [
            "Daniel G. McClement",
            "Nathan P. Lawrence",
            "J. Backstr√∂m",
            "Philip D. Loewen",
            "M. Forbes",
            "R. B. Gopaluni"
        ],
        "submission_date": "2022-03-17",
        "semantic_scholar_id": "e24c2c88979024d45ffe62530c1119efd7692a7c"
    },
    "2203-01387": {
        "paper_title": "A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems",
        "authors": [
            "Rafael Figueiredo Prudencio",
            "M. Maximo",
            "E. Colombini"
        ],
        "submission_date": "2022-03-02",
        "semantic_scholar_id": "61e7a3d5606043594a8ce377870479f77a6b58c2"
    },
    "2111-07171": {
        "paper_title": "Deep Reinforcement Learning with Shallow Controllers: An Experimental Application to PID Tuning",
        "authors": [
            "Nathan P. Lawrence",
            "M. Forbes",
            "Philip D. Loewen",
            "Daniel G. McClement",
            "Johan U. Backstrom",
            "R. B. Gopaluni"
        ],
        "submission_date": "2021-11-13",
        "semantic_scholar_id": "d325aa9305ec92a75511dff3c5b2cdcc8b6c3953"
    },
    "2103-14060": {
        "paper_title": "A Meta-Reinforcement Learning Approach to Process Control",
        "authors": [
            "Daniel G. McClement",
            "Nathan P. Lawrence",
            "Philip D. Loewen",
            "M. Forbes",
            "Johan U. Backstrom",
            "R. B. Gopaluni"
        ],
        "submission_date": "2021-03-25",
        "semantic_scholar_id": "e813e2a1c5b82b7e609ce0b9a7af90061373b542"
    },
    "2102-06177": {
        "paper_title": "Multi-Task Reinforcement Learning with Context-based Representations",
        "authors": [
            "Shagun Sodhani",
            "Amy Zhang",
            "Joelle Pineau"
        ],
        "submission_date": "2021-02-11",
        "semantic_scholar_id": "96b78897fba37282038bde12e48f8995d1276008"
    },
    "2009-13303": {
        "paper_title": "Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey",
        "authors": [
            "Wenshuai Zhao",
            "J. P. Queralta",
            "Tomi Westerlund"
        ],
        "submission_date": "2020-09-24",
        "semantic_scholar_id": "5a1b92aa50797a7c1e99b8840ff01aad66038596"
    },
    "2006-04779": {
        "paper_title": "Conservative Q-Learning for Offline Reinforcement Learning",
        "authors": [
            "Aviral Kumar",
            "Aurick Zhou",
            "G. Tucker",
            "S. Levine"
        ],
        "submission_date": "2020-06-08",
        "semantic_scholar_id": "28db20a81eec74a50204686c3cf796c42a020d2e"
    },
    "2005-01643": {
        "paper_title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems",
        "authors": [
            "S. Levine",
            "Aviral Kumar",
            "G. Tucker",
            "Justin Fu"
        ],
        "submission_date": "2020-05-04",
        "semantic_scholar_id": "5e7bc93622416f14e6948a500278bfbe58cd3890"
    },
    "1910-10897": {
        "paper_title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
        "authors": [
            "Tianhe Yu",
            "Deirdre Quillen",
            "Zhanpeng He",
            "Ryan C. Julian",
            "Karol Hausman",
            "Chelsea Finn",
            "S. Levine"
        ],
        "submission_date": "2019-10-24",
        "semantic_scholar_id": "0bc855f84668b35cb65618d996d09f6e434d28c9"
    },
    "2004-05490": {
        "paper_title": "Deep Reinforcement Learning for Process Control: A Primer for Beginners",
        "authors": [
            "P StevenSpielberg",
            "Aditya Tulsyan",
            "Nathan P. Lawrence",
            "Philip D. Loewen",
            "R. B. Gopaluni"
        ],
        "submission_date": "2020-04-11",
        "semantic_scholar_id": "797ddbd5c7fde64c562380c6b59788777a182e32"
    },
    "1906-04005": {
        "paper_title": "Safe Reinforcement Learning Using Robust MPC",
        "authors": [
            "Mario Zanon",
            "S. Gros"
        ],
        "submission_date": "2019-06-10",
        "semantic_scholar_id": "5020d1dce1392dd6c5621ac55fa573b3aa541eaa"
    },
    "1904-07292": {
        "paper_title": "Reinforcement Learning for Batch Bioprocess Optimization",
        "authors": [
            "Panagiotis Petsagkourakis",
            "I. O. Sandoval",
            "E. Bradford",
            "Dongda Zhang",
            "E. A. Rio-Chanona"
        ],
        "submission_date": "2019-04-15",
        "semantic_scholar_id": "7b8c9353e5f7ea1698d17fb42ff11315c5930f28"
    },
    "1904-04152": {
        "paper_title": "Data-Driven Economic NMPC Using Reinforcement Learning",
        "authors": [
            "S. Gros",
            "Mario Zanon"
        ],
        "submission_date": "2019-04-08",
        "semantic_scholar_id": "a4dcf3fa6379f86627e80a0f8467dd7932fc64a4"
    },
    "1903-08254": {
        "paper_title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables",
        "authors": [
            "Kate Rakelly",
            "Aurick Zhou",
            "Deirdre Quillen",
            "Chelsea Finn",
            "S. Levine"
        ],
        "submission_date": "2019-03-19",
        "semantic_scholar_id": "4625628163a2ee0e6cd320cd7a14b4ccded2a631"
    },
    "1805-00909": {
        "paper_title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review",
        "authors": [
            "S. Levine"
        ],
        "submission_date": "2018-05-02",
        "semantic_scholar_id": "6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba"
    },
    "1710-11248": {
        "paper_title": "Learning Robust Rewards with Adversarial Inverse Reinforcement Learning",
        "authors": [
            "Justin Fu",
            "Katie Luo",
            "S. Levine"
        ],
        "submission_date": "2017-10-30",
        "semantic_scholar_id": "5e2c4e7b3302549b3718601c44d9af6c7554efef"
    }
}