\section{Introduction}
In the explosion of generative AI in the last couple years, more and more use cases are lending themselves towards large language model (LLM) applicability. Typically an LLM application developer is faced with the question of how to best adapt some particular model for their downstream task; this question usually involves distinguishing among the techniques of few-shot learning or prompt engineering, retrieval augmented generation (RAG) \citep{NEURIPS2020_6b493230}, supervised fine-tuning (SFT), some variant of reinforcement learning for human feedback (RLHF) \citep{schulman2017proximal, bai2022constitutional}, or perhaps some combination of these techniques \citep{balaguer2024rag}.

The motivation for this paper lies in the recognition that despite the impressive capabilities of pre-trained LLMs, they are inherently constrained by the scope and recency of their training data. Internet-scale corpora, which form the foundation of LLM pre-training, are by nature finite snapshots, limited both temporally and in their breadth of covered knowledge. This limitation poses a significant hurdle for applications that necessitate up-to-date information or domain-specific knowledge that postdates the model's training cut-off or falls outside the scope of the training corpus.

Moreover, the dynamic and continuously evolving landscape of human knowledge further complicates the challenge, as new events unfold and specialized domains generate proprietary or niche content that is not publicly accessible. Consequently, developers aiming to deploy LLMs in scenarios that demand current or specialized knowledge must devise strategies for domain adaptation that effectively incorporate this new information into the model.

While RAG \citep{NEURIPS2020_6b493230} provides an ingenious workaround by augmenting model responses with an external knowledge base, this approach sidesteps the core issue of assimilating new knowledge directly into the model itself. Thus, we are compelled to explore alternative methods that enable LLMs to internalize and retain new information through direct training interventions.

Herein lies the crux of our inquiry: how can we construct a training dataset from a body of documents that facilitates the learning of new knowledge through simple SFT techniques? Addressing this question is not only of theoretical interest but also carries significant practical implications for the deployment of LLMs in real-world settings where accuracy, currency, and domain specificity are paramount.

Our investigation into the domain of knowledge ingestion via direct training has yielded several notable contributions:

\begin{enumerate}
\item \textbf{Analysis of Token-based Q\&A Dataset Generation:} We provide a comprehensive evaluation of the standard practice of token-based Q\&A dataset generation. Our findings reveal that this method may not ensure complete or uniform coverage of new knowledge within a document corpus. This observation is critical as it underscores the potential limitations of prevailing dataset preparation strategies and highlights the need for more targeted approaches to knowledge incorporation.


\item \textbf{Development of a Fact-based Generation Process:} In response to the identified shortcomings, we propose a fact-based generation process. This methodology prioritizes the even distribution of attention across all salient facts within a source document, ensuring that each piece of information is adequately represented in the training data. This approach stands to significantly enhance the model's ability to internalize diverse and detailed knowledge from domain-specific corpora.  

\item \textbf{Empirical Validation of SFT for New Knowledge Learning:} We demonstrate that even straightforward SFT can lead to substantial improvements in model performance when handling out-of-domain, post-cutoff knowledge. Our results not only validate the effectiveness of our proposed fact-based generation process but also provide a compelling case for the practicality of SFT as a tool for domain adaptation in LLMs. This contribution has profound implications for the application of generative AI in dynamic fields where staying abreast of the latest information is crucial.  
  
\item \textbf{Benchmarking against Retrieval-Augmented Models:} We extend our study to include a benchmark comparison between our SFT models and those employing RAG. This analysis provides insights into the trade-offs and relative merits of direct training versus retrieval-based augmentation, offering valuable guidance for practitioners in the selection and implementation of knowledge ingestion methodologies.  
  
\item \textbf{Exploration of Hyperparameter Sensitivity:} Recognizing that the tuning of hyperparameters is often a nuanced and impactful aspect of model training, we delve into the sensitivity of our models to a few hyperparameter settings. Our exploration sheds light on the robustness of our findings and sets the stage for future work on optimization strategies tailored to knowledge ingestion tasks.  
\end{enumerate}

Here, we illustrate not only the breadth of our contributions but also their potential implications for the field of AI and LLM application development. By doing so, we aim to provide a thorough understanding of the impact of our research and its relevance to both academic inquiry and practical application.