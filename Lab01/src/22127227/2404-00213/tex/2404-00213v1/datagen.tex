\section{Dataset Generation}
\label{sec:datagen}
We identify six Wikipedia articles, five about recent sporting championships -- 2023 Cricket World Cup \citep{enwiki:1213361100}, 2023 American Football Superbowl \citep{enwiki:1215541265}, 2023 FIFA Women's World Cup \citep{enwiki:1214162630}, 2023 PGA Championship \citep{enwiki:1213024529}, 2023 NCAA Men's Basketball Division I Tournament \citep{enwiki:1215798602} -- that were post-cutoff and one about a pre-cutoff sports event, the 2018 FIFA World Cup \citep{enwiki:1216022372}. Sporting events were chosen as they are fact/statistic rich, easy to understand, and related information is mostly black-and-white for being pre- or post-cutoff. For each of the documents, we use the text extracts API with explaintext enabled to fetch the plain text portions of each section, returned in JSON format. We filter and clean the sections, removing any blank ones or ones without meaningful plain text contribution (such as those that are entirely table-based save for some annotations).

For each of our cleaned documents, we generate two types of datasets: token-based scaling and fact-based scaling. The motivation for having two sets of datasets is detailed in section \ref{sec:tokenfactcov}. We endeavor to scale Q\&A datasets with different questions covering the same content -- as opposed to simply training on the same data for more steps -- as the increased examples will allow the model to see the contained target knowledge in more ways. This varied repetition increases the effectiveness of learning while mitigating overfitting as much as possible that would otherwise lead to significant performance degradation \citep{ovadia2024finetuning}. We do not want the model to memorize the phrasing of the source document, but rather the knowledge contained within it.

\subsection{Token-based}
Our steps for generating the token-based scaling datasets are:
\begin{itemize}
    \item Initialize an empty question bank and seed it with one Q\&A pair written manually based on the article's overview section.
    \item For each section in the document, calculate its token count using OpenAI's tiktoken library. Repeatedly query base GPT-4 with temperature 1 and top\_p=0.95 (which we use for all dataset generation tasks in this section) to generate question-and-answer pairs until the generated token count between unique questions and answers for that section surpasses 10 times our source section tokens.
    \item We are able to create our per-section 1x and 5x datasets using a subset of the 10x generations. All questions are unique from an exact match standpoint.
    \item The 1x, 5x, and 10x fine-tuning datasets are created from the union of the per-section ones for each respective scaling factor.
\end{itemize}

The evaluation sets for each document were generated according to an identical procedure, to 1x token scale. All questions were again ensured to be unique and not contained within the train set already.

\subsection{Fact-based}
For fact-based scaling, first we needed to generate the list of atomic facts contained in the documents. This was done by querying GPT-4 with the cleaned document sections from before. Then, for each document:
\begin{itemize}
    \item We iterate over the atomic facts and generate 10 unique question-answer pairs by querying GPT-4.
    \item Pairs are only accepted if they are not already present in the question bank so far so as to avoid duplication, else a new pair is generated.
    \item From this 10x set, we create the 1x and 5x scaling subsets.
\end{itemize}

In the fact-based dataset generation prompt, we give GPT-4 the option to skip Q\&A generation if the fact is too broad, unclear, or otherwise not related to the topic of the document. We manually inspect the few such cases where the model elects to follow this option. We either filter out those atomic facts if they are indeed unrelated or, if they are kept, regenerate question-answer pairs with the ``skip" prompt option removed.

An example of one such fact that GPT-4 chose to skip is "Russia is a nation that lost millions of lives in fighting Nazism", which was generated for the 2018 FIFA World Cup article that was hosted in Russia. A statement along these lines was mentioned in a quote in the original document, and the model (correctly) deemed this too unspecific to the sports event to be included. An example fact that GPT-4 skipped which we still generated question-answer pairs for would be for the American football Superbowl LVII article: "In February 2022, over 200 liberal religious leaders petitioned NFL Commissioner Roger Goodell." While seemingly less related, this concerned some calls to move the location of Superbowl LVII from its original Arizona venue.

Fact-based evaluation sets were generated on the accepted set of atomic facts using the prompt without the skip option, deduplicated on an exact match criterion, and at 1x scale.

We include samples of each of the prompts used for dataset generation in appendix \ref{sec:sampleprompts}. Samples of atomic facts identified for each article are listed in appendix \ref{sec:atomicfacts}.