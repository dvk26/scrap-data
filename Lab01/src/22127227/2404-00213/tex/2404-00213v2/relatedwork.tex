\section{Related Work}
There are several bodies of related work on the topic of post-training domain adaptation for language models, with emphasis on factuality. One such body is the study of knowledge injection, which sets out with the same goal of encoding new knowledge into language model systems \citep{wang2020kadapter, Chen_2022}. Past popular knowledge injection methods might involve retrieval/augmentation using some external knowledge base \citep{fan2020enhanced, song2016better}; the knowledge base need not be a vector database as is state-of-practice in modern RAG systems \citep{NEURIPS2020_6b493230}, instead falling to alternative constructions like knowledge graphs \citep{martino2023knowledge}. Another common formulation of knowledge injection involves using model adapters, such as \cite{lauscher2020common}, where only the adapter parameters meant to encode the domain adaptation are left unfrozen during training in an attempt to mitigate catastrophic forgetting.

Catastrophic forgetting, or the tendency of models to freely overwrite structure learned for any previous task in the learning of some subsequent one \citep{MCCLOSKEY1989109}, is itself of particular import for our objective. Avoiding such forgetting partially motivates the complexities of external augmentation techniques or partially-frozen adapter training.

This is not to say direct training approaches are avoided altogether, however. \cite{xu-etal-2023-kilm} tries continued pretraining using a knowledge infill objective to boost factual retention. \cite{tian2023finetuning} uses the constrained optimization of direct preference optimization \citep{rafailov2023direct} to encourage factuality during post-training. We take particular interest in \cite{ovadia2024finetuning}, which leverages unsupervised training on chunks of post-cutoff Wikipedia articles, paraphrased some number of times, to teach the model new knowledge. Their approach of using careful repetition to boost FT performance matches our choice in this study, though we differ in our techniques of supervised vs. unsupervised learning, where they choose unsupervised to effectively do continued pretraining for knowledge injection. \cite{ovadia2024finetuning} also does not consider the fact density of the underlying corpuses they set out to learn, aligning more with our token-based scaling as outlined in section \ref{sec:datagen}.