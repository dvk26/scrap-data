\documentclass{article}
\usepackage{PRIMEarxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\usepackage{listings}
\usepackage{mdframed}
\usepackage[round]{natbib}
\usepackage{comment}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{float}
\usepackage{xcolor}

\usepackage{bm}
\usepackage{tikz} % for checkmark
\usepackage{pifont} % for xmark
\usepackage{amssymb}
\usepackage{amsmath,upgreek}
\usepackage{titlesec}
\usepackage{subcaption}
\usepackage[title]{appendix}
\usepackage{stfloats}
\usepackage{multicol,subeqnarray}
\usepackage[most]{tcolorbox}
\definecolor{block-gray}{gray}{0.85}

\definecolor{xlinkcolor}{cmyk}{1,0.6,0,0}
\usepackage[bookmarks=false,         % show bookmarks bar?
     pdfnewwindow=true,      % links in new window
     colorlinks=true,    % false: boxed links; true: colored links
     linkcolor=xlinkcolor,     % color of internal links
     citecolor=xlinkcolor,     % color of links to bibliography
     filecolor=xlinkcolor,  % color of file links
     urlcolor=xlinkcolor,      % color of external links
final=true
]{hyperref}
\graphicspath{{img/}}     % organize your images and other figures under media/ folder
\usepackage{booktabs} % For prettier tables  
\usepackage{array} % for centering column content  
\newfloat{listing}{htbp}{loa}  
\floatname{listing}{Listing} 

\lstset{  
  basicstyle=\ttfamily\small,  
  breaklines=true,  
  frame=none,  
  language={},  
  keywordstyle=\color{black}\bfseries,  
  commentstyle=\color{green},  
  stringstyle=\color{red},  
  numberstyle=\tiny\color{gray},  
  rulecolor=\color{black},  
  showspaces=false,  
  showstringspaces=false,  
  showtabs=false,  
  tabsize=4,  
  columns=fullflexible,  
  escapeinside={(*@}{@*)},  
  morecomment=[l]{\#}, 
}  

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}


%% 
\title{Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning}

\author{
   \\
  \textbf{Microsoft}\\
Nick Mecklenburg, 
Yiyou Lin, 
Xiaoxiao Li, 
Daniel Holstein,  
Leonardo Nunes, \\
Sara Malvar,
Bruno Silva, 
Ranveer Chandra,
Vijay Aski,
Pavan Kumar Reddy Yannam,
Tolga Aktas,
Todd Hendry
}

\begin{document}
\maketitle


\begin{abstract}
    In recent years, Large Language Models (LLMs) have shown remarkable performance in generating human-like text, proving to be a valuable asset across various applications. However, adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date. This paper investigates the effectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge injection in LLMs, specifically focusing on the domain of recent sporting events. We compare different dataset generation strategies – token-based and fact-based scaling – to create training data that helps the model learn new information. Our experiments on GPT-4 \citep{openai2024gpt4} demonstrate that while token-based scaling can lead to improvements in Q\&A accuracy, it may not provide uniform coverage of new knowledge. Fact-based scaling, on the other hand, offers a more systematic approach to ensure even coverage across all facts. We present a novel dataset generation process that leads to more effective knowledge ingestion through SFT, and our results show considerable performance improvements in Q\&A tasks related to out-of-domain knowledge. This study contributes to the understanding of domain adaptation for LLMs and highlights the potential of SFT in enhancing the factuality of LLM responses in specific knowledge domains.
\end{abstract}

\keywords{knowledge injection, supervised fine tuning, large language models}


\input{introduction}
\input{relatedwork}
\input{datagen}
\input{methodology}
\input{evaluation}
\input{results}
\input{limitations}

\clearpage
\bibliographystyle{plainnat}
\bibliography{paper}

\input{appendices}


\end{document}
