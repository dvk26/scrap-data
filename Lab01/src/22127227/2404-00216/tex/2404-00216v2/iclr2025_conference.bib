@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{chatgpt,
	author = {OpenAI},
	journal = {OpenAI blog},
	title = {large-scale generative pre-training model for conversation},
	url = {https://openai.com/blog/chatgpt},
	year = {2022},
	bdsk-url-1 = {https://openai.com/blog/chatgpt}}


@misc{openai2023gpt4,
	archiveprefix = {arXiv},
	author = {OpenAI},
	eprint = {2303.08774},
	primaryclass = {cs.CL},
	title = {GPT-4 Technical Report},
	year = {2023}}

@article{DBLP:journals/corr/abs-2302-13971,
	author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie{-}Anne Lachaux and Timoth{\'{e}}e Lacroix and Baptiste Rozi{\`{e}}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aur{\'{e}}lien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2302-13971.bib},
	doi = {10.48550/ARXIV.2302.13971},
	eprint = {2302.13971},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 28 Aug 2023 21:26:20 +0200},
	title = {LLaMA: Open and Efficient Foundation Language Models},
	url = {https://doi.org/10.48550/arXiv.2302.13971},
	volume = {abs/2302.13971},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2302.13971},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2302.13971}}


@misc{touvron2023llama,
	archiveprefix = {arXiv},
	author = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
	eprint = {2307.09288},
	primaryclass = {cs.CL},
	title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
	year = {2023}}

@misc{huang2023survey,
	archiveprefix = {arXiv},
	author = {Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
	eprint = {2311.05232},
	primaryclass = {cs.CL},
	title = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
	year = {2023}}


@misc{zhang2023sirens,
	archiveprefix = {arXiv},
	author = {Yue Zhang and Yafu Li and Leyang Cui and Deng Cai and Lemao Liu and Tingchen Fu and Xinting Huang and Enbo Zhao and Yu Zhang and Yulong Chen and Longyue Wang and Anh Tuan Luu and Wei Bi and Freda Shi and Shuming Shi},
	eprint = {2309.01219},
	primaryclass = {cs.CL},
	title = {Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models},
	year = {2023}}

@article{chen2023combating,
  title={Combating misinformation in the age of llms: Opportunities and challenges},
  author={Chen, Canyu and Shu, Kai},
  journal={arXiv preprint arXiv:2311.05656},
  year={2023}
}

@article{zhu2020modifying,
  title={Modifying memories in transformer models},
  author={Zhu, Chen and Rawat, Ankit Singh and Zaheer, Manzil and Bhojanapalli, Srinadh and Li, Daliang and Yu, Felix and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2012.00363},
  year={2020}
}

@article{meng2022mass,
  title={Mass-editing memory in a transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2210.07229},
  year={2022}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@inproceedings{mitchell2022memory,
  title={Memory-based model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={15817--15831},
  year={2022},
  organization={PMLR}
}

@article{sinitsin2020editable,
  title={Editable neural networks},
  author={Sinitsin, Anton and Plokhotnyuk, Vsevolod and Pyrkin, Dmitriy and Popov, Sergei and Babenko, Artem},
  journal={arXiv preprint arXiv:2004.00345},
  year={2020}
}

@article{yao2023editing,
  title={Editing large language models: Problems, methods, and opportunities},
  author={Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng, Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  journal={arXiv preprint arXiv:2305.13172},
  year={2023}
}

@article{huang2023transformer,
  title={Transformer-patcher: One mistake worth one neuron},
  author={Huang, Zeyu and Shen, Yikang and Zhang, Xiaofeng and Zhou, Jie and Rong, Wenge and Xiong, Zhang},
  journal={arXiv preprint arXiv:2301.09785},
  year={2023}
}

@article{de2021editing,
  title={Editing factual knowledge in language models},
  author={De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  journal={arXiv preprint arXiv:2104.08164},
  year={2021}
}

@article{cohen2024evaluating,
  title={Evaluating the ripple effects of knowledge editing in language models},
  author={Cohen, Roi and Biran, Eden and Yoran, Ori and Globerson, Amir and Geva, Mor},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={283--298},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{zhong2023mquake,
  title={Mquake: Assessing knowledge editing in language models via multi-hop questions},
  author={Zhong, Zexuan and Wu, Zhengxuan and Manning, Christopher D and Potts, Christopher and Chen, Danqi},
  journal={arXiv preprint arXiv:2305.14795},
  year={2023}
}

@article{wang2024deepedit,
  title={DeepEdit: Knowledge Editing as Decoding with Constraints},
  author={Wang, Yiwei and Chen, Muhao and Peng, Nanyun and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2401.10471},
  year={2024}
}

@article{shi2024retrieval,
  title={Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models},
  author={Shi, Yucheng and Tan, Qiaoyu and Wu, Xuansheng and Zhong, Shaochen and Zhou, Kaixiong and Liu, Ninghao},
  journal={arXiv preprint arXiv:2403.19631},
  year={2024}
}

@inproceedings{li-etal-2023-contrastive,
	abstract = {Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, inco- herence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and significantly outperforms four strong decoding algorithms (e.g., nucleus, top-k) in automatic and human evaluations across wikipedia, news and story domains.},
	address = {Toronto, Canada},
	author = {Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori and Zettlemoyer, Luke and Lewis, Mike},
	booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	doi = {10.18653/v1/2023.acl-long.687},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	pages = {12286--12312},
	publisher = {Association for Computational Linguistics},
	title = {Contrastive Decoding: Open-ended Text Generation as Optimization},
	url = {https://aclanthology.org/2023.acl-long.687},
	year = {2023},
	bdsk-url-1 = {https://aclanthology.org/2023.acl-long.687},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2023.acl-long.687}}

@article{chuang2023dola,
	author = {Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James and He, Pengcheng},
	journal = {arXiv preprint arXiv:2309.03883},
	title = {DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models},
	year = {2023}}

@article{zhang2023alleviating,
  title={Alleviating hallucinations of large language models through induced hallucinations},
  author={Zhang, Yue and Cui, Leyang and Bi, Wei and Shi, Shuming},
  journal={arXiv preprint arXiv:2312.15710},
  year={2023}
}

@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}

@article{fan2018hierarchical,
  title={Hierarchical neural story generation},
  author={Fan, Angela and Lewis, Mike and Dauphin, Yann},
  journal={arXiv preprint arXiv:1805.04833},
  year={2018}
}

@article{hu2023survey,
  title={A survey of knowledge enhanced pre-trained language models},
  author={Hu, Linmei and Liu, Zeyi and Zhao, Ziwang and Hou, Lei and Nie, Liqiang and Li, Juanzi},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2023},
  publisher={IEEE}
}

@article{pan2024unifying,
  title={Unifying large language models and knowledge graphs: A roadmap},
  author={Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2024},
  publisher={IEEE}
}

@article{huang2023look,
  title={Look before you leap: An exploratory study of uncertainty measurement for large language models},
  author={Huang, Yuheng and Song, Jiayang and Wang, Zhijie and Chen, Huaming and Ma, Lei},
  journal={arXiv preprint arXiv:2307.10236},
  year={2023}
}

@inproceedings{feng2024retrieval,
  title={Retrieval-generation synergy augmented large language models},
  author={Feng, Zhangyin and Feng, Xiaocheng and Zhao, Dezhi and Yang, Maojin and Qin, Bing},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={11661--11665},
  year={2024},
  organization={IEEE}
}

@article{madaan2022memory,
  title={Memory-assisted prompt editing to improve gpt-3 after deployment},
  author={Madaan, Aman and Tandon, Niket and Clark, Peter and Yang, Yiming},
  journal={arXiv preprint arXiv:2201.06009},
  year={2022}
}

@article{zheng2023can,
  title={Can We Edit Factual Knowledge by In-Context Learning?},
  author={Zheng, Ce and Li, Lei and Dong, Qingxiu and Fan, Yuxuan and Wu, Zhiyong and Xu, Jingjing and Chang, Baobao},
  journal={arXiv preprint arXiv:2305.12740},
  year={2023}
}


@article{mitchell2021fast,
  title={Fast model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2110.11309},
  year={2021}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}


@article{wang2023easyedit,
  title={Easyedit: An easy-to-use knowledge editing framework for large language models},
  author={Wang, Peng and Zhang, Ningyu and Xie, Xin and Yao, Yunzhi and Tian, Bozhong and Wang, Mengru and Xi, Zekun and Cheng, Siyuan and Liu, Kangwei and Zheng, Guozhou and others},
  journal={arXiv preprint arXiv:2308.07269},
  year={2023}
}


@article{li2024understanding,
  title={Understanding and Patching Compositional Reasoning in LLMs},
  author={Li, Zhaoyi and Jiang, Gangwei and Xie, Hong and Song, Linqi and Lian, Defu and Wei, Ying},
  journal={arXiv preprint arXiv:2402.14328},
  year={2024}
}

@article{kang2024comparing,
  title={Comparing hallucination detection metrics for multilingual generation},
  author={Kang, Haoqiang and Blevins, Terra and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2402.10496},
  year={2024}
}

@article{yang2024kg,
  title={KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques},
  author={Yang, Rui and Liu, Haoran and Zeng, Qingcheng and Ke, Yu He and Li, Wanxin and Cheng, Lechao and Chen, Qingyu and Caverlee, James and Matsuo, Yutaka and Li, Irene},
  journal={arXiv preprint arXiv:2403.05881},
  year={2024}
}

@article{song2024fmint,
  title={FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model},
  author={Song, Zezheng and Yuan, Jiaxin and Yang, Haizhao},
  journal={arXiv preprint arXiv:2404.14688},
  year={2024}
}

@misc{mei2024slang,
      title={SLANG: New Concept Comprehension of Large Language Models}, 
      author={Lingrui Mei and Shenghua Liu and Yiwei Wang and Baolong Bi and Xueqi Cheng},
      year={2024},
      eprint={2401.12585},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{xu2024editing,
  title={Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models},
  author={Xu, Derong and Zhang, Ziheng and Zhu, Zhihong and Lin, Zhenxi and Liu, Qidong and Wu, Xian and Xu, Tong and Zhao, Xiangyu and Zheng, Yefeng and Chen, Enhong},
  journal={arXiv preprint arXiv:2402.18099},
  year={2024}
}

@misc{bi2024decoding,
  title={Decoding by Contrasting Knowledge: Enhancing LLMs' Confidence on Edited Facts}, 
  author={Baolong Bi and Shenghua Liu and Lingrui Mei and Yiwei Wang and Pengliang Ji and Xueqi Cheng},
  year={2024},
  eprint={2405.11613},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{tonmoy2024comprehensive,
	archiveprefix = {arXiv},
	author = {S. M Towhidul Islam Tonmoy and S M Mehedi Zaman and Vinija Jain and Anku Rani and Vipula Rawte and Aman Chadha and Amitava Das},
	eprint = {2401.01313},
	primaryclass = {cs.CL},
	title = {A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models},
	year = {2024}}

@article{kaddour2023challenges,
  title={Challenges and applications of large language models},
  author={Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  journal={arXiv preprint arXiv:2307.10169},
  year={2023}
}

@article{li2024dawn,
  title={The dawn after the dark: An empirical study on factuality hallucination in large language models},
  author={Li, Junyi and Chen, Jie and Ren, Ruiyang and Cheng, Xiaoxue and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2401.03205},
  year={2024}
}

@article{zhang2024self,
  title={Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation},
  author={Zhang, Xiaoying and Peng, Baolin and Tian, Ye and Zhou, Jingyan and Jin, Lifeng and Song, Linfeng and Mi, Haitao and Meng, Helen},
  journal={arXiv preprint arXiv:2402.09267},
  year={2024}
}

@misc{zhang2024truthx,
      title={TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space}, 
      author={Shaolei Zhang and Tian Yu and Yang Feng},
      year={2024},
      eprint={2402.17811},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17811}
}

@misc{li2024inferencetimeinterventionelicitingtruthful,
      title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model}, 
      author={Kenneth Li and Oam Patel and Fernanda Viégas and Hanspeter Pfister and Martin Wattenberg},
      year={2024},
      eprint={2306.03341},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.03341}, 
}


@misc{chen2024truthforestmultiscaletruthfulness,
      title={Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning}, 
      author={Zhongzhi Chen and Xingwu Sun and Xianfeng Jiao and Fengzong Lian and Zhanhui Kang and Di Wang and Cheng-Zhong Xu},
      year={2024},
      eprint={2312.17484},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.17484}, 
}


@article{yang2023chatgpt,
  title={Chatgpt is not enough: Enhancing large language models with knowledge graphs for fact-aware language modeling},
  author={Yang, Linyao and Chen, Hongyang and Li, Zhao and Ding, Xiao and Wu, Xindong},
  journal={arXiv preprint arXiv:2306.11489},
  year={2023}
}

@article{ovadia2023fine,
  title={Fine-tuning or retrieval? comparing knowledge injection in llms},
  author={Ovadia, Oded and Brief, Menachem and Mishaeli, Moshik and Elisha, Oren},
  journal={arXiv preprint arXiv:2312.05934},
  year={2023}
}

@article{yang2023alignment,
  title={Alignment for honesty},
  author={Yang, Yuqing and Chern, Ethan and Qiu, Xipeng and Neubig, Graham and Liu, Pengfei},
  journal={arXiv preprint arXiv:2312.07000},
  year={2023}
}

@article{rlhf2023,
	author = {John Schulman},
	journal = {Berkeley EECS},
	title = {Reinforcement learning from human feedback: Progress and challenges},
	url = {https://www.youtube.com/watch?v=hhiLw5Q_UFg},
	year = {2023},
	bdsk-url-1 = {https://www.youtube.com/watch?v=hhiLw5Q_UFg}}

@misc{yang2023alignment,
	archiveprefix = {arXiv},
	author = {Yuqing Yang and Ethan Chern and Xipeng Qiu and Graham Neubig and Pengfei Liu},
	eprint = {2312.07000},
	primaryclass = {cs.CL},
	title = {Alignment for Honesty},
	year = {2023}}

@article{tian2023fine,
  title={Fine-tuning language models for factuality},
  author={Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2311.08401},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@misc{petroni2020contextaffectslanguagemodels,
      title={How Context Affects Language Models' Factual Predictions}, 
      author={Fabio Petroni and Patrick Lewis and Aleksandra Piktus and Tim Rocktäschel and Yuxiang Wu and Alexander H. Miller and Sebastian Riedel},
      year={2020},
      eprint={2005.04611},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.04611}, 
}

@misc{xie2024adaptivechameleonstubbornsloth,
      title={Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts}, 
      author={Jian Xie and Kai Zhang and Jiangjie Chen and Renze Lou and Yu Su},
      year={2024},
      eprint={2305.13300},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13300}, 
}

@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@inproceedings{fan2024survey,
  title={A survey on rag meeting llms: Towards retrieval-augmented large language models},
  author={Fan, Wenqi and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={6491--6501},
  year={2024}
}

@article{santhanam2021colbertv2,
  title={Colbertv2: Effective and efficient retrieval via lightweight late interaction},
  author={Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2112.01488},
  year={2021}
}

@article{chen2022rich,
  title={Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence},
  author={Chen, Hung-Ting and Zhang, Michael JQ and Choi, Eunsol},
  journal={arXiv preprint arXiv:2210.13701},
  year={2022}
}

@article{karpukhin2020dense,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2004.04906},
  year={2020}
}

@article{li2024investigating,
  title={Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style},
  author={Li, Yuepei and Zhou, Kang and Qiao, Qiao and Nguyen, Bach and Wang, Qing and Li, Qi},
  journal={arXiv preprint arXiv:2409.10955},
  year={2024}
}

@article{bi2024struedit,
  title={StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge Editing for Large Language Models},
  author={Bi, Baolong and Liu, Shenghua and Wang, Yiwei and Mei, Lingrui and Gao, Hongcheng and Fang, Junfeng and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2409.10132},
  year={2024}
}

@article{schick2024toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{burns2022discovering,
  title={Discovering latent knowledge in language models without supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2212.03827},
  year={2022}
}

@article{kai2024sh2,
  title={SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully},
  author={Kai, Jushi and Zhang, Tianhang and Hu, Hai and Lin, Zhouhan},
  journal={arXiv preprint arXiv:2401.05930},
  year={2024}
}

@article{min2023factscore,
  title={Factscore: Fine-grained atomic evaluation of factual precision in long form text generation},
  author={Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2305.14251},
  year={2023}
}

@article{bi2024adaptive,
  title={Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities},
  author={Bi, Baolong and Liu, Shenghua and Wang, Yiwei and Mei, Lingrui and Gao, Hongcheng and Xu, Yilong and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2406.12468},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{wang2023survey,
  title={Survey on factuality in large language models: Knowledge, retrieval and domain-specificity},
  author={Wang, Cunxiang and Liu, Xiaoze and Yue, Yuanhao and Tang, Xiangru and Zhang, Tianhang and Jiayang, Cheng and Yao, Yunzhi and Gao, Wenyang and Hu, Xuming and Qi, Zehan and others},
  journal={arXiv preprint arXiv:2310.07521},
  year={2023}
}

@inproceedings{gunjal2024detecting,
  title={Detecting and preventing hallucinations in large vision language models},
  author={Gunjal, Anisha and Yin, Jihan and Bas, Erhan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={18135--18143},
  year={2024}
}

@article{liu2024exploring,
  title={Exploring and evaluating hallucinations in llm-powered code generation},
  author={Liu, Fang and Liu, Yang and Shi, Lin and Huang, Houkun and Wang, Ruifeng and Yang, Zhen and Zhang, Li},
  journal={arXiv preprint arXiv:2404.00971},
  year={2024}
}

@misc{nakano2022webgptbrowserassistedquestionansweringhuman,
      title={WebGPT: Browser-assisted question-answering with human feedback}, 
      author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
      year={2022},
      eprint={2112.09332},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.09332}, 
}

@misc{yao2023reactsynergizingreasoningacting,
      title={ReAct: Synergizing Reasoning and Acting in Language Models}, 
      author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
      year={2023},
      eprint={2210.03629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.03629}, 
}

@misc{qin2024toollearningfoundationmodels,
      title={Tool Learning with Foundation Models}, 
      author={Yujia Qin and Shengding Hu and Yankai Lin and Weize Chen and Ning Ding and Ganqu Cui and Zheni Zeng and Yufei Huang and Chaojun Xiao and Chi Han and Yi Ren Fung and Yusheng Su and Huadong Wang and Cheng Qian and Runchu Tian and Kunlun Zhu and Shihao Liang and Xingyu Shen and Bokai Xu and Zhen Zhang and Yining Ye and Bowen Li and Ziwei Tang and Jing Yi and Yuzhang Zhu and Zhenning Dai and Lan Yan and Xin Cong and Yaxi Lu and Weilin Zhao and Yuxiang Huang and Junxi Yan and Xu Han and Xian Sun and Dahai Li and Jason Phang and Cheng Yang and Tongshuang Wu and Heng Ji and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2304.08354},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.08354}, 
}

@misc{guu2020realmretrievalaugmentedlanguagemodel,
      title={REALM: Retrieval-Augmented Language Model Pre-Training}, 
      author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
      year={2020},
      eprint={2002.08909},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2002.08909}, 
}

@misc{izacard2021leveragingpassageretrievalgenerative,
      title={Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering}, 
      author={Gautier Izacard and Edouard Grave},
      year={2021},
      eprint={2007.01282},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2007.01282}, 
}

@misc{zhong2022traininglanguagemodelsmemory,
      title={Training Language Models with Memory Augmentation}, 
      author={Zexuan Zhong and Tao Lei and Danqi Chen},
      year={2022},
      eprint={2205.12674},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.12674}, 
}


@misc{si2023promptinggpt3reliable,
      title={Prompting GPT-3 To Be Reliable}, 
      author={Chenglei Si and Zhe Gan and Zhengyuan Yang and Shuohang Wang and Jianfeng Wang and Jordan Boyd-Graber and Lijuan Wang},
      year={2023},
      eprint={2210.09150},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.09150}, 
}


@misc{wu2024pyvene,
      title={pyvene: A Library for Understanding and Improving PyTorch Models via Interventions}, 
      author={Zhengxuan Wu and Atticus Geiger and Aryaman Arora and Jing Huang and Zheng Wang and Noah D. Goodman and Christopher D. Manning and Christopher Potts},
      year={2024},
      eprint={2403.07809},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.07809}, 
}
@misc{HaluEval,
  author = {Junyi Li and Xiaoxue Cheng and Wayne Xin Zhao and Jian-Yun Nie and Ji-Rong Wen },
  title = {HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models},
  year = {2023},
  journal={arXiv preprint arXiv:2305.11747},
  url={https://arxiv.org/abs/2305.11747}
}

@article{ni2024llms,
  title={When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation},
  author={Ni, Shiyu and Bi, Keping and Guo, Jiafeng and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2402.11457},
  year={2024}
}

@article{ni2024large,
  title={Are Large Language Models More Honest in Their Probabilistic or Verbalized Confidence?},
  author={Ni, Shiyu and Bi, Keping and Yu, Lulu and Guo, Jiafeng},
  journal={arXiv preprint arXiv:2408.09773},
  year={2024}
}

@article{zhang2024mm,
  title={Mm-llms: Recent advances in multimodal large language models},
  author={Zhang, Duzhen and Yu, Yahan and Li, Chenxing and Dong, Jiahua and Su, Dan and Chu, Chenhui and Yu, Dong},
  journal={arXiv preprint arXiv:2401.13601},
  year={2024}
}

@article{chen2023vlp,
  title={Vlp: A survey on vision-language pre-training},
  author={Chen, Fei-Long and Zhang, Du-Zhen and Han, Ming-Lun and Chen, Xiu-Yi and Shi, Jing and Xu, Shuang and Xu, Bo},
  journal={Machine Intelligence Research},
  volume={20},
  number={1},
  pages={38--56},
  year={2023},
  publisher={Springer}
}