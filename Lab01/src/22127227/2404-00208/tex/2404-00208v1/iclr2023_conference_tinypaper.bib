@misc{silver2019fewshot,
      title={Few-Shot Bayesian Imitation Learning with Logical Program Policies}, 
      author={Tom Silver and Kelsey R. Allen and Alex K. Lew and Leslie Pack Kaelbling and Josh Tenenbaum},
      year={2019},
      eprint={1904.06317},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@phdthesis{10.5555/1714168,
author = {Solar-Lezama, Armando},
advisor = {Bodik, Rastislav},
title = {Program Synthesis by Sketching},
year = {2008},
isbn = {9781109097450},
publisher = {University of California at Berkeley},
address = {USA},
abstract = {The goal of software synthesis is to generate programs automatically from high-level specifications. However, efficient implementations for challenging programs require a combination of high-level algorithmic insights and low-level implementation details. Deriving the low-level details is a natural job for a computer, but the synthesizer can not replace the human insight. Therefore, one of the central challenges for software synthesis is to establish a synergy between the programmer and the synthesizer, exploiting the programmer's expertise to reduce the burden on the synthesizer. This thesis introduces sketching , a new style of synthesis that offers a fresh approach to the synergy problem. Previous approaches have relied on meta-programming, or variations of interactive theorem proving to help the synthesizer deduce an efficient implementation. The resulting systems are very powerful, but they require the programmer to master new formalisms far removed from traditional programming models. To make synthesis accessible, programmers must be able to provide their insight effortlessly, using formalisms they already understand. In Sketching, insight is communicated through a partial program, a sketch that expresses the high-level structure of an implementation but leaves holes in place of the low-level details. This form of synthesis is made possible by a new SAT-based inductive synthesis procedure that can efficiently synthesize an implementation from a small number of test cases. This algorithm forms the core of a new counterexample guided inductive synthesis procedure (CEGIS) which combines the inductive synthesizer with a validation procedure to automatically generate test inputs and ensure that the generated program satisfies its specification. With a few extensions, CEGIS can even use its sequential inductive synthesizer to generate concurrent programs; all the concurrency related reasoning is delegated to an off-the-shelf validation procedure. The resulting synthesis system scales to real programming problems from a variety of domains ranging from bit-level ciphers to manipulations of linked datastructures. The system was even used to produce a complete optimized implementation of the AES cipher. The concurrency aware synthesizer was also used to synthesize, in a matter of minutes, the details of a fine-locking scheme for a concurrent set, a sense reversing barrier, and even a solution to the dining philosophers problem. The system was also extended with domain specific knowledge to better handle the problem of implementing stencil computations, an important domain in scientific computing. For this domain, we were able to encode domain specific insight as a problem reduction that converted stencil sketches into simplified sketch problems which CEGIS resolved in a matter of minutes. This specialized synthesizer was used to quickly implement a MultiGrid solver for partial differential equations containing many difficult implementation strategies from the literature. In short, this thesis shows that sketching is a viable approach to making synthesis practical in a general programming context.},
note = {AAI3353225}
}

@misc{wierstra2011natural,
      title={Natural Evolution Strategies}, 
      author={Daan Wierstra and Tom Schaul and Tobias Glasmachers and Yi Sun and JÃ¼rgen Schmidhuber},
      year={2011},
      eprint={1106.4487},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{salimans2017evolution,
      title={Evolution Strategies as a Scalable Alternative to Reinforcement Learning}, 
      author={Tim Salimans and Jonathan Ho and Xi Chen and Szymon Sidor and Ilya Sutskever},
      year={2017},
      eprint={1703.03864},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{JMLR:v21:17-678,
  author  = {James Martens},
  title   = {New Insights and Perspectives on the Natural Gradient Method},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {146},
  pages   = {1--76},
  url     = {http://jmlr.org/papers/v21/17-678.html}
}

@misc{tomczak2012,
	author = {Jakub M. Tomczak},
	title = {{F}isher information matrix for {G}aussian and categorical distributions},
	howpublished = {\url{https://www.ii.pwr.edu.pl/~tomczak/PDF/[JMT]Fisher_inf.pdf}},
	year = {2012},
	note = {[Accessed 03-12-2023]},
}

@misc{staines2012variational,
      title={Variational Optimization}, 
      author={Joe Staines and David Barber},
      year={2012},
      eprint={1212.4507},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}