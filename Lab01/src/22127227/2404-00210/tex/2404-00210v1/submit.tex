
\begin{abstract}
We propose a novel Vision-Language Model (VLM) based navigation approach to compute a robot's trajectory in human-centered environments. 
Our goal is to make real-time decisions on robot actions that are socially compliant with human expectations. We utilize the perception model to detect important social entities and prompt a VLM to generate guidance for socially compliant robot behavior. Our approach uses a VLM-based scoring module that computes a cost term that ensures socially appropriate and effective robot actions generated by the underlying planner. 
Our overall approach reduces reliance on large datasets (for training) and enhances adaptability in decision-making. In practice, it results in improved socially compliant navigation in human-shared environments. We demonstrate and evaluate our system in four different real-world social navigation scenarios with a Turtlebot robot. We observe at least $36.37\%$ improvement of success rate in four challenging social navigation scenarios comparing with other state-of-the-art methods. %Our approach results in \todo{experiment result}.

\end{abstract}


\section{Introduction}\label{sec:intro}
% What is social navigation and why is it important
Mobile robots integrated into diverse indoor and outdoor human-centric environments are becoming increasingly prevalent. These robots serve various functions, ranging from package and food delivery~\cite{starship, scout} to service~\cite{dilligent} and home assistance~\cite{astro}. Overall, these roles necessitate interaction with humans and navigating seamlessly through public spaces with pedestrians. In such dynamic scenarios, it is important for the robots to engage in socially compliant interactions and navigation~\cite{xiao2022motion, mirsky2021conflict, mavrogiannis2023core}. 

% challenges in social navigation:
\textit{Challenges of social navigation:} This paper focuses on the challenges of social navigation~\cite{mavrogiannis2023core}, and encompasses the ability of robots to navigate while adhering to social etiquette, especially, contextual appropriateness, which requires robot to understand the relative importance of the previous objectives and context of the environment, tasks or interpersonal behaviors. However, navigating socially across varying contexts presents distinct challenges~\cite{francis2023principles, mavrogiannis2023core}, including ensuring safety, comfort, politeness and social norms.

% Contextual Appropriateness: technical challenges from previous works
\textit{Inferring contextually appropriate navigation behaviors is challenging.} Humans have various behaviors and the environmental or task contexts are also not easily to be categorized~\cite{mavrogiannis2023core}. A common strategy to handle the challenge is by learning-based approaches to empirically learn the complicated contexts. Imitation Learning (IL) is a recent emerging paradigm for desired navigation behavior~\cite{hirose2023sacson, xiao2020appld, xiao2022learning, panigrahi2023study, raj2024targeted}. This approach enables autonomous agents to navigate socially by learning from human demonstration. Other learning approaches, such as Reinforcement Learning (RL) have also been used to address this problem~\cite{kretzschmar2016socially}. While both methods demonstrate promising results in real-world settings, substantial datasets~\cite{thorDataset2019, karnan2022scand, nguyenmusohu} for training and reward engineering are required for their successful application. 

% the potential of Language models 
\textit{Language models inherently suit to contextual understanding but not well applied in social navigation.} Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) demonstrate a deep understanding of contextual information and have the potential to perform chain-of-thought~\cite{wei2022chain} and common sense reasoning~\cite{geva2021did, gpt4v2023}. It is inherently for social navigation, especially the challenges of Contextual appropriateness and politeness, which require understanding the task/environmental context and the behavior of humans. This capability has also been evaluated across diverse domains of robotics, including human-like driving scenarios~\cite{wen2023road, sha2023languagempc} and autonomous robot navigation~\cite{10160969,pmlr-v205-shah23b}. However, using language models for social navigation is not well explored and the language models suffer from high latency for real-time navigation, and the issue impedes the smoothness and efficiency of human-robot social interaction. 
% We leverage the capabilities of VLMs for scene understanding and reasoning to perform decision-making in social navigation, particularly in human-centered environments.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/cover.png} 
\caption{Our approach (blue) demonstrates more social compliant than DWA (red) and BC (yellow) approaches in the frontal encountering scenario (left) and the intersection scenario (right).}
\label{fig:cover}
\end{figure} 


{\bf Main Results: } In this paper, we present a new approach that uses VLMs to interpret contextual information from the robot observation to help autonomous agents improve their navigation abilities in human-centered environments. In particular, we leverage a VLM to analyze and reason about the current social interaction, and generate an immediate \textit{preferred robot action} to guide the motion planner. Our VLM-based scoring module computes the \textit{social cost}, which is used for the bottom-level motion planner to output the appropriate robot action. To overcome the limitation of existing VLMs' latency issue, we utilize the state-of-art perception model (\ie YOLO~\cite{YOLO2023}) to detect key entities that are used for social interactions (\eg humans, gestures, and doors) and trigger a VLM to compute the social cost. We demonstrate our approach in four different indoor scenarios with human interactions. Unlike previous social navigation approaches, our method can better deal with social navigation scenarios by interpreting the situation based on \textit{common sense}, through observation without any training with a large dataset. Some of our main results include:
\begin{itemize}
  \item We bridge the gap between VLMs and social navigation and propose a novel approach for social navigation with contextual understanding. By integrating VLMs with optimization-based local motion planners and state-of-the-art perception models, our approach enables robots to detect social entities efficiently and make real-time decisions with respect to socially compliant robot behavior. 
  \item We propose a VLM-based scoring module that translates the current robot observation and the textual instructions into a relevant social cost term. This cost term is used for the bottom-level motion planner to output the sub-optimal robot action. 
  \item We evaluate our approach in four different real-world indoor social navigation scenarios. Our approach achieves at least $36.37\%$ improvement in success rate averagely in the four challenging social navigation scenarios. We measure the success rate, collision rate, and clearing distance. We also perform a user study and compare the results with a Dynamic-Window approach (DWA)~\cite{fox1997dynamic} and Behavior Cloning (BC)~\cite{pomerleau1988alvinn} method trained on a state-of-the-art large Socially CompliAnt Navigation Dataset
(SCAND)~\cite{karnan2022scand}. 
%\todo{experiment result}
  
  %a multi-modal fusion method~\cite{panigrahi2023study}. \todo{experiment result}
\end{itemize}

\section{Related Work}
\label{sec:prev}
In this section, we give an overview of existing works related to social navigation, including safety requirements of social navigation, different challenges of contextual appropriateness, and Large Foundation Models (LFMs) for robot navigation. 

\subsection{Safety Requirement of Social Navigation}
For social navigation, keeping safety is the basic requirement for interaction with humans and navigation in dynamic challenging scenarios~\cite{van2010optimal, fox1997dynamic, liang2021vo, liang2021crowd}. The DWA~\cite{fox1997dynamic} calculates the collision constraints of the robots' actions and chooses the best feasible action closest to the target as the output. Model Predictive Control approaches~\cite{10341869, williams2017information} are also widely used for collision avoidance tasks, which provide smooth trajectories for the robot to follow; the learning-based MPC~\cite{williams2017information} enhances the performance of the original MPC method by empirically learning the best actions for navigation.  However, this method can be computationally costly. Velocity-Obstacle (VO)-based approaches are more efficient and can be used to simulate the actions of crowds~\cite{van2010optimal, best2014densesense}, but these approaches don't take into account the uncertainties in the perception output. PRVO~\cite{gopalakrishnan2017prvo} and OFVO~\cite{liang2021vo} handle the uncertainties of perception in motion planning, but those approaches require a hard threshold to set the confidence for planning. To deal with this issue, learning-based methods empirically train the policies by demonstrations~\cite{liang2021crowd, aradi2020survey, sun2021motion}, whereas other methods~\cite{liang2021crowd,aradi2020survey} use reinforcement learning to train the robot in a simulator and implement it in real-world scenarios. However, learning-based approaches require lots of data or even on-policy training with realistic simulators to learn the task.
%We apply optimization-based collision avoidance approaches as baseline for social navigation.

\subsection{Contextual Appropriateness of Social Navigation}
Researchers have proposed diverse methodologies to integrate social awareness into mobile robot navigation systems. The development of a comprehensive social navigation system poses inherent complexity, requiring sophisticated perception and reasoning capabilities to navigate environments shared with humans and other robots \cite{mirsky2021conflict}. Establishing an accurate definition of social navigation is paramount, given its potential variation across cultures and platforms. Except for the safety requirement, assessing social compliance presents an additional challenge, contingent on the scenario and platform, and should adhere to Contextual Appropriateness, which includes Comfort, Legibility, Social Competency, Politeness, Understanding Other Agents, and Proactivity \cite{gao2022evaluation, francis2023principles}. 
Various methodologies are employed to address this challenge, with a significant focus on enhancing learning methods through reinforcement learning, learning from demonstration—particularly by analyzing examples of human trajectories or robots operated by humans—and the utilization of simulated datasets \cite{9679193,5354147, 7539621, li2017role, nazeri2024vanp}. Additionally, various datasets have been collected for this purpose \cite{thorDataset2019, karnan2022scand, nguyenmusohu}. Despite extensive research employing diverse approaches with machine learning techniques~\cite{10323465}, less emphasis has been placed on formulating social navigation using VLMs, which inherently analyze the contextual information of the environment~\cite{kruse2013human, charalampous2017recent, mavrogiannis2023core}.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]%{figures/pipeline.pdf}
{figures/architecture.png}
\caption{The overall system architecture of our social navigation algorithm. Our real-world perception (vision) model detects important social entities (\eg humans, gestures, and doors) in real time and prompts the VLM-based scoring module to compute social cost $C_{social}$, which is used to generate socially compliant robot action. 
}
\label{fig:overview}
\end{figure*} 

\subsection{Large Foundation Models for Navigation}
Recent advancements in Language Foundation Models (LFMs)~\cite { bommasani2022opportunities}, encompassing Vision-Language Models (VLMs) and Language-Language Models (LLMs), show significant potential in robotic navigation, despite the challenges like bias and reliability~\cite{payandeh2023susceptible} associated with the LFMs. SayCan~\cite{Ahn2022} integrate LLMs for high-level task planning. GPT-Driver~\cite{ mao2023gptdriver} evaluates the performance of GPT-3.5 in simulation for autonomous driving, framing motion planning as a language modeling problem. LL3MVN~\cite{Yu_2023} constructs semantic maps of environments and utilizes LLMs to reach long-term goals, while LLaDA~\cite{li2024driving} enables autonomous vehicles to adapt to diverse traffic rules across regions. LM-Nav~\cite{shah2022lmnav} utilizes GPT-3 and CLIP~\cite{radford2021learning} to navigate outdoor environments based on natural language instructions, combining language and visual cues for optimal path planning. Xu et al.~\cite{xu2023vision} employ VLMs to generate visual-language maps for navigation tasks. RT-1~\cite{brohan2022rt1} and RT-2~\cite{brohan2023rt2} models leverage multimodal robotics data for navigating complex environments and offer greater generalizability through larger dataset training \cite{shah2022lmnav, xu2023vision, brohan2022rt1, brohan2023rt2}. However, for social navigation, language models are not well explored, even though VLMs and LLMs inherently handle the understanding of contextual information. We bridge the gap and propose a novel approach for social navigation by taking advantage of the contextual understanding capability of VLMs.

% However, limited research has focused on short-horizon path planning for socially aware navigation in real-world settings. 

% their adoption has led to deeper insights into social interactions and more nuanced, context-aware navigation strategies. Works such as SayCan~\cite{Ahn2022} integrate LLMs for high-level task planning in natural language instructions, leveraging pretrained skills for context-specific actions. GPT-Driver~\cite{ mao2023gptdriver} evaluates the performance of GPT-3.5 in simulation for autonomous driving, framing motion planning as a language modeling problem. LL3MVN~\cite{Yu_2023} constructs semantic maps of environments and utilizes LLMs to reach long-term goals, while LLaDA~\cite{li2024driving} enables autonomous vehicles to adapt to diverse traffic rules across regions. LMPC~\cite{liang2024learning} fine-tunes PaLM 2~\cite{anil2023palm} using in-context learning data to enhance generalization to unseen embodiments.

% Multimodal foundation models have further enhanced navigation capabilities by integrating textual and visual data for more adaptive strategies. LM-Nav~\cite{shah2022lmnav} utilizes GPT-3 and CLIP~\cite{radford2021learning} to navigate outdoor environments based on natural language instructions, combining language and visual cues for optimal path planning. Xu et al.~\cite{xu2023vision} employ VLMs to generate visual-language maps for navigation tasks. RT-1~\cite{brohan2022rt1} and RT-2~\cite{brohan2023rt2} models leverage multimodal robotics data for navigating complex environments and offer greater generalizability through larger dataset training \cite{shah2022lmnav, xu2023vision, brohan2022rt1, brohan2023rt2}. However, limited research has focused on short-horizon path planning for socially aware navigation in real-world settings. 


\section{Approach}\label{sec:ap}

In this section, we define the social navigation problem and describe our approach in detail. 

\subsection{Problem Definition}

Navigation is the task of following an efficient collision-free path from an initial location to a goal~\cite{mavrogiannis2023core}. In general, the overall system consists of a global planner and a local planner. A global planner is designed to find a collision-free path to reach a goal, while a local planner aims to navigate the robot through its immediate surroundings, making real-time adjustments to deal with dynamic obstacles. 

For social robot navigation, humans are no longer perceived only as dynamic obstacles but also as social entities~\cite{rios2015proxemics, mirsky2021conflict}. It necessitates integrating social norms into robot behaviors. 
We define the social robot navigation problem as a {\em Markov Decision Process (MDP)}: $\langle \cs, \ca, \ct, \cc \rangle$ where $\s = (x, y, \theta) \in \cs$ is a state space consisting of a robot pose, $\a = (v, w) \in \ca$ is an action space consisting of a linear and an angular velocity of a robot, $\ct: \cs \times \ca \rightarrow \cs$ is the transition function characterizing the dynamics of the robot, and $\cc: \cs \times \ca \rightarrow \RR $ is a cost function. Given a cost function $\cc$, the motion planner finds $(v^*, w^*)$ that minimizes the expected cost. The cost function takes the following form:
\begin{equation}\label{eq:cost}
\begin{aligned}
    C(\s,\a)=\alpha\cdot C_{goal}+\beta\cdot C_{obst}+\gamma\cdot C_{social},
\end{aligned}
\end{equation}
where $C_{goal}$ encourages movement toward the goal, $C_{obst}$ discourages collisions with obstacles, and $C_{social}$ encourages the robot to follow the social norms. $\alpha$, $\beta$, and $\gamma$ are a non-negative weight for each cost term. 

The social cost term $C_{social}$ encompasses various factors that govern human-robot interactions in shared environments. Defining them mathematically poses challenges. For our approach, we define $C_{social}$ as:
\begin{equation}\label{eq:social}
\begin{aligned}
    C_{social}= \|\cb - \cb_{h} \|,
\end{aligned}
\end{equation}
where $\cb$ is a navigation behavior, and $\cb_{h}$ is a navigation behavior humans would adopt in accordance with social conventions. Minimizing the deviation between them will encourage the robot to emulate socially acceptable human behavior. While $\cb_{h}$ can be obtained through various methods, including large datasets~\cite{thorDataset2019, karnan2022scand, nguyenmusohu}, we leverage the power of VLM to compute appropriate behavior based on a rich contextual understanding and nuanced interpretations from the image and the given prompts. We elaborate further in Section~\ref{sec:vlm}.


\subsection{VLM-based Social Navigation Architecture}

Figure~\ref{fig:overview} highlights the overview of our approach. Our formulation is built upon an autonomous navigation systen comprising a global path planner and a local motion planner. The global path planner generates a high-level trajectory for the robot to follow, considering obstacles and the goal location. The local planner refines this trajectory by considering the sensor inputs and outputs a robot action that minimizes the cost function $C$.

To improve navigation abilities in a social-interaction context, we introduce a VLM-based scoring module. As the robot navigates through the environment, our real-world perception module detects social entities, such as humans, gestures, and doors, in real time. When important social cues are detected, our VLM-based scoring module is prompted and outputs the immediate robot action that is socially acceptable based on the current robot observation. It is calculated as a cost term to be used by the motion planner. 
Our approach is designed to make real-time decisions on robot actions based on the current observations. The social cost term is integrated only when it is necessary, \ie when there is any human interaction involved. 


\begin{figure}[tb]
\centering
\begin{GrayBox}
    %\hspace{0.05\linewidth}
    \includegraphics[width=\linewidth]{figures/pic2.png}\\

    \small{
    
    \textbf{Task:} \\
    The image depicts your current view while you navigating towards the goal. How will you navigate concerning the person in your view? You will need to follow general walking etiquette. \\

    \textbf{Ego state:} \\
    - heading direction: \textcolor{blue}{straight}\\
    - linear velocity: \textcolor{blue}{0.28}\\

    \textbf{Remember:} \\
    - Move to the right when passing by a person.\\
    - Pass on the left when overtaking a person.\\
    - Do not obstruct others' paths.\\
    - ... \\

    \textbf{Answer Format:} \\
    Move \textcolor{red}{DIRECTION} with \textcolor{red}{SPEED} \\
    - options for DIRECTION: left, straight, right \\
    - options for SPEED: slow down, speed up, constant, stop\\   

    }
    
\end{GrayBox}
\caption{An example prompt used in our approach. Parameterized inputs are highlighted in blue. Formatted outputs are highlighted in red. The example output of an example image is \textit{Move right with slow down}, to slowly pass by a person on the right side. 
}
\label{fig:prompt}
\end{figure} 

\subsection{VLM-based Scoring Module} \label{sec:vlm}


VLM plays a crucial role in our approach to infer immediate socially compatible navigation behavior $\cb_h^{t+1}$ based on its pre-trained large internet-scale dataset:
\begin{equation}\label{eq:vlm}
\begin{aligned}
    \cb_h^{t+1} = \text{VLM}(\ci^t, \cp ,\a^t) ,
\end{aligned}
\end{equation}
where $\ci^t$ is an RGB image of the robot view at time $t$, $\cp$ is a textual prompt, $\a^t$ is a current robot action at time $t$. Inspired by In-Context Learning (ICL), our prompt $\cp$ is designed to leverage the VLM's reasoning abilities through zero-shot examples. This approach offers an interpretable interface, mirroring human reasoning and decision-making processes, without extensive training~\cite{min2022rethinking}.


Figure~\ref{fig:prompt} shows an example prompt $\cp$ used in our experiment. We provide a high-level task description along with an image $\ci^t$ captured from the robot's perspective. Furthermore, the current robot action $\a^t=(v^t,w^t) \in \ca$ is provided. %, which will be employed in calculating $C_{social}$. 
Supplementary instructions regarding walking etiquette are also included. Although the VLM demonstrates proficient navigation abilities even in the absence of explicit instructions, offering reasoning guidelines enhances its decision-making processes~\cite{min2022rethinking}. These guidelines not only facilitate comprehensive reasoning and judgment within the VLM but also enable the robot to adapt to specific rules more effectively. 
We format the output of VLM as an immediate action that the robot should take for the next step, specifying the heading direction and the speed.

Our VLM-based scoring module starts from the insight that the action space of a mobile robot can be readily mapped to linguistic terms. For example, the action ``move forward at a constant speed'' can be linked to a linear velocity of $v^{t}$ m/s and an angular velocity of 0, where $v^{t}$ represents the current linear velocity. The heading direction on the right indicates a positive value, while the direction on the left indicates a negative.  Leveraging this understanding, we structure the output of the VLM into a linguistic format comprising the heading, and the speed. Subsequently, our VLM-based scoring module extracts $\cb_h^{t+1} \mapsto (v_h^{t+1}, w_h^{t+1}) \in \ca$ from these tokens; $v_h^{t+1} = v^t + \delta_{s}$, where $\delta_{s}$ is derived from the response for SPEED; $w_h^{t+1} = \delta_{d}$, where $\delta_{d}$ is derived from the response for DIRECTION. 
Thus, the social cost term for the next time step can be calculated:
\begin{equation}\label{eq:social_vlm}
\begin{aligned}
    C_{social}^{t+1}= w_{l} \cdot \| v - v_h^{t+1} \| + w_{a} \cdot \| w - w_h^{t+1} \|,
\end{aligned}
\end{equation}
where $w_{l}$ and $w_{a}$ are a non-negative weight values. Given all the cost terms, our low-level optimization-based motion planner finds the robot action $(v^*, w^*)$ that minimizes the cost. 




% $\cb_h \rightarrow \ca$
% We map \cb^{t} \rightarrow \ca



\begin{figure*}[htb]
\centering
\includegraphics[width=\linewidth]{figures/result.png}
\caption{Results of the robot motion with our approach. The scenario starts from left to right: Frontal Approach, Intersection, Intersection with Gesture, and Narrow Doorway.}
\label{fig:qualitative}
\end{figure*} 

\begin{table*}[htb] 
\centering
\caption{Quantitative Reults: we achieve at least $36.37\%$ improvement in success rate averagely in the four challenging social navigation scenarios.}
\begin{tabular}{rccccc} 
 \midrule
 \multirow{2}{*}{\textbf{Metric}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{Scenario}} \\
 \cmidrule(lr{0.5em}){3-6}
 & &  Frontal Approach & Frontal Approach  w/ Gesture & Intersection & Narrow Doorway \\
 \midrule
 \multirow{3}{*}{Success Rate (\%)} & BC & 26.66 & 0 & 13.33 & 33.33\\
 & DWA & \textbf{100} & \textbf{0} & 93.33 & \textbf{100}\\
 & Ours & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100}\\
 \midrule
 \multirow{3}{*}{Collision Rate (\%)} & BC & 46.66 & 60.00 & 33.33 & 40.00\\
 & DWA & 26.66 & 20.00 & 13.33 & 33.33 \\
 & Ours & \textbf{6.66} & \textbf{0} & \textbf{0} & \textbf{6.66} \\
 \midrule
 \multirow{3}{*}{User Study Score} & BC & 2.80 $\pm$ 1.45 & 2.23 $\pm$ 1.54 & 2.80 $\pm$ 1.40 & 2.60 $\pm$ 1.33\\
 & DWA & 3.99 $\pm$ 0.80 & 3.38 $\pm$ 0.64 & 3.57 $\pm$ 0.62 & 3.59 $\pm$ 0.83\\
 & Ours & \textbf{4.31 $\pm$ 0.72} & \textbf{4.28 $\pm$ 0.56} & \textbf{4.35 $\pm$ 0.70} & \textbf{4.04 $\pm$ 0.74}\\
 \midrule
\end{tabular}
%}
\label{table:quantitative}
\end{table*} 



\section{Experiments}\label{sec:exp}

In this section, we detail the implementation of our method and describe our qualitative and quantitative results including comparisons with other methods and the user study. 

\subsection{Implementation Details}

Our approach is tested on a Turtlebot 2 equipped with a Velodyne VLP16 lidar, a MMlove fisheye lens RGB camera, and a laptop with intel i7 CPU, and an Nvidia GeForce RTX 2080 GPU. 
We use You Only Look Once (YOLO)~\cite{YOLO2023} as our real-world perception model to detect key objects (\eg humans, door, and gesture) and Generative Pre-trained Transformer 4 with Vision (GPT-4V)~\cite{gpt4v2023} as our VLM to comprehend the social dynamics and output the immediate preferred robot action. We combined our method with a low-level motion planner DWA~\cite{fox1997dynamic}. We compare our method with DWA without social cost $C_{social}$ and BC~\cite{pomerleau1988alvinn} method trained on a state-of-the-art large set of Socially CompliAnt Navigation Dataset
(SCAND)~\cite{karnan2022scand}.

%multi-modal fusion method~\cite{panigrahi2023study}, which adopts an imitation learning approach to learn navigation behavior by fusing an RGB image and a point cloud.

To validate our approach, we carefully follow the social robot navigation studies~\cite{francis2023principles, pirk2022protocol} that have set up the benchmark scenarios and the metrics for measuring social navigation. 
We present qualitative, quantitative, and user study results in four different indoor social navigation scenarios:
\begin{itemize}
  \item \textbf{Frontal Approach:} A robot and a human approach each other from two ends of a straight trajectory.  
  \item \textbf{Frontal Approach with Gesture:} A robot and a human approach each other from two ends of a straight trajectory. The human recognizes the robot and then gestures for it to stop.
  \item \textbf{Intersection:} A robot and a human cross each other on perpendicular trajectories. 
  \item \textbf{Narrow Doorway:} A robot and a human cross each other's paths by moving through a narrow doorway. 
\end{itemize}

\subsection{Qualitative Result}

Based on the protocols and principles set by other studies~\cite{francis2023principles, pirk2022protocol}, the robot is expected to behave in a socially compliant way as follows: 
\begin{itemize}
  \item \textbf{Frontal Approach:} The robot is expected to yield or slow down and modify its original trajectory so as not to obstruct the human path. Similar to driving rules, it is conventional to keep on the right side. 
  \item \textbf{Frontal Approach with Gesture:} The robot is expected to yield by interpreting the human gesture. 
  \item \textbf{Intersection:} The robot is expected to drive slowly when it approaches the human. It may come to a complete stop or modify its original trajectory to go behind the human to not obstruct the path. 
  \item \textbf{Narrow Doorway:} The robot is expected to wait outside the door and yield to the human. 
\end{itemize}

Fig.~\ref{fig:qualitative} shows snapshots of the resulting robot motion using our approach in four different scenarios. We demonstrate that our approach follows the social convention and navigates toward its goal as expected. 
Fig.~\ref{fig:cover} illustrates the resulting trajectories of our approach in comparison to those of DWA and BC methods. A notable observation is that while DWA also effectively avoids collisions with individuals, our approach generates trajectories that align more closely with social norms. For instance, in the frontal approach scenario, while DWA tends to maneuver around the person either to the right or left, our approach predominantly bypasses them on the right side. Similarly, in the intersection scenario, whereas DWA occasionally obstructs the person's path by veering to avoid collision directly in front, our approach endeavors to pass behind the individual. Additionally, BC successfully avoids humans but fails to recover and follow the original path.
%The accompanying video shows the resulting robot motions of our approach, compared with two other methods.  


\begin{table}[htb] 
\centering
\caption{Social Compliance Questionnaire}
\begin{tabularx}{\linewidth}{l|l} 
 \midrule
 \multicolumn{2}{l} {\textbf{Scenario 1: Frontal Approach}} \\
 \midrule
 1 & The robot moved to avoid me.\\
 2 & The robot obstructed my path.$^{*}$\\
 3 & The robot maintained a safe and comfortable distance at all times.\\
 4 & The robot nearly collided with me.$^{*}$\\
 5 & It was clear what the robot wanted to do.\\
 \midrule
  \multicolumn{2}{l} {\textbf{Scenario 2: Frontal Approach with Gesture}} \\ 
 \midrule
 6 & The robot maintained a safe and comfortable distance at all times.\\
 7 & The robot slowed down and stopped.\\
 8 & The robot followed my command\\
 9 & I felt the robot paid attention to what I was doing.\\
 \midrule
 \multicolumn{2}{l} {\textbf{Scenario 3: Intersection}} \\ 
 \midrule
 10 & The robot let me cross the intersection by maintaining a safe and \\ & comfortable distance.\\
 11 & The robot changed course to let me pass.\\
 12 & The robot paid attention to what I was doing.\\
 13 & The robot slowed down and stopped to let me pass.\\
 \midrule
 \multicolumn{2}{l} {\textbf{Scenario 4: Narrow Doorway}} \\ 
 \midrule
 14 & The robot got in my way.$^{*}$\\
 15 & The robot moved to avoid me.\\
 16 & The robot made room for me to enter or exit.\\
 17 & It was clear what the robot wanted to do.\\
 \midrule
\end{tabularx}
%}
\label{table:userstudy}
\end{table} 


\subsection{Quantitative Result}

To further validate our approach, we evaluate the methods using three different 
metrics~\cite {francis2023principles}. The success rate describes whether the robot reaches the goal. For the frontal approach with gesture scenario, we mark it as successful when the robot reacts to the gesture. The collision rate describes whether the robot collided with the human or other objects in the environment. We also mark it as collision when we manually intervene to avoid an imminent collision with the human subject or surroundings. The user study score is an average score we obtained from the user study detailed in Section~\ref{sec:userstudy}.

Table~\ref{table:quantitative} reports the results averaged over 15 runs for each method and scenario. The results demonstrate that our approach, DWA with social cost, outperforms other methods in every metric. DWA excels at following a path smoothly, yet it faces challenges in collision avoidance as it relies solely on the lidar sensor. This occurs when the participant walks too quickly and the robot attempts to make an abrupt change in motion. The outcomes of BC varied. At times, when attempting to avoid collisions, it failed to return to its original path. Conversely, there were instances where it didn't attempt collision avoidance at all, resulting in collisions with the participants. For gesture recognition, only our proposed method successfully responded to the participants' gestures. In total, we achieve at least $36.37\%$ improvement in success rate averagely in the four challenging social navigation scenarios.
%The results demonstrate that \todo{}.

\subsection{User Study}\label{sec:userstudy}

To validate the social compliance of our method, we conduct a user study. 
We ask the participants to walk along the predefined trajectory and ask them to answer questionnaires about the robot motion~\cite{pirk2022protocol} (Table~\ref{table:userstudy}). * denote negatively formulated questions, for which we reverse-code the ratings to ensure comparability with the positively formulated ones. The three methods are randomly shuffled and repeated three times. Each scenario is tested on five different participants. 
We use a five-level Likert scale to ask participants to rate their agreement toward these statements. 

Fig.~\ref{fig:userstudy} and the user study score on Table~\ref{table:quantitative} show the study result. The plot shows the per-question average scores for the three methods in each scenario. 
Based on the results, it's evident that our method received the highest level of agreement from participants across all questions, indicating its strong adherence to social norms. 
%From the result, we can see that \todo{}.

\begin{figure}[htb]
\centering
\includegraphics[width=0.49\linewidth]{figures/graph1.png}
\includegraphics[width=0.49\linewidth]{figures/graph2.png}
\includegraphics[width=0.49\linewidth]{figures/graph3.png}
\includegraphics[width=0.49\linewidth]{figures/graph4.png}
\includegraphics[width=0.3\linewidth]{figures/legend.png}
\caption{User Study Average Scores}
\label{fig:userstudy}
\end{figure} 

%\subsection{Discussion}
% Talk about the response time

\section{Conclusion}\label{sec:con}
We propose a novel social navigation approach based on Vision-Language Models (VLMs), focusing on real-time, socially compliant desicion-making in human-centric environments. We utilize the perception model to detect important social entities and prompt a VLM to generate guidance for socially compliant behavior. Our approach features a VLM-based scoring system that ensures socially appropriate and effective robot actions, minimizing reliance on large training datasets and the need for explicit rules or rewards w.r.t. reinforcement learning-based approaches, thereby enhancing the adaptability and scalability of the decision-making process in autonomous robot navigation. In practice, it results in improved socially compliant navigation in human-shared environments. We demonstrate and evaluate our system in four different real-world social navigation scenarios with a Turtlebot robot.


% Our approach uses a VLM-based scoring module that computes a cost term that ensures socially appropriate and effective robot actions generated by the underlying planner. 
% This not only reduces the reliance on extensive training data but also alleviates the need to define rules or rewards for every possible scenario explicitly, thereby enhancing the adaptability and scalability of the decision-making process in autonomous robot navigation. 
% Our overall approach reduces reliance on large datasets (for training) and enhances adaptability in decision-making. 
% In practice, it results in improved socially compliant navigation in human-shared environments. We demonstrate and evaluate our system in four different real-world social navigation scenarios with a Turtlebot robot.

