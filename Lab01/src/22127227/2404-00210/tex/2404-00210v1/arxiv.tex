
\begin{abstract}
We propose VLM-Social-Nav, a novel Vision-Language Model (VLM) based navigation approach to compute a robot's trajectory in human-centered environments. 
Our goal is to make real-time decisions on robot actions that are socially compliant with human expectations. We utilize a perception model to detect important social entities and prompt a VLM to generate guidance for socially compliant robot behavior. VLM-Social-Nav uses a VLM-based scoring module that computes a cost term that ensures socially appropriate and effective robot actions generated by the underlying planner. 
Our overall approach reduces reliance on large datasets (for training) and enhances adaptability in decision-making. In practice, it results in improved socially compliant navigation in human-shared environments. We demonstrate and evaluate our system in four different real-world social navigation scenarios with a Turtlebot robot. We observe at least $36.37\%$ improvement in average success rate and $20.00\%$ improvement in average collision rate in the four social navigation scenarios. The user study score shows that VLM-Social-Nav generates the most socially compliant navigation behavior. 

\end{abstract}


\section{Introduction}\label{sec:intro}
% What is social navigation and why is it important
Mobile robots integrated into diverse indoor and outdoor human-centric environments are becoming increasingly prevalent. These robots serve various functions, ranging from package and food delivery~\cite{starship, scout} to service~\cite{dilligent} and home assistance~\cite{astro}. Overall, these roles necessitate interaction with humans and navigating seamlessly through public spaces with pedestrians. In such dynamic scenarios, it is important for the robots to engage in socially compliant interactions and navigation~\cite{xiao2022motion, mirsky2021conflict, mavrogiannis2023core}. 

% challenges in social navigation:
\textit{Challenges of social navigation:} This paper focuses on the challenges of social navigation~\cite{mavrogiannis2023core} and addresses the ability of robots to navigate while adhering to social etiquette, especially contextual appropriateness, which requires robot to understand the relative importance of the context of the environment, tasks or interpersonal behaviors. However, navigating socially across varying contexts presents distinct challenges~\cite{francis2023principles, mavrogiannis2023core, mirsky2021conflict}, including ensuring safety, comfort, politeness, and social norms.

% Contextual Appropriateness: technical challenges from previous works
\textit{Inferring contextually appropriate navigation behaviors is challenging.} Humans have various behaviors and the environmental or task contexts are also not easily to be categorized~\cite{mavrogiannis2023core}. A common strategy to handle the challenge is by learning-based approaches to empirically learn the complicated contexts. Imitation Learning (IL) is a recent emerging paradigm for desired navigation behavior~\cite{hirose2023sacson, xiao2020appld, xiao2022learning, panigrahi2023study, raj2024targeted}. This approach enables autonomous agents to navigate socially by learning from human demonstration. Other learning approaches, such as Reinforcement Learning (RL) have also been used to address this problem~\cite{kretzschmar2016socially}. While both methods demonstrate promising results in real-world settings, substantial datasets~\cite{thorDataset2019, karnan2022scand, nguyenmusohu} for training and reward engineering are required for their successful application. 

% the potential of Language models 
\textit{Language models are inherently well-suited for contextual understanding but not well applied in social navigation.} Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) demonstrate a deep understanding of contextual information and have the potential to perform chain-of-thought~\cite{wei2022chain} and common sense reasoning~\cite{geva2021did, gpt4v2023}. Those processes are inherent to social navigation, especially the challenges of contextual appropriateness and politeness, which require understanding the task/environmental context and the behavior of humans. This capability has also been evaluated across diverse domains of robotics, including human-like driving scenarios~\cite{wen2023road, sha2023languagempc} and autonomous robot navigation~\cite{10160969,pmlr-v205-shah23b}. However, using language models for social navigation is not well explored and the language models suffer from high latency for real-time navigation, and the issue impedes the smoothness and efficiency of human-robot social interaction. 
% We leverage the capabilities of VLMs for scene understanding and reasoning to perform decision-making in social navigation, particularly in human-centered environments.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/cover_final.pdf} 
\caption{The trajectories of our approach, VLM-Social-Nav (red), DWA (blue), and BC (yellow) approaches in the frontal encountering scenario (left) and the intersection scenario (right). The resulting trajectories show that VLM-Social-Nav demonstrates more social compliant behavior as it is instructed by a prompt (\ie Move to the right when passing by a person, and do not obstruct others' paths).
} \vspace{-1.0em}
\label{fig:cover}
\end{figure} 


{\bf Main Results: } In this paper, we present VLM-Social-Nav, a new approach that uses VLMs to interpret contextual information from the robot observation to help autonomous agents improve their navigation abilities in human-centered environments. In particular, we leverage a VLM to analyze and reason about the current social interaction, and generate an immediate \textit{preferred robot action} to guide the motion planner. Our VLM-based scoring module computes the \textit{social cost}, which is used for the bottom-level motion planner to output the appropriate robot action. To overcome the limitation of existing VLMs' latency issue, we utilize a state-of-art perception model (\ie YOLO~\cite{YOLO2023}) to detect key entities that are used for social interactions (\eg humans, gestures, and doors), query a VLM to generate socially compliant navigation behavior, and compute the social cost. We demonstrate VLM-Social-Nav in four different indoor scenarios with human interactions. Unlike previous social navigation approaches, VLM-Social-Nav can better deal with social navigation scenarios by interpreting the situation based on \textit{common sense}, through observation without any training with a large dataset. Some of our main results include:
\begin{itemize}
  \item We propose VLM-Social-Nav, a novel approach for social robot navigation by integrating VLMs with optimization-based or scoring-based local motion planners and a state-of-the-art perception model. This enables robots to detect social entities efficiently and make real-time decisions on socially compliant robot behavior. 
  \item We propose a VLM-based scoring module that translates the current robot observation and the textual instructions into a relevant social cost term. This cost term is used for the bottom-level motion planner to output appropriate robot action. 
  \item We evaluate VLM-Social-Nav in four different real-world indoor social navigation scenarios. We measure the success rate and the collision rate. We also perform a user study and compare the results with a Dynamic-Window Approach (DWA)~\cite{fox1997dynamic} and Behavior Cloning (BC)~\cite{pomerleau1988alvinn} method trained on a state-of-the-art large Socially CompliAnt Navigation Dataset (SCAND)~\cite{karnan2022scand}. We achieve at least $36.37\%$ improvement in average success rate and $20.00\%$ improvement in average collision rate in the four social navigation scenarios. The user study score shows that VLM-Social-Nav generates the most socially compliant navigation behavior. 
%\todo{experiment result}
  
  %a multi-modal fusion method~\cite{panigrahi2023study}. \todo{experiment result}
\end{itemize}

\section{Related Work}
\label{sec:prev}
In this section, we give an overview of existing works related to social navigation, including safety requirements of social navigation, different challenges of contextual appropriateness, and Large Foundation Models (LFMs) for robot navigation. 

\subsection{Safety Requirement of Social Navigation}
For social navigation, keeping safety is the basic requirement for interaction with humans and navigation in dynamic challenging scenarios~\cite{van2010optimal, fox1997dynamic, liang2021vo, liang2021crowd}. DWA~\cite{fox1997dynamic} calculates the collision constraints of the robots' actions and chooses the best feasible action closest to the target as the output. Model Predictive Control approaches~\cite{10341869, williams2017information} are also widely used for collision avoidance tasks, which provide smooth trajectories for the robot to follow; the learning-based Model Predictive Control (MPC)~\cite{williams2017information, xiao2022learning} enhances the performance of the original MPC method by empirically learning the best actions for navigation, which requires a large expert demonstration dataset for training. Velocity-Obstacle (VO)-based approaches are more efficient and can be used to simulate the actions of crowds~\cite{van2010optimal, best2014densesense}, but these approaches don't take into account the uncertainties in the perception output. PRVO~\cite{gopalakrishnan2017prvo} and OFVO~\cite{liang2021vo} handle the uncertainties of perception in motion planning, but those approaches require a hard threshold to set the confidence for planning. To deal with this issue, learning-based methods empirically train the policies by demonstrations~\cite{liang2021crowd, aradi2020survey, sun2021motion}, whereas other methods~\cite{liang2021crowd,aradi2020survey} use reinforcement learning to train the robot in a simulator and implement it in real-world scenarios. However, learning-based approaches require a significant amount of data or on-policy training with realistic simulators to learn the task.
%We apply optimization-based collision avoidance approaches as baseline for social navigation.

\subsection{Contextual Appropriateness of Social Navigation}
Researchers have proposed diverse methodologies to integrate social awareness into mobile robot navigation systems. The development of a comprehensive social navigation system poses inherent complexity, requiring sophisticated perception and reasoning capabilities to navigate environments shared with humans and other robots \cite{mirsky2021conflict}. Establishing an accurate definition of social navigation is paramount, given its potential variation across cultures and platforms. Except for the safety requirement, assessing social compliance presents an additional challenge, contingent on the scenario and platform, and should adhere to Contextual Appropriateness, which includes Comfort, Legibility, Social Competency, Politeness, Understanding Other Agents, and Proactivity \cite{gao2022evaluation, francis2023principles}. 
Various methodologies are employed to address this challenge, with a significant focus on enhancing learning methods through reinforcement learning, learning from demonstration—particularly by analyzing examples of human trajectories or robots operated by humans—and the utilization of simulated datasets \cite{9679193, 5354147, 7539621, li2017role, nazeri2024vanp}. Additionally, various datasets have been collected for this purpose. % \cite{thorDataset2019, karnan2022scand, nguyenmusohu}. 
Socially CompliAnt Navigation Dataset (SCAND)~\cite{karnan2022scand} and Multi-Modal Social Human Navigation Dataset (MuSoHu)~\cite{nguyenmusohu} are two recent large-scale social human navigation datasets in many natural human-inhabited public spaces for robots to learn similar, human-like, socially compliant navigation behaviors. 
Despite extensive research employing diverse approaches with machine learning techniques~\cite{10323465}, less emphasis has been placed on formulating social navigation using VLMs, which successfully analyze the contextual information of the environment.%~\cite{kruse2013human, charalampous2017recent, mavrogiannis2023core}.

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\linewidth]{figures/pipeline.pdf}
\caption{The overall system architecture of VLM-Social-Nav. Our real-world perception (vision) model detects important social entities (\eg humans, gestures, and doors) in real time and prompts the VLM-based scoring module, indicated as in green parts, to compute social cost $C_{social}$, which is used to generate socially compliant robot action. 
} \vspace{-1.0em}
\label{fig:overview}
\end{figure*} 

\subsection{Large Foundation Models for Navigation}
Recent advancements in Language Foundation Models (LFMs)~\cite { bommasani2022opportunities}, encompassing Vision-Language Models (VLMs) and Language-Language Models (LLMs), show significant potential in robotic navigation, despite the challenges like bias and reliability~\cite{payandeh2023susceptible} associated with the LFMs. SayCan~\cite{Ahn2022} integrate LLMs for high-level task planning. GPT-Driver~\cite{ mao2023gptdriver} evaluates the performance of GPT-3.5 in simulation for autonomous driving, framing motion planning as a language modeling problem. LL3MVN~\cite{Yu_2023} constructs semantic maps of environments and utilizes LLMs to reach long-term goals, while LLaDA~\cite{li2024driving} enables autonomous vehicles to adapt to diverse traffic rules across regions. LM-Nav~\cite{shah2022lmnav} utilizes GPT-3 and CLIP~\cite{radford2021learning} to navigate outdoor environments based on natural language instructions, combining language and visual cues for optimal path planning. 
RT-1~\cite{brohan2022rt1} and RT-2~\cite{brohan2023rt2} models leverage multimodal robotics data for navigating complex environments and offer greater generalizability through larger dataset training \cite{shah2022lmnav, xu2023vision, brohan2022rt1, brohan2023rt2}. However, language models are not well explored for social navigation, even though VLMs and LLMs are powerful for understanding contextual information. We bridge the gap and propose a novel approach for social navigation by taking advantage of the contextual understanding capability of VLMs.

% However, limited research has focused on short-horizon path planning for socially aware navigation in real-world settings. 

% their adoption has led to deeper insights into social interactions and more nuanced, context-aware navigation strategies. Works such as SayCan~\cite{Ahn2022} integrate LLMs for high-level task planning in natural language instructions, leveraging pretrained skills for context-specific actions. GPT-Driver~\cite{ mao2023gptdriver} evaluates the performance of GPT-3.5 in simulation for autonomous driving, framing motion planning as a language modeling problem. LL3MVN~\cite{Yu_2023} constructs semantic maps of environments and utilizes LLMs to reach long-term goals, while LLaDA~\cite{li2024driving} enables autonomous vehicles to adapt to diverse traffic rules across regions. LMPC~\cite{liang2024learning} fine-tunes PaLM 2~\cite{anil2023palm} using in-context learning data to enhance generalization to unseen embodiments. %Xu et al.~\cite{xu2023vision} employ VLMs to generate visual-language maps for navigation tasks. 

% Multimodal foundation models have further enhanced navigation capabilities by integrating textual and visual data for more adaptive strategies. LM-Nav~\cite{shah2022lmnav} utilizes GPT-3 and CLIP~\cite{radford2021learning} to navigate outdoor environments based on natural language instructions, combining language and visual cues for optimal path planning. Xu et al.~\cite{xu2023vision} employ VLMs to generate visual-language maps for navigation tasks. RT-1~\cite{brohan2022rt1} and RT-2~\cite{brohan2023rt2} models leverage multimodal robotics data for navigating complex environments and offer greater generalizability through larger dataset training \cite{shah2022lmnav, xu2023vision, brohan2022rt1, brohan2023rt2}. However, limited research has focused on short-horizon path planning for socially aware navigation in real-world settings. 


\section{Approach}\label{sec:ap}

In this section, we define the social navigation problem and describe VLM-Social-Nav in detail. 

\subsection{Problem Definition}

Navigation is the task of generating and following an efficient collision-free path from an initial location to a goal~\cite{mavrogiannis2023core}. In general, the overall system consists of a global planner and a local planner. A global planner is designed to find a collision-free path to reach a goal, while a local planner aims to navigate the robot through its immediate surroundings, making real-time adjustments to deal with vehicle dynamics and around obstacles. 

For social robot navigation, humans are no longer perceived only as dynamic obstacles but also as social entities~\cite{rios2015proxemics, mirsky2021conflict}. It necessitates integrating social norms into robot behaviors. 
We define the social robot navigation problem as a {\em Markov Decision Process (MDP)}: $\langle \cs, \ca, \ct, \cc \rangle$ where $\s = (x, y, \theta) \in \cs$ is a state consisting of a robot pose, $\a = (v, w) \in \ca$ is an action consisting of a linear and an angular velocity of a robot, $\ct: \cs \times \ca \rightarrow \cs$ is the transition function characterizing the dynamics of the robot, and $\cc: \cs \times \ca \rightarrow \RR $ is a cost function. Given a cost function $\cc$, the motion planner finds $(v^*, w^*)$ that minimizes the expected cost. The cost function takes the following form:
\begin{equation}\label{eq:cost}
\begin{aligned}
    C(\s,\a)=\alpha\cdot C_{goal}+\beta\cdot C_{obst}+\gamma\cdot C_{social},
\end{aligned}
\end{equation}
where $C_{goal}$ encourages movement toward the goal, $C_{obst}$ discourages collisions with obstacles, and $C_{social}$ encourages the robot to follow the social norms. $\alpha$, $\beta$, and $\gamma$ are a non-negative weight for each cost term. 

The social cost term $C_{social}$ encompasses various factors that govern human-robot interactions in shared environments. Defining them mathematically poses challenges. For VLM-Social-Nav, we define $C_{social}$ as:
\begin{equation}\label{eq:social}
\begin{aligned}
    C_{social}= \|\cb - \cb_{h} \|,
\end{aligned}
\end{equation}
where $\cb$ is a navigation behavior, and $\cb_{h}$ is a navigation behavior humans would adopt in accordance with social conventions. Minimizing the deviation between them will encourage the robot to emulate socially acceptable human behavior. While $\cb_{h}$ can be obtained through various methods, including large datasets~\cite{thorDataset2019, karnan2022scand, nguyenmusohu}, we leverage the power of VLM to compute appropriate behavior based on a rich contextual understanding and nuanced interpretations from perceived images and given prompts. We elaborate further in Section~\ref{sec:vlm}.


\subsection{VLM-based Social Navigation Architecture}

Figure~\ref{fig:overview} highlights the overview of VLM-Social-Nav. Our formulation is built upon an autonomous navigation system comprising a global path planner and a local motion planner. Our local motion planner considers the sensor inputs and outputs a robot action that minimizes the cost function $C$.
%The global path planner generates a high-level trajectory for the robot to follow, considering obstacles and the goal location. The local planner refines this trajectory by considering the sensor inputs and outputs a robot action that minimizes the cost function $C$.

To improve navigation abilities in a social-interaction context, we introduce a VLM-based scoring module. As the robot navigates through the environment, our real-world perception module detects social entities, such as humans, gestures, and doors, in real time. When important social cues are detected, our VLM-based scoring module is prompted and outputs a socially acceptable robot action based on the current robot observation. It is calculated as a cost term to be used by the motion planner. 
VLM-Social-Nav is designed to make real-time decisions on robot actions based on the current observations. The social cost term is integrated only when it is necessary, \ie when there is any human interaction involved. 


\begin{figure}[tb]
\begin{GrayBox}       
    \small{\textbf{Input Image:} \\ \\}
    %\hspace{0.2\linewidth}
    \includegraphics[width=0.65\linewidth]{figures/frames_65.jpg} \\ \\
    \small{\textbf{Input Prompt:}} \\ \\
    \small{
    \textit{Task:} \\
    How will you navigate concerning the person in your view? You will need to follow general walking etiquette. \\
    
    \textit{Ego state:} \\
    - heading direction: \textcolor{blue}{straight}\\
    - linear velocity: \textcolor{blue}{0.28}\\

    \textit{Remember:} \\
    - Move to the right when passing by a person.\\
    - Do not obstruct others' paths.\\
    - Pass on the left when overtaking a person.\\
    - ... \\

    \textit{Answer Format:} \\
    Move \textcolor{red}{DIRECTION} with \textcolor{red}{SPEED} \\
    - options for DIRECTION: left, straight, right \\
    - options for SPEED: slow down, speed up, constant, stop
    }
\end{GrayBox}
\caption{An example input image ($\ci^t$) and prompt ($\cp$) used in VLM-Social-Nav. Parameterized inputs ($\a^t$) are highlighted in blue. Formatted outputs are highlighted in red. The example input data is one of the frontal approach scenarios from MuSoHu~\cite{nguyenmusohu}. The output of VLM is \textit{Move right with slow down}, to slowly pass by a person on the right side, which was the same action the collected data took. 
} \vspace{-1.0em}
\label{fig:prompt}
\end{figure} 

\subsection{VLM-based Scoring Module} \label{sec:vlm}


VLM plays a crucial role in VLM-Social-Nav to infer immediate socially compatible navigation behavior $\cb_h^{t+1}$ based on its pre-trained large internet-scale dataset:
\begin{equation}\label{eq:vlm}
\begin{aligned}
    \cb_h^{t+1} = \text{VLM}(\ci^t, \cp ,\a^t) ,
\end{aligned}
\end{equation}
where $\ci^t$ is an RGB image of the robot view at time $t$, $\cp$ is a textual prompt, $\a^t$ is a current robot action at time $t$. Inspired by In-Context Learning (ICL), our prompt $\cp$ is designed to leverage the VLM's reasoning abilities through zero-shot examples. This approach offers an interpretable interface, mirroring human reasoning and decision-making processes, without extensive training~\cite{min2022rethinking}.

Our VLM-based scoring module starts from the insight that the action space of a mobile robot can be readily mapped to linguistic terms. For example, the action ``move forward at a constant speed'' can be linked to a linear velocity of $v^{t}$ m/s and an angular velocity of 0. %, where $v^{t}$ represents the current linear velocity. 
The heading direction on the left indicates a positive value of $w^{t}$, while the direction on the right indicates a negative. Leveraging this understanding, we structure the output of the VLM into a linguistic format comprising the heading and the speed. Subsequently, our scoring module extracts $\cb_h^{t+1} \mapsto (v_h^{t+1}, w_h^{t+1}) \in \ca$ from these tokens; $v_h^{t+1} = v^t + \delta_{s}$, where $\delta_{s}$ is derived from the response for SPEED; $w_h^{t+1} = \delta_{d}$, where $\delta_{d}$ is derived from the response for DIRECTION. 
Thus, the social cost term for the next time step can be calculated:
\begin{equation}\label{eq:social_vlm}
\begin{aligned}
    C_{social}^{t+1}= w_{l} \cdot \| v - v_h^{t+1} \| + w_{a} \cdot \| w - w_h^{t+1} \|,
\end{aligned}
\end{equation}
where $w_{l}$ and $w_{a}$ are a non-negative weight values. Given all the cost terms, our low-level optimization-based motion planner finds the robot action $(v^*, w^*)$ that minimizes the cost. 

Figure~\ref{fig:prompt} shows an example prompt $\cp$ used in our experiment. We provide a high-level task description along with an image $\ci^t$ captured from the robot's perspective. Furthermore, the current robot action $\a^t=(v^t,w^t) \in \ca$ is provided. %, which will be employed in calculating $C_{social}$. 
Supplementary instructions regarding walking etiquette are also included. Although the VLM demonstrates proficient navigation abilities even in the absence of explicit instructions, offering reasoning guidelines enhances its decision-making processes~\cite{min2022rethinking}. These guidelines not only facilitate comprehensive reasoning and judgment within the VLM but also enable the robot to adapt to specific rules more effectively. For example, in a country where it's customary to walk on the left, we can rephrase the prompt as ``Move to the left when passing by another person''. 
% We format the output of VLM as an immediate action that the robot should take for the next step, specifying the heading direction and the speed.




% $\cb_h \rightarrow \ca$
% We map \cb^{t} \rightarrow \ca

\begin{figure*}[htb]
\centering
\begin{tabular}{rc}
\spheading{(a) Frontal Approach} \hspace{-0.6em} & \includegraphics[width=0.82\linewidth]{figures/result/result_frontal.pdf} \\
\spheading{(b) Frontal Approach with Gesture}  \hspace{-0.6em} & \includegraphics[width=0.82\linewidth]{figures/result/result_gesture.pdf} \\
\spheading{(c) Intersection} \hspace{-0.6em}  &  \includegraphics[width=0.82\linewidth]{figures/result/result_intersection.pdf} \\
\spheading{(d) Narrow Doorway} \hspace{-0.6em} &  \includegraphics[width=0.82\linewidth]{figures/result/result_door.pdf} \\
\end{tabular}
\caption{Qualitative Results: the robot motion results with VLM-Social-Nav for four social navigation scenarios: (a) Frontal Approach, (b) Frontal Approach with Gesture, (c) Intersection, and (d) Narrow Doorway. The gray solid arrow shows the participant's path. The red solid arrow shows the robot's path. The red and gray dashed arrows show the robot's and participant's path respectively after a stop motion. A caption on the top left shows the result from the VLM.} \vspace{0.4em}
\label{fig:qualitative}
\end{figure*}  

\begin{table*}[htb] 
\centering
\caption{Quantitative Results: performance comparisons using BC~\cite{pomerleau1988alvinn} method trained on SCAND dataset~\cite{karnan2022scand}, DWA~\cite{fox1997dynamic}, and VLM-Social-Nav. We achieve at least $36.37\%$ improvement in average success rate and $20.00\%$ improvement in average collision rate in the four social navigation scenarios. The user study score shows that VLM-Social-Nav generates the most socially compliant navigation behavior. }
\begin{tabular}{rccccc} 
 \midrule
 \multirow{2}{*}{\textbf{Metric}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{Scenario}} \\
 \cmidrule(lr{0.5em}){3-6}
 & &  (a) Frontal Approach & (b) Frontal Approach  w/ Gesture & (c) Intersection & (d) Narrow Doorway \\
 \midrule
 \multirow{3}{*}{Success Rate (\%)} & BC & 26.66 & 0 & 13.33 & 33.33\\
 & DWA & \textbf{100} & \textbf{0} & 93.33 & \textbf{100}\\
 & VLM-Social-Nav & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100}\\
 \midrule
 \multirow{3}{*}{Collision Rate (\%)} & BC & 46.66 & 60.00 & 33.33 & 40.00\\
 & DWA & 26.66 & 20.00 & 13.33 & 33.33 \\
 & VLM-Social-Nav & \textbf{6.66} & \textbf{0} & \textbf{0} & \textbf{6.66} \\
 \midrule
 \multirow{3}{*}{User Study Score} & BC & 2.80 $\pm$ 1.45 & 2.23 $\pm$ 1.54 & 2.80 $\pm$ 1.40 & 2.60 $\pm$ 1.33\\
 & DWA & 3.99 $\pm$ 0.80 & 3.38 $\pm$ 0.64 & 3.57 $\pm$ 0.62 & 3.59 $\pm$ 0.83\\
 & VLM-Social-Nav & \textbf{4.31 $\pm$ 0.72} & \textbf{4.28 $\pm$ 0.56} & \textbf{4.35 $\pm$ 0.70} & \textbf{4.04 $\pm$ 0.74}\\
 \midrule
\end{tabular}
\label{table:quantitative}
\end{table*} 



\section{Experiments}\label{sec:exp}

In this section, we detail the implementation of VLM-Social-Nav and describe our qualitative and quantitative results including comparisons with other methods and the user study. 

\subsection{Implementation Details}

VLM-Social-Nav is tested on a Turtlebot 2 equipped with a Velodyne VLP16 lidar, and a Zed 2i camera, and a laptop with intel i7 CPU and an Nvidia GeForce RTX 2080 GPU. 
We use YOLO~\cite{YOLO2023} as our real-world perception model to detect key objects (\eg humans, door, and gesture) and Generative Pre-trained Transformer 4 with Vision (GPT-4V)~\cite{gpt4v2023} as our VLM to comprehend the social dynamics and output the immediate preferred robot action. We combined our method with a low-level motion planner DWA~\cite{fox1997dynamic}. We compare VLM-Social-Nav with DWA without social cost $C_{social}$ and BC~\cite{pomerleau1988alvinn} method trained on a state-of-the-art, large-scale social navigation dataset, SCAND~\cite{karnan2022scand}.

%multi-modal fusion method~\cite{panigrahi2023study}, which adopts an imitation learning approach to learn navigation behavior by fusing an RGB image and a point cloud.

To validate VLM-Social-Nav, we carefully follow the social robot navigation studies~\cite{francis2023principles, pirk2022protocol} that have set up the benchmark scenarios and the metrics for measuring social navigation. 
We present qualitative, quantitative, and user study results in four different indoor social navigation scenarios:
\begin{itemize}
  \item \textbf{Frontal Approach:} A robot and a human approach each other from two ends of a straight trajectory.  
  \item \textbf{Frontal Approach with Gesture:} A robot and a human approach each other from two ends of a straight trajectory. The human recognizes the robot and then gestures for it to stop.
  \item \textbf{Intersection:} A robot and a human cross each other on perpendicular trajectories. 
  \item \textbf{Narrow Doorway:} A robot and a human cross each other's paths by moving through a narrow doorway. 
\end{itemize}

\subsection{Qualitative Result}

Based on the protocols and principles set by other studies~\cite{francis2023principles, pirk2022protocol}, the robot is expected to behave in a socially compliant way as follows: 
\begin{itemize}
  \item \textbf{Frontal Approach:} The robot is expected to yield or slow down and modify its original trajectory so as not to obstruct the human path. Similar to driving rules in North America, it is conventional to keep on the right side. 
  \item \textbf{Frontal Approach with Gesture:} The robot is expected to yield by interpreting the human gesture. 
  \item \textbf{Intersection:} The robot is expected to drive slowly when it approaches the human. It may come to a complete stop or modify its original trajectory to go behind the human to not obstruct the path. 
  \item \textbf{Narrow Doorway:} The robot is expected to wait outside the door and yield to the human. 
\end{itemize}

Fig.~\ref{fig:qualitative} shows snapshots of the resulting robot motion using VLM-Social-Nav in four different scenarios. We demonstrate that VLM-Social-Nav follows the social convention and navigates toward its goal as expected. 
Fig.~\ref{fig:cover} illustrates the resulting trajectories of VLM-Social-Nav in comparison to those of DWA and BC methods. A notable observation is that while DWA also effectively avoids collisions with individuals, VLM-Social-Nav generates trajectories that align more closely with social norms. For instance, in the frontal approach scenario, while DWA tends to maneuver around the person either to the right or left, VLM-Social-Nav predominantly bypasses them on the right side. Similarly, in the intersection scenario, whereas DWA occasionally obstructs the person's path by veering to avoid collision directly in front, VLM-Social-Nav endeavors to pass behind the individual. Additionally, BC successfully avoids humans but fails to recover and follow the original path. This leads to many failures in reaching the goal. 
The accompanying supplementary video shows the resulting robot motions. 


\begin{table}[htb] 
\centering
\caption{Social Compliance Questionnaire}
\begin{tabularx}{\linewidth}{l|l} 
 \midrule
 \multicolumn{2}{l} {\textbf{Scenario 1: Frontal Approach}} \\
 \midrule
 1 & The robot moved to avoid me.\\
 2 & The robot obstructed my path.$^{*}$\\
 3 & The robot maintained a safe and comfortable distance at all times.\\
 4 & The robot nearly collided with me.$^{*}$\\
 5 & It was clear what the robot wanted to do.\\
 \midrule
  \multicolumn{2}{l} {\textbf{Scenario 2: Frontal Approach with Gesture}} \\ 
 \midrule
 6 & The robot maintained a safe and comfortable distance at all times.\\
 7 & The robot slowed down and stopped.\\
 8 & The robot followed my command\\
 9 & I felt the robot paid attention to what I was doing.\\
 \midrule
 \multicolumn{2}{l} {\textbf{Scenario 3: Intersection}} \\ 
 \midrule
 10 & The robot let me cross the intersection by maintaining a safe and \\ & comfortable distance.\\
 11 & The robot changed course to let me pass.\\
 12 & The robot paid attention to what I was doing.\\
 13 & The robot slowed down and stopped to let me pass.\\
 \midrule
 \multicolumn{2}{l} {\textbf{Scenario 4: Narrow Doorway}} \\ 
 \midrule
 14 & The robot got in my way.$^{*}$\\
 15 & The robot moved to avoid me.\\
 16 & The robot made room for me to enter or exit.\\
 17 & It was clear what the robot wanted to do.\\
 \midrule
\end{tabularx} 
%}
\label{table:userstudy}
\end{table} 


\subsection{Quantitative Result}

To further validate VLM-Social-Nav, we evaluate the methods using three different 
metrics~\cite {francis2023principles}. The success rate describes whether the robot reaches the goal. For the frontal approach with gesture scenario, we mark it as successful when the robot reacts to the gesture. The collision rate describes whether the robot collided with the human or other objects in the environment. We also mark it as collision when we manually intervene to avoid an imminent collision with the human subject or surroundings. The user study score is an average score we obtained from the user study detailed in Section~\ref{sec:userstudy}.

Table~\ref{table:quantitative} reports the results averaged over 15 runs for each method and scenario. The results demonstrate that VLM-Social-Nav, DWA with social cost, outperforms other methods in every metric. DWA excels at following a path smoothly, yet it faces challenges in collision avoidance %as it relies solely on the lidar sensor. 
%This occurs 
when the participant walks too quickly and the robot attempts to make an abrupt change in motion. We also observed that the outcomes of BC varied. At times, when attempting to avoid collisions, it failed to return to its original path and failed to reach the goal. Conversely, there were instances where it didn't attempt collision avoidance at all, resulting in collisions with the participants. For gesture recognition, only our proposed method successfully responded to the participants' gestures. In total, we achieve at least $36.37\%$ improvement in the average success rate and $20.00\%$ improvement in the average collision rate in the four social navigation scenarios.
%The results demonstrate that \todo{}.

\subsection{User Study}\label{sec:userstudy}

To validate the social compliance of VLM-Social-Nav, we conduct a user study. 
We ask the participants to walk along the predefined trajectory and ask them to answer questionnaires about the robot motion~\cite{pirk2022protocol} (Table~\ref{table:userstudy}). * denotes negatively formulated questions, for which we reverse-code the ratings to ensure comparability with the positively formulated ones. The three methods are randomly shuffled and repeated three times. Each scenario is tested on five different participants. 
We use a five-level Likert scale to ask participants to rate their agreement toward these statements. 

Fig.~\ref{fig:userstudy} and the user study score on Table~\ref{table:quantitative} show the study result. The plot shows the per-question average scores for the three methods in each scenario. 
Based on the results, it's evident that VLM-Social-Nav received the highest level of agreement from participants across all questions, indicating its strong adherence to social norms. The standard error of the BC method was also large, indicating that the performance of the BC method was not constant. The score difference between VLM-Social-Nav and DWA was not large in the narrow doorway scenario. This is because, when attempting to enter the narrow doorway, the robot motion results from DWA indicated that it failed to find a plan and froze, resembling almost a complete stop. 
%From the result, we can see that \todo{}.

\begin{figure}[t]
\centering
\includegraphics[width=0.49\linewidth]{figures/plot/frontal.pdf}
\includegraphics[width=0.49\linewidth]{figures/plot/gesture.pdf}
\includegraphics[width=0.49\linewidth]{figures/plot/intersection.pdf}
\includegraphics[width=0.49\linewidth]{figures/plot/door.pdf}
\includegraphics[width=0.35\linewidth]{figures/plot/legend.pdf}
\caption{User Study Average Scores: the per-question average scores for the three methods in each scenario. The results indicate that VLM-Social-Nav (Ours) garnered the highest level of agreement from participants across all questions, highlighting its robust alignment with social norms.} \vspace{-1.0em}
\label{fig:userstudy}
\end{figure} 

% \subsection{Discussion}
% From our experiments, we conclude that integrating VLM into robot navigation helped improve the robot navigation behavior in a socially compliant manner. 

% Although our robot experiments were conducted only indoors, according to the example in Fig.~\ref{fig:prompt}, our approach can be extended to outdoor scenarios. This will be our direct future work, 

\section{Conclusion}\label{sec:con}
We propose a novel social navigation approach based on Vision-Language Models (VLMs), focusing on real-time, socially compliant decision-making in human-centric environments. We utilize the perception model to detect important social entities and prompt a VLM to generate guidance for socially compliant behavior. VLM-Social-Nav features a VLM-based scoring that ensures socially appropriate and effective robot actions. This minimizes the dependence on extensive training datasets and eliminates the necessity for explicit rules or rewards typically associated with reinforcement learning methods. By furnishing textual instructions to VLM, we can instruct the robot to adhere to specific navigation rules, such as navigating on the right or left according to cultural norms. %Consequently, our approach significantly boosts the adaptability and scalability of decision-making in autonomous social robot navigation. 
In practice, it results in improved socially compliant navigation in human-shared environments. We demonstrate and evaluate our system in four different real-world social navigation scenarios with a Turtlebot robot.

Although our robot experiments were conducted only indoors, according to the example in Fig.~\ref{fig:prompt}, VLM-Social-Nav can be extended to outdoor scenarios. Our immediate focus will be on advancing our work into a global outdoor navigation system. %This will require resolving the drawbacks of our current formulation. 
VLM-Social-Nav is taking certain social entities to query VLM. To make it work on more general cases, when or where to query VLM will be a critical consideration. 
Our experiments revealed that VLM excels when presented with choices rather than being tasked with making singular, decisive actions. We aim to explore more on the methods to provide VLM with better navigation options beyond simple direction and speed adjustments. 
% We plan to address these limitations in our future work, and extend our formulation to long-range navigation.

%Our approach relies on a large VLM that operates online. Consequently, its performance is susceptible to fluctuations in internet connectivity. Also, 
% Although such limitations can be easily overcome by the advancements in the model itself, we are interested in overcoming this issue algorithmically



% Our approach uses a VLM-based scoring module that computes a cost term that ensures socially appropriate and effective robot actions generated by the underlying planner. 
% This not only reduces the reliance on extensive training data but also alleviates the need to define rules or rewards for every possible scenario explicitly, thereby enhancing the adaptability and scalability of the decision-making process in autonomous robot navigation. 
% Our overall approach reduces reliance on large datasets (for training) and enhances adaptability in decision-making. 
% In practice, it results in improved socially compliant navigation in human-shared environments. We demonstrate and evaluate our system in four different real-world social navigation scenarios with a Turtlebot robot.

