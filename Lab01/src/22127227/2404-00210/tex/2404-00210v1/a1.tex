
\begin{abstract}
%With the growing presence of robots in human-centered environments, they must not only navigate safely among people but also comply with intricate social norms. Traditional learning-based approaches face limitations due to the need for extensive datasets and defining reward functions. 

We propose a novel Vision-Language Models (VLMs) based navigation approach that improves a robot's navigation behavior in human-centered environments. 
Our goal is to make real-time decisions on robot actions that are socially compliant with human expectations. 
We utilize the perception model to detect important social entities, prompting VLM to generate guidance for socially compliant robot behavior. Based on this guidance, our VLM-based scoring module computes a cost term that ensures socially appropriate and effective robot actions generated by the underlying motion planner. 
Our approach reduces reliance on large datasets and enhances adaptability in decision-making, offering promising prospects for improved robot navigation in human-shared environments. We demonstrate and evaluate our system in four different real-world social navigation scenarios with Turtlebot. Our approach results in \todo{experiment result}.

\end{abstract}


\section{Introduction}\label{sec:intro}

% motivation scenario
Mobile robots integrated into diverse indoor and outdoor human-centric environments are becoming increasingly prevalent. These robots serve various functions, ranging from package and food delivery~\cite{starship, scout} to service~\cite{dilligent} and home assistance~\cite{astro}. Crucially, these roles necessitate interaction with humans and navigating seamlessly through public spaces with pedestrians. In such dynamic scenarios, it is important for the robots to navigate and interact with humans in a socially compliant manner~\cite{xiao2022motion, mirsky2021conflict, mavrogiannis2023core}. 

% autonomous navigation
% Extensive research has been dedicated to the study of autonomous mobile robots traversing from one point to another without encountering collisions~\cite{fox1997dynamic, zelinsky1992mobile, jan2008optimal}. 
% However, in human-centered environments, the navigation task gains complexity. While avoiding any obstacle on the way to the goal is still required, they also need to consider other dynamic agents (\ie humans) sharing the space. Consequently, the robots must adapt their decision-making process to generate navigation behaviors that align with the underlying social conventions.

% safety-distance
% A fundamental prerequisite concerning the interaction between robots and human social dynamics is remaining the safety distance for each other~\cite{francis2023principles}. 
% Thus, the research addressing the challenge of maintaining a safe distance from humans in cluttered dynamic scenes has been carried out~\cite{10341869, liang2021crowd}. 
% Nonlinear Model Predictive Control (NMPC) is one of the widely used methods for local motion planning and control in such scenarios~\cite{schoels2020nmpc, 10341702, xiao2022learning}. They deal with the collision-free robot navigation problem as a nonlinear optimization problem. While these methods incorporate safety protocols, they have yet to account for additional social navigation protocols, such as social competency required to comply with social norms for sharing space.

% social norm
%Many social competencies involve following conventions rather than optimizing performance. For example, in the absence of norms there is no optimization preference for driving on the left or right, but identifying and following the local norms helps prevent conflicts in the hallway interaction scenario. 
%While these social conventions are instinctive to humans, devising algorithms for robotic motions could be challenging. To deal with such challenges, machine learning approaches for robots to learn a variety of unwritten social norms have been employed~\cite{xiao2022motion, mirsky2021conflict, mavrogiannis2023core}. 
Commonly referred to as social robot navigation~\cite{mavrogiannis2023core}, which encompasses the ability of robots to navigate while adhering to social norms and etiquette, it adds another layer of complexity to the navigation problem. 
One prevalent approach to achieving such social compliance in robot navigation involves the utilization of machine learning techniques.
Imitation Learning (IL) is a recent emerging paradigm for desired navigation behavior~\cite{hirose2023sacson, xiao2020appld, xiao2022learning, panigrahi2023study, raj2024targeted}. This approach enables autonomous agents to navigate socially by learning from human demonstration. However, it requires large-scale datasets containing socially compliant navigation demonstrations in the real-world scenario~\cite{thorDataset2019, karnan2022scand, nguyenmusohu}. Other learning approaches, such as Reinforcement Learning (RL) have also been applied to address this problem~\cite{kretzschmar2016socially}. 
While both methods demonstrate remarkable results in real-world settings, substantial datasets for training and reward engineering are essential for their successful application. 
%While both methods demonstrate remarkable results in real-world settings, learning-based approaches are limited to specific social scenarios. IL requires substantial datasets for training and defining a reward function for social conventions for RL is not a straightforward task. 


% VLM
Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) demonstrate a deep understanding of contextual information and have potential to perform chain-of-thought~\cite{wei2022chain} and common sense reasoning~\cite{geva2021did, gpt4v2023}. 
This capability has been evaluated across diverse domains, including human-like driving scenarios~\cite{wen2023road, sha2023languagempc} and tasks in autonomous robot navigation~\cite{10160969,pmlr-v205-shah23b}. 
Building upon these recent research findings, we leverage the capabilities of VLMs for scene understanding and reasoning to inform decision-making in autonomous robot navigation, particularly in navigating human-centered environments in a socially compliant manner. %This not only reduces the reliance on extensive training data but also alleviates the need to explicitly define rules or rewards for every possible scenario, thereby enhancing the adaptability and scalability of the decision-making process in autonomous robot navigation.

%streamlines the decision-making process, thereby enhancing the efficiency and effectiveness of autonomous robot navigation in complex social settings.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{example-image} 
\caption{Cover Image}
\label{fig:cover}
\end{figure} 


% \newline \vspace{-0.5em}
% \newline 
{\bf Main Results: } In this paper, we present a new approach that uses VLMs to interpret contextual information from the robot observation to help autonomous agents improve their navigation abilities in human-centered environments. In particular, we leverage a VLM to analyze and reason about the current social interaction, and generate an immediate \textit{preferred robot action} to guide the bottom-level motion planner. %Our bottom-level motion planner assures efficient and collision-free navigation while VLM is used for interpreting the situation and outputs socially compliant navigation behaviors. 
Our VLM-based scoring module computes the \textit{social cost}, which is used for the bottom-level motion planner to output the optimal robot action. To overcome the limitation of existing VLMs' latency issue, we utilize the state-of-art perception model (\ie YOLO~\cite{YOLO2023}) to detect key entities that are used for social interactions (\eg humans, gestures, and doors) and then to trigger VLM to compute the social cost. 
We demonstrate our approach in four different indoor scenarios with human interactions. Unlike previous social navigation approaches, our method can deal with social navigation problems by interpreting the situation based on \textit{common sense}, through observation without any training with a large dataset. 

In summary, our main contributions include:
\begin{itemize}
  \item We propose a novel approach for social robot navigation by integrating VLMs with optimization-based or scoring-based local motion planners and state-of-the-art perception models. This enables robots to detect social entities efficiently and make real-time decisions on socially compliant robot behavior. 
  %. We utilize the state-of-art perception model for detection, which allows our approach for real-time decision-making. 
  %This enables robots to utilize external contextual information and current observations to generate socially compliant actions tailored to the situation.
  \item We propose a VLM-based contextual scoring module that translates the current robot observation and the textual instructions into a relevant social cost term. This cost term is used for the bottom-level motion planner to output the sub-optimal robot action. 
  \item We evaluate our approach in four different real-world indoor social navigation scenarios. We measure the success rate, collision rate, and clearing distance. We also perform a user study and compare the results with a Dynamic-Window approach (DWA)~\cite{fox1997dynamic} and a multi-modal fusion method~\cite{panigrahi2023study}. \todo{experiment result}
\end{itemize}

\section{Related Work}\label{sec:prev}

In this section, we discuss the existing work on collision-free navigation, social robot navigation, and Large Foundation Models (LFMs) in the context of robot navigation. 

\subsection{Collision-free Navigation}
For indoor navigation, collision avoidance is a classical problem that has been studied for decades~\cite{van2010optimal, fox1997dynamic, liang2021vo, liang2021crowd}. The DWA~\cite{fox1997dynamic} calculates the collision constraints of the robots' actions and chooses the best feasible action closest to the target as the output. However, this method is computationally costly. Velocity-Obstacle (VO)-based approaches are more efficient and can be used to simulate the actions of crowds~\cite{van2010optimal, best2014densesense}, but these approaches don't take into account the uncertainties of the perceptions. PRVO~\cite{gopalakrishnan2017prvo} and OFVO~\cite{liang2021vo} calculate the uncertainties of perception in motion planning, but those approaches require a hard threshold to set the confidence for planning. To deal with this issue, learning-based methods empirically train the policies by demonstrations~\cite{liang2021crowd, aradi2020survey, sun2021motion}, whereas CrowdSteer~\cite{liang2021crowd} uses a reinforcement learning method to train the robot in a simulator and implement it in real-world scenarios. Model Predictive Control approaches~\cite{10341869, williams2017information} are also widely used for collision avoidance tasks, which provide smooth trajectories for the robot to follow; the learning-based MPC~\cite{williams2017information} enhances the performance of the original MPC method by empirically learning the best actions for navigation. %In our approach, we also use MPC methods as motion planning baselines.

%\subsection{Model Predictive Control}
%MPC~\cite{10341702, 10341869}

\subsection{Social Navigation}

Researchers have extensively explored diverse methodologies to integrate social awareness into mobile robot navigation systems. The development of a comprehensive social navigation system poses inherent complexity, requiring sophisticated perception and reasoning capabilities to navigate environments shared with humans and other robots \cite{mirsky2021conflict}. Establishing an accurate definition of social navigation is paramount, given its potential variation across cultures and platforms. Assessing social compliance presents an additional challenge, contingent on the scenario and platform, and should adhere to key principles including Safety, Comfort, Legibility, Social Competency, Politeness, Understanding Other Agents, Proactivity, and Contextual Appropriateness \cite{gao2022evaluation, francis2023principles}. 
Various methodologies are employed to address this challenge, with a significant focus on enhancing learning methods through reinforcement learning, learning from demonstration—particularly by analyzing examples of human trajectories or robots operated by humans—and the utilization of simulated datasets \cite{5354147, 7539621, li2017role}. Additionally, various datasets have been collected for this purpose \cite{thorDataset2019, karnan2022scand, nguyenmusohu}. Despite extensive research employing diverse approaches with machine learning techniques, less emphasis has been placed on formulating social navigation using VLMs for low-level motion planning \cite{kruse2013human, charalampous2017recent, mavrogiannis2023core}.

% Researchers have explored various approaches to embed social awareness into mobile robot navigation system. Creating a comprehensive social navigation system is inherently complex and cross-disciplinary in nature, requiring advanced perceptions and reasoning to navigate environments populated by humans and other robots\cite{mirsky2021conflict}. Regardless of the solution to the problem, having an accurate definition of social navigation is crucial, which may vary across different cultures and platforms. Evaluating social compliance presents another challenge that can differ based on the scenario and platform and should follow key principles. These include Safety - protecting humans, other robots, and their environments; Comfort - avoiding annoyance or stress; Legibility - behaving in ways that make goals understandable; Social Competency - complying with social norms; Politeness - being respectful and considerate; Understanding Other Agents - predicting and accommodating the behaviors of others; Proactivity - taking the initiative to prevent and resolve issues; and Contextual Appropriateness - ensuring actions are suitable for the specific context~\cite{gao2022evaluation, francis2023principles}. There are various methods for tackling the problem. A considerable amount of work is focused on enhancing learning methods through reinforcement learning~\cite{5354147}, learning from demonstration—particularly by analyzing examples of human trajectories or robots being operated by humans— ~\cite{7539621}and the utilization of simulated datasets~\cite{li2017role}. Additionally, different datasets are collected for this purpose~\cite{thorDataset2019, karnan2022scand, nguyenmusohu}. While extensive research has been conducted using various approaches with machine learning techniques, less has focused on the formulation of social navigation using VLMs for low-level motion planning~\cite{kruse2013human,charalampous2017recent, mavrogiannis2023core}.


\subsection{Large Foundation Models for Navigation}

Recent advancements in Language Foundation Models (LFMs)~\cite { bommasani2022opportunities}, including Vision-Language Models (VLMs) and Language-Language Models (LLMs), have revolutionized robotic navigation. Despite challenges like bias and reliability~\cite{payandeh2023susceptible} concerns associated with LFMs, their adoption has led to deeper insights into social interactions and more nuanced, context-aware navigation strategies. Works such as SayCan~\cite{Ahn2022} integrate LLMs for high-level task planning in natural language instructions, leveraging pretrained skills for context-specific actions. GPT-Driver~\cite{ mao2023gptdriver} evaluates the performance of GPT-3.5 in simulation for autonomous driving, framing motion planning as a language modeling problem. LL3MVN~\cite{Yu_2023} constructs semantic maps of environments and utilizes LLMs to reach long-term goals, while LLaDA~\cite{li2024driving} enables autonomous vehicles to adapt to diverse traffic rules across regions. LMPC~\cite{liang2024learning} fine-tunes PaLM 2~\cite{anil2023palm} using in-context learning data to enhance generalization to unseen embodiments.

Multimodal foundation models have further enhanced navigation capabilities by integrating textual and visual data for more adaptive strategies. LM-Nav~\cite{shah2022lmnav} utilizes GPT-3 and CLIP~\cite{radford2021learning} to navigate outdoor environments based on natural language instructions, combining language and visual cues for optimal path planning. Xu et al.~\cite{xu2023vision} employ VLMs to generate visual-language maps for navigation tasks. RT-1~\cite{rt12022arxiv} and RT-2~\cite{rt22023arxiv} models leverage multimodal robotics data for navigating complex environments and offer greater generalizability through larger dataset training \cite{shah2022lmnav, xu2023vision, rt12022arxiv, rt22023arxiv}. However, limited research has focused on short-horizon path planning for socially aware navigation in real-world settings. We propose a novel approach combining VLMs and vision-perception models for low-level socially compliant motion planning.

% Recent advancements in LFMs~\cite { bommasani2022opportunities} such as VLMs and LLMs have significantly impacted the field of robotic navigation. Despite known challenges like bias, accuracy, and reliability \cite{payandeh2023susceptible} issues associated with LFMs, their adoption has facilitated deeper insights into social interactions and enabled more nuanced, context-aware navigation strategies. Several works have studied the adaptation of these models in robotics for navigation tasks in zero-shot \cite{wu2024voronav} or few-shot settings \cite{chen2023autotamp}. SayCan~\cite{ Ahn2022} integrates LLM for high-level task planning in natural language instructions, complemented by a learned value function to contextualize these instructions within the environment. It leverages pretrained skills to execute feasible and context-specific actions guided by LLMs. GPT-Driver~\cite{ mao2023gptdriver} evaluates the performance of GPT-3.5 in simulation for autonomous driving by formulating motion planning as a language modeling problem. LL3MVN~\cite{ Yu_2023} constructs a semantic map of the environment, selects a long-term goal, and utilizes LLMs to describe the map's content to reach the goal. LLaDA~\cite{ li2024driving} utilizes LLMs to enable autonomous vehicles to adapt to a variety of traffic rules and norms across different geographical regions. LMPC~\cite{liang2024learning} fine-tunes PaLM 2~\cite{anil2023palm} using data collected from in-context learning to enhance future teachability. This approach reduces the reliance on in-context learning for robotics tasks and enhances generalization to unseen embodiments.

% Multimodal foundation models have significantly enhanced navigation capabilities by integrating textual and visual data for more adaptive and flexible strategies. LM-Nav~\cite{shah2022lmnav} leverages GPT-3 and CLIP~\cite{radford2021learning} to navigate real-world outdoor environments based on natural language instructions by constructing a topological graph and planning the optimal path through a combination of language and visual cues. Xu et al.~\cite{xu2023vision} use VLMs to generate a visual-language map of an unseen environment for a language instruction navigation task. RT-1~\cite{brohan2023rt1}, a novel transformer-based robotic control model, learns from extensive multimodal robotics data to navigate complex environments and make informed decisions. RT-2~\cite{brohan2023rt2} extends RT-1 with a Vision-Language-Action framework for direct, closed-loop control, offering greater generalizability through larger dataset training. However, less has been studied on short-horizon path planning using foundation models for real word socially aware navigation. We combine a VLM and a vision-perception model for low-level socially compliant motion planning.


\begin{figure*}[t]
\centering
\includegraphics[width=0.75\linewidth]{figures/pipeline.pdf}
\caption{The overall system architecture of our social navigation framework. Our real-world perception (vision) module detects important social entities (\eg humans, gestures, and doors) in real time and prompts the VLM-based scoring module to compute social cost $C_{social}$, which is used to generate socially compliant robot action. 
}
\label{fig:overview}
\end{figure*} 

\section{Approach}\label{sec:ap}

In this section, we define the social navigation problem and describe our approach in detail. 

\subsection{Problem Definition}

Navigation is the task of following an efficient collision-free path from an initial location to a goal~\cite{mavrogiannis2023core}. In general, the system consists of a global planner and a local planner. A global planner is designed to find a collision-free path to reach a goal, while a local planner aims to navigate the robot through its immediate surroundings, making real-time adjustments to deal with dynamic obstacles. 

For social robot navigation, humans are no longer perceived only as dynamic obstacles but also as social entities~\cite{rios2015proxemics, mirsky2021conflict}. It necessitates integrating social norms into robot behaviors. 
We define the social robot navigation problem as a Markov Decision Process (MDP): $\langle \cs, \ca, \ct, \cc \rangle$ where $\s = (x, y, \theta) \in \cs$ is a state space consisting of a robot pose, $\a = (v, w) \in \ca$ is an action space consisting of a linear and an angular velocity of a robot, $\ct: \cs \times \ca \rightarrow \cs$ is the transition function characterizing the dynamics of the robot, and $\cc: \cs \times \ca \rightarrow \RR $ is a cost function. Given a cost function $\cc$, the motion planner finds $(v^*, w^*)$ that minimizes the expected cost. The cost function takes the following form:
\begin{equation}\label{eq:cost}
\begin{aligned}
    C(\s,\a)=\alpha\cdot C_{goal}+\beta\cdot C_{obst}+\gamma\cdot C_{social}
\end{aligned}
\end{equation}
where $C_{goal}$ encourages movement toward the goal, $C_{obst}$ discourages collisions with obstacles, and $C_{social}$ encourages the robot to follow the social norms. $\alpha$, $\beta$, and $\gamma$ is a non-negative weight for each cost term. 

The social cost term $C_{social}$ encompasses various factors that govern human-robot interactions in shared environments. Defining them mathematically poses challenges. In this paper, we define $C_{social}$ as:
\begin{equation}\label{eq:social}
\begin{aligned}
    C_{social}= \|\cb - \cb_{h} \|
\end{aligned}
\end{equation}
where $\cb$ is a navigation behavior, and $\cb_{h}$ is a navigation behavior humans would adopt in accordance with social conventions. Minimizing the deviation between them will encourage the robot to emulate socially acceptable human behavior. While $\cb_{h}$ can be obtained through various methods, including large datasets~\cite{thorDataset2019, karnan2022scand, nguyenmusohu}, we leverage the power of VLM to provide appropriate behavior based on a rich contextual understanding and nuanced interpretations from the image and the given prompts. We elaborate further in Section~\ref{sec:vlm}.


\subsection{VLM-based Social Navigation Framework}

Figure~\ref{fig:overview} depicts the overview of our approach. Our approach is built upon an autonomous navigation framework comprising a global path planner and a local motion planner. The global path planner generates a high-level trajectory for the robot to follow, considering obstacles and the goal location. The local planner refines this trajectory by considering the sensor inputs and outputs a robot action that minimizes the cost function $C$.

To improve navigation abilities in a social-interaction context, we introduce a VLM-based scoring module. As the robot navigates through the environment, our real-world perception module detects social entities, such as humans, gestures, and doors, in real time. When important social cues are detected, our VLM-based scoring module is prompted and outputs the immediate robot action that is socially acceptable based on the current robot observation. It is then calculated as a cost term to be fed to the motion planner. 
The framework is designed to make real-time decisions on robot actions based on the current observation. The social cost term is integrated only when it is necessary, \ie when there is any human interaction involved. 


%Within our local planner, we integrate a learning-based vision model for object detection, a VLM-based contextual weight module, and a motion planner. Despite the effectiveness of VLM in various vision-language tasks, its integration poses challenges for real-world deployment, primarily due to its latency constraint. Thus, we let state-of-the-art learning-based vision models for real-time object detection that have made significant advancements in terms of both speed and accuracy~\cite{redmon2016you}. It is used to determine when to trigger the VLM, only when contextual inference is needed. For example, in our experiment, we identify humans and our VLM-based contextual weight module discerns the current human-interaction scenario from the robot's current visual input. Subsequently, it assigns weights to the controller's objective cost terms, considering the inferred situation. Optionally, textual instructions to guide robot navigation behavior could be additionally employed. These cost terms contribute to the generation of precise immediate robot actions that consider the current situation.

\subsection{VLM-based Scoring Module} \label{sec:vlm}


%The VLM plays a crucial role in our system, leveraging prompts to translate contextual social cues from the robot's observations into actionable decisions. Inspired by In-Context Learning (ICL)~\cite{min2022rethinking}, our module is designed to leverage the VLM's reasoning abilities through few-shot or zero-shot examples. This approach offers an interpretable interface, mirroring human reasoning and decision-making processes, without necessitating extensive training. In our system, we provide the RGB image of the current robot view to VLM, along with prompts with examples of inputs and outputs. Optionally, supplementary instructions detailing preferred robot behaviors may also be provided.

When individuals navigate social environments, they often achieve their desired goals by adhering to social norms without exerting significant effort. This capability stems from accumulated experiences and learning from an early age, coupled with the human capacity for nonverbal communication. However, in the context of social robot navigation, inferring socially compatible navigation behaviors $B_h$ tailored to specific situations presents a notably crucial and challenging problem.

\begin{equation}\label{eq:vlm}
\begin{aligned}
    \cb_h^t = \text{VLM}(\ci^t, \cp ,\a^t)
\end{aligned}
\end{equation}
$\ci^t$ is an RGB image of the current robot view, $\cp$ is a social norm, $\a^t$ is a previous robot action 

$\cb_h$ is illustrated in a simple 


Figure~\ref{fig:prompt} shows an example prompt used in our experiment. We ask questions about the preferred immediate action, one regarding the heading direction, and another regarding the speed. 

\begin{figure}[tb]
\centering
\begin{GrayBox}
    \hspace{0.1\linewidth}\includegraphics[width=0.8\linewidth]{example-image}\\
    
    \textbf{Task:} \\
    The image depicts your current view while you navigating towards the goal. How will you navigate concerning the person in your view? You will need to follow general walking etiquette. \\

    \textbf{Ego state:} \\
    - heading direction: \textcolor{blue}{straight}\\
    - linear velocity: \textcolor{blue}{0.28}\\

    \textbf{Remember:} \\
    - Move to the right when passing by a person.\\
    - Pass on the left when overtaking a person.\\
    - Do not obstruct others' paths.\\
    - ... \\

    \textbf{Answer Format:} \\
    Move \textcolor{red}{DIRECTION} with \textcolor{red}{SPEED} \\
    - options for DIRECTION: left, straight, right \\
    - options for SPEED: slow down, speed up, constant, stop\\    
    
\end{GrayBox}
\caption{An example prompt used in our approach. 
}
\label{fig:prompt}
\end{figure} 

\section{Experiments}\label{sec:exp}

In this section, we detail the implementation of our method and describe our qualitative and quantitative results including comparisons with other methods and the user study. 

\subsection{Implementation Details}

Our approach is tested on a Turtlebot 2 equipped with a Velodyne VLP16 lidar, a MMlove fisheye lens RGB camera, and a laptop with intel i7 CPU, and an Nvidia GeForce RTX 2080 GPU. 
We use You Only Look Once (YOLO)~\cite{YOLO2023} as our real-world perception model to detect key objects (\eg humans, door, and gesture) and GPT-4V~\cite{gpt4v2023} as our VLM to comprehend the social dynamics and output the immediate preferred robot action. We combine our method with a low-level motion planner DWA~\cite{fox1997dynamic}. We compare our method with DWA without social cost $C_{social}$ and multi-modal fusion method~\cite{panigrahi2023study}, which adopts an imitation learning approach to learn navigation behavior by fusing an RGB image and a point cloud.

To validate our approach, we carefully follow the social robot navigation studies~\cite{francis2023principles, pirk2022protocol} that have set up the benchmark scenarios and the metrics for measuring social navigation. 
We present qualitative, quantitative, and user study results in four different indoor social navigation scenarios:
\begin{itemize}
  \item \textbf{Frontal Approach:} A robot and a human are approaching each other from two ends of a straight trajectory.  
  \item \textbf{Intersection:} A robot and a human cross each other on perpendicular trajectories. 
  \item \textbf{Intersection with Gesture:} A robot and a human cross each other on perpendicular trajectories. The human recognizes the robot and then gestures for it to stop.
  \item \textbf{Narrow Doorway:} A robot and a human cross each other's paths by moving through a narrow doorway. 
\end{itemize}

\begin{figure*}[htb]
\centering
\includegraphics[width=0.35\linewidth]{example-image} 
\includegraphics[width=0.35\linewidth]{example-image} 
\includegraphics[width=0.35\linewidth]{example-image} 
\includegraphics[width=0.35\linewidth]{example-image} 
\caption{Results of the robot motion with our approach. The scenario starts from the top left image and proceeds clockwise: Frontal Approach, Intersection, Intersection with Gesture, and Narrow Doorway.}
\label{fig:qualitative}
\end{figure*} 

\begin{table*}[htb] 
\centering
\caption{Quantitative Reults}
\begin{tabular}{rccccc} 
 \midrule
 \multirow{2}{*}{\textbf{Metric}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{Scenario}} \\
 \cmidrule(lr{0.5em}){3-6}
 & &  Frontal Approach & Intersection & Intersection w/ Gesture & Narrow Doorway \\
 \midrule
 \multirow{3}{*}{Success Rate (\%)} & multi-modal & - & - & - & -\\
 & DWA & - & - & - & -\\
 & Ours & - & - & - & -\\
 \midrule
 \multirow{3}{*}{Collision Rate (\%)} & multi-modal & - & - & - & -\\
 & DWA & - & - & - & -\\
 & Ours & - & - & - & -\\
 \midrule
 \multirow{3}{*}{Clearing distance ($m$)} & multi-modal & - & - & - & -\\
 & DWA & - & - & - & -\\
 & Ours & - & - & - & -\\
 \midrule
\end{tabular}
%}
\label{table:quantitative}
\end{table*} 

\subsection{Qualitative Result}

Based on other studies~\cite{francis2023principles, pirk2022protocol}, the robot is expected to behave in a socially compliant way as follows: 
\begin{itemize}
  \item \textbf{Frontal Approach:} The robot is expected to yield or slow down and modify its original trajectory so as not to obstruct the human path. Similar to driving rules, it is conventional to keep on the right side. 
  \item \textbf{Intersection:} The robot is expected to drive slowly when it approaches the human. It may come to a complete stop or modify its original trajectory to go behind the human to not obstruct the path. 
  \item \textbf{Intersection with Gesture:} The robot is expected to yield by interpreting the human gesture. 
  \item \textbf{Narrow Doorway:} The robot is expected to wait outside the door and yield to the human. 
\end{itemize}

Fig.~\ref{fig:qualitative} shows snapshots of the resulting robot motion using our approach in four different scenarios. We demonstrate that our approach follows the social convention and navigates toward its goal as expected. 
The accompanying video shows the resulting robot motions of our approach, compared with DWA and the multi-modal fusion method. 

\subsection{Quantitative Result}

To further validate our approach, we evaluate the methods using three different 
metrics~\cite {francis2023principles}. The success rate describes whether the robot reaches the goal. The collision rate describes whether the robot collided with the human or other objects in the environment. The clearing distance describes the minimum distance to obstacles in a trajectory. 

Table~\ref{table:quantitative} reports the results averaged over 20 runs for each method and scenario. The results demonstrate that \todo{}.

\subsection{User Study}

To validate the social compliance of our method, we conduct a user study. 
We ask the participants to walk along the predefined trajectory and ask them to answer questionnaires about the robot motion~\cite{pirk2022protocol} (Table~\ref{table:userstudy}). The three methods are randomly shuffled and repeated five times. Each scenario is tested on ten different participants. 
We use a five-level Likert scale to ask participants to rate their agreement toward these statements. 

Fig.~\ref{fig:userstudy} shows the study result. The plot shows the per-question average scores with error bars for the three methods in each scenario. From the result, we can see that \todo{}.

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\linewidth]{example-image} 
\caption{User Study Average Scores}
\label{fig:userstudy}
\end{figure} 

\begin{table}[htb] 
\centering
\caption{Social Compliance Questionnaire}
\begin{tabularx}{\linewidth}{l|l} 
 \midrule
 \multicolumn{2}{l} {\textbf{Scenario 1: Frontal Approach}} \\
 \midrule
 1 & The robot moved to avoid me.\\
 2 & The robot obstructed my path.\\
 3 & The robot maintained a safe and comfortable distance at all times.\\
 4 & The robot nearly collided with me.\\
 5 & It was clear what the robot wanted to do.\\
 \midrule
 \multicolumn{2}{l} {\textbf{Scenario 2: Intersection}} \\ 
 \midrule
 6 & The robot let me cross the intersection by maintaining a safe and \\ & comfortable distance.\\
 7 & The robot changed course to let me pass.\\
 8 & The robot paid attention to what I was doing.\\
 9 & The robot slowed down and stopped to let me pass.\\
 \midrule
  \multicolumn{2}{l} {\textbf{Scenario 3: Intersection with Gesture}} \\ 
 \midrule
 10 & The robot maintained a safe and comfortable distance at all times.\\
 11 & The robot slowed down and stopped.\\
 12 & The robot followed my command\\
 13 & I felt the robot paid attention to what I was doing.\\
 \midrule
 \multicolumn{2}{l} {\textbf{Scenario 4: Narrow Doorway}} \\ 
 \midrule
 14 & The robot got in my way.\\
 15 & The robot moved to avoid me.\\
 16 & The robot made room for me to enter or exit.\\
 17 & It was clear what the robot wanted to do.\\
 \midrule
\end{tabularx}
%}
\label{table:userstudy}
\end{table} 

%\subsection{Discussion}
% Talk about the response time

\section{Conclusion}\label{sec:con}

This not only reduces the reliance on extensive training data but also alleviates the need to define rules or rewards for every possible scenario explicitly, thereby enhancing the adaptability and scalability of the decision-making process in autonomous robot navigation.

