
\begin{abstract}
We propose VLM-Social-Nav, a novel Vision-Language Model (VLM) based navigation approach to compute a robot's motion in human-centered environments. Our goal is to make real-time decisions on robot actions that are socially compliant with human expectations. We utilize a perception model to detect important social entities and prompt a VLM to generate guidance for socially compliant robot behavior. VLM-Social-Nav uses a VLM-based scoring module that computes a cost term that ensures socially appropriate and effective robot actions generated by the underlying planner. Our overall approach reduces reliance on large training datasets and enhances adaptability in decision-making. In practice, it results in improved socially compliant navigation in human-shared environments. We demonstrate and evaluate our system in four different real-world social navigation scenarios with a Turtlebot robot. We observe at least $27.38\%$ improvement in the average success rate and $19.05\%$ improvement in the average collision rate in the four social navigation scenarios. Our user study score shows that VLM-Social-Nav generates the most socially compliant navigation behavior. 

\end{abstract}

\begin{keywords}
Motion and Path Planning, Task and Motion Planning, Integrated Planning and Control
\end{keywords}


\section{Introduction}\label{sec:intro}
% What is social navigation and why is it important
Mobile robots integrated into diverse indoor and outdoor human-centric environments are becoming increasingly prevalent. These robots serve various functions, ranging from package and food delivery~\cite{starship, scout} to service~\cite{dilligent} and home assistance~\cite{astro}. Overall, these roles necessitate interaction with humans and navigating seamlessly through public spaces with pedestrians. In such dynamic scenarios, it is important for the robots to engage in socially compliant interactions and navigation~\cite{xiao2022motion, mirsky2024conflict, mavrogiannis2023core}. 

% challenges in social navigation:
%\textit{Challenges of social navigation:} 
This paper focuses on the challenges of social navigation~\cite{mavrogiannis2023core}. It addresses the ability of robots to navigate while adhering to social etiquette, especially contextual appropriateness, which requires robots to understand environment contexts, current tasks, and interpersonal behaviors. Therefore, navigating socially across varying contexts presents distinct challenges~\cite{francis2023principles, mavrogiannis2023core, mirsky2024conflict}, including ensuring safety, comfort, and politeness, as well as adhering to social norms.

% Contextual Appropriateness: technical challenges from previous works
\textit{Inferring contextually appropriate navigation behaviors is challenging.} Humans have various behaviors and the environmental or task contexts also cannot be easily categorized~\cite{mavrogiannis2023core}. 
A common strategy to handle the challenge is by learning-based approaches to empirically learn the complicated contexts. Imitation Learning (IL) is a recent emerging paradigm for desired navigation behavior~\cite{hirose2023sacson, xiao2020appld, xiao2022learning, panigrahi2023study, raj2024targeted}. This approach enables autonomous robots to navigate socially by learning from human demonstrations. Other learning approaches, such as Reinforcement Learning (RL) have also been used to address this problem~\cite{kretzschmar2016socially}. While both methods demonstrate promising results in real-world settings, substantial datasets~\cite{thorDataset2019, karnan2022scand, nguyenmusohu} for training and reward engineering are required for their successful application and it is hard to generalize. 

% the potential of Language models 
\textit{Language models are inherently well-suited for contextual understanding but not well applied in social navigation.} Recent Large Language Models (LLMs) and Vision-Language Models (VLMs) demonstrate a deep understanding of contextual information and have the potential to perform chain-of-thought~\cite{wei2022chain} and common sense reasoning~\cite{geva2021did, gpt4v2023}. Those processes are inherent to social navigation, especially the challenges of contextual appropriateness and politeness, which require understanding the task/environmental context and the behavior of humans. This capability has also been evaluated across diverse domains of robotics, including human-like driving scenarios~\cite{wen2024road, sha2023languagempc} and autonomous robot navigation~\cite{10160969,shah2022lmnav}. However, using language models for social navigation is not well explored and the language models suffer from high latency for real-time navigation, and the issue impedes the smoothness and efficiency of human-robot social interaction. 
% We leverage the capabilities of VLMs for scene understanding and reasoning to perform decision-making in social navigation, particularly in human-centered environments.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/cover_final.pdf} 
\caption{The trajectories of our approach, VLM-Social-Nav (red), DWA (blue), and BC (yellow) approaches in the frontal encountering scenario (left) and the intersection scenario (right). The resulting trajectories show that VLM-Social-Nav demonstrates more social compliant behavior as it is instructed by a prompt (\ie Move to the right when passing by a person, and do not obstruct others' paths).
} %\vspace{-1.0em}
\label{fig:cover}
\end{figure} 


{\bf Main Results: } In this paper, we present VLM-Social-Nav, a new approach that uses VLMs to interpret contextual information from robot observation to help autonomous robots improve their navigation abilities in human-centered environments. In particular, we leverage a VLM to analyze and reason about the current social interaction, and generate an immediate \textit{preferred robot action} to guide an underlying motion planner. Our VLM-based scoring module computes a \textit{social cost}, which is used for the bottom-level motion planner to output the appropriate robot action. To overcome the limitation of existing VLMs' latency issue, we utilize a state-of-the-art perception model (\ie YOLO~\cite{YOLO2023}) to detect key entities that are used for social interactions (\eg humans, gestures, and doors) and query a VLM to generate socially compliant navigation behavior and compute the social cost. We demonstrate VLM-Social-Nav in four different indoor scenarios with human interactions. Unlike previous social navigation approaches, VLM-Social-Nav can better navigate through social scenarios by interpreting the situation based on \textit{common sense} without any dedicated training on a large dataset. Some of our main results include:
\begin{itemize}
  \item We propose VLM-Social-Nav, a novel approach for social robot navigation by integrating VLMs with optimization-based or scoring-based motion planners and a state-of-the-art perception model for better VLM efficiency. 
  % This enables robots to detect social entities efficiently and make real-time decisions on socially compliant robot behavior. 
  \item We propose a VLM-based scoring module that translates the current robot observation and textual instructions into a relevant social cost term. This cost term is used for the bottom-level motion planner to output appropriate robot action.
  \item We evaluate VLM-Social-Nav in four different real-world indoor social navigation scenarios 
  % We measure the success rate and the collision rate. 
  along with a user study and compare the results with a Dynamic-Window Approach (DWA)~\cite{fox1997dynamic} and Behavior Cloning (BC)~\cite{pomerleau1988alvinn} method trained on a state-of-the-art large Socially CompliAnt Navigation Dataset (SCAND)~\cite{karnan2022scand}. VLM-Social-Nav achieves at least $27.38\%$ improvement in average success rate and $19.05\%$ improvement in average collision rate in the four social navigation scenarios. The user study score shows that VLM-Social-Nav generates the most socially compliant navigation behavior. 
%\todo{experiment result}
  
  %a multi-modal fusion method~\cite{panigrahi2023study}. \todo{experiment result}
\end{itemize}

\section{Related Work}
\label{sec:prev}
In this section, we give an overview of existing works related to safety requirements and different challenges of contextual appropriateness in social robot navigation, and Large Foundation Models (LFMs) for robot navigation. 

\subsection{Safety Requirement of Social Navigation}
For social navigation, safety is a basic requirement for interaction with humans and navigation in dynamic challenging scenarios~\cite{van2010optimal, fox1997dynamic, liang2021vo, liang2021crowd}. DWA~\cite{fox1997dynamic} calculates the collision constraints of the robots' actions and chooses the best feasible action closest to the target as the output. Model Predictive Control (MPC) approaches~\cite{10341869, williams2017information} are also widely used for collision avoidance tasks, which provide smooth trajectories for the robot to follow; learning-based MPC~\cite{williams2017information, xiao2022learning} enhances the performance of the original MPC method by empirically learning the best actions for navigation, which requires a large expert demonstration dataset for training. Velocity-Obstacle (VO)-based approaches are more efficient and can be used to simulate the actions of crowds~\cite{van2010optimal, best2014densesense}, but these approaches don't take into account the uncertainties in the perception output. PRVO~\cite{gopalakrishnan2017prvo} and OFVO~\cite{liang2021vo} handle the uncertainties of perception in motion planning, but those approaches require a hard threshold to set the confidence for planning. To deal with this issue, learning-based methods empirically train the policies by demonstrations~\cite{liang2021crowd, aradi2020survey, sun2021motion}, whereas other methods~\cite{liang2021crowd,aradi2020survey} use reinforcement learning to train the robot in a simulator and implement it in real-world scenarios. However, learning-based approaches require a significant amount of data or on-policy training with realistic simulators to learn the task. Social interactions are highly nuanced and context-dependent. Simulating these interactions accurately requires sophisticated models of human behavior and interaction dynamics, which is not trivial.

\subsection{Contextual Appropriateness of Social Navigation}
Researchers have proposed diverse methodologies to integrate social awareness into mobile robot navigation systems. The development of a comprehensive social navigation system poses inherent complexity, requiring sophisticated perception and reasoning capabilities to navigate environments shared with humans and other robots \cite{mirsky2024conflict}. Establishing an accurate definition of social navigation is paramount, given its potential variation across cultures and platforms. Except for the safety requirement, assessing social compliance presents an additional challenge, contingent on the scenario and platform, and should consider contextual appropriateness%, which includes comfort, legibility, social competency, politeness, understanding other agents, and proactivity 
~\cite{gao2022evaluation, francis2023principles}. 
Various methodologies are employed to address this challenge, with a significant focus on enhancing learning methods through reinforcement learning, learning from demonstration—particularly by analyzing examples of human trajectories or robots operated by humans—and the utilization of simulated datasets \cite{9679193, 5354147, 7539621, li2017role, nazeri2024vanp}. Additionally, various datasets have been collected for this purpose. % \cite{thorDataset2019, karnan2022scand, nguyenmusohu}. 
Socially CompliAnt Navigation Dataset (SCAND)~\cite{karnan2022scand} and Multi-Modal Social Human Navigation Dataset (MuSoHu)~\cite{nguyenmusohu} are two recent large-scale social human navigation datasets in many natural human-inhabited public spaces for robots to learn similar, human-like, socially compliant navigation behaviors. 
Despite extensive research employing diverse approaches with machine learning techniques~\cite{10323465}, VLMs have not been used to solve the social navigation problem, which, however, have shown the potential to successfully analyze the contextual information of the environment.%~\cite{kruse2013human, charalampous2017recent, mavrogiannis2023core}.

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\linewidth]{figures/pipeline.pdf}
\caption{The overall system architecture of VLM-Social-Nav. Our real-world perception (vision) model detects important social entities (\eg humans, gestures, and doors) in real time and prompts the VLM-based scoring module to compute social cost $\cc_\textrm{social}$, which is used to generate socially compliant robot action. 
} \vspace{-1.0em}
\label{fig:overview}
\end{figure*} 

\subsection{Large Foundation Models for Navigation}
Recent advancements in Language Foundation Models (LFMs)~\cite { bommasani2022opportunities}, encompassing Vision-Language Models (VLMs) and Large-Language Models (LLMs), show significant potential in robotic navigation, despite the challenges like bias and reliability~\cite{payandeh2023susceptible} associated with the LFMs. SayCan~\cite{Ahn2022} integrates LLMs for high-level task planning. GPT-Driver~\cite{ mao2023gptdriver} evaluates the performance of GPT-3.5 in simulation for autonomous driving, framing motion planning as a language modeling problem. L3MVN~\cite{yu2023l3mvn} constructs semantic maps of environments and utilizes LLMs to reach long-term goals, while LLaDA~\cite{li2024driving} enables autonomous vehicles to adapt to diverse traffic rules across regions. LM-Nav~\cite{shah2022lmnav} utilizes GPT-3 and CLIP~\cite{radford2021learning} to navigate outdoor environments based on natural language instructions, combining language and visual cues for optimal path planning. 
%RT-1~\cite{brohan2022rt1} and RT-2~\cite{brohan2023rt2} models leverage multimodal robotics data for navigating complex environments and offer greater generalizability through larger dataset training \cite{shah2022lmnav, xu2023vision, brohan2022rt1, brohan2023rt2}. 
Despite their powerful capabilities in contextual understanding and commonsense reasoning, language models have not been extensively investigated for social navigation. Our approach proposes a novel method to navigate robots in a socially compliant manner.

% However, limited research has focused on short-horizon path planning for socially aware navigation in real-world settings. 

% their adoption has led to deeper insights into social interactions and more nuanced, context-aware navigation strategies. Works such as SayCan~\cite{Ahn2022} integrate LLMs for high-level task planning in natural language instructions, leveraging pretrained skills for context-specific actions. GPT-Driver~\cite{ mao2023gptdriver} evaluates the performance of GPT-3.5 in simulation for autonomous driving, framing motion planning as a language modeling problem. LL3MVN~\cite{Yu_2023} constructs semantic maps of environments and utilizes LLMs to reach long-term goals, while LLaDA~\cite{li2024driving} enables autonomous vehicles to adapt to diverse traffic rules across regions. LMPC~\cite{liang2024learning} fine-tunes PaLM 2~\cite{anil2023palm} using in-context learning data to enhance generalization to unseen embodiments. %Xu et al.~\cite{xu2023vision} employ VLMs to generate visual-language maps for navigation tasks. 

% Multimodal foundation models have further enhanced navigation capabilities by integrating textual and visual data for more adaptive strategies. LM-Nav~\cite{shah2022lmnav} utilizes GPT-3 and CLIP~\cite{radford2021learning} to navigate outdoor environments based on natural language instructions, combining language and visual cues for optimal path planning. Xu et al.~\cite{xu2023vision} employ VLMs to generate visual-language maps for navigation tasks. RT-1~\cite{brohan2022rt1} and RT-2~\cite{brohan2023rt2} models leverage multimodal robotics data for navigating complex environments and offer greater generalizability through larger dataset training \cite{shah2022lmnav, xu2023vision, brohan2022rt1, brohan2023rt2}. However, limited research has focused on short-horizon path planning for socially aware navigation in real-world settings. 


\section{Approach}\label{sec:ap}

In this section, we define the social navigation problem and describe VLM-Social-Nav in detail. 

\subsection{Problem Definition}

Navigation is the task of generating and following an efficient collision-free path from an initial location to a goal~\cite{mavrogiannis2023core}. In general, the overall system consists of a global planner and a local planner. A global planner is designed to find a collision-free path to reach a goal, while a local planner aims to navigate the robot through its immediate surroundings, making real-time adjustments to deal with vehicle dynamics and surrounding obstacles. 

For social robot navigation, humans are no longer perceived only as dynamic obstacles but also as social entities~\cite{rios2015proxemics, mirsky2024conflict}. It necessitates integrating social norms into robot behaviors. 
We define the social robot navigation problem as a {\em Markov Decision Process (MDP)}: $\langle \cs, \ca, \ct, \cc \rangle$ where $\s = (x, y, \theta) \in \cs$ is a state consisting of a robot pose, $\a = (v, w) \in \ca$ is an action consisting of a linear and an angular velocity of the robot, $\ct: \cs \times \ca \rightarrow \cs$ is the transition function characterizing the dynamics of the robot, and $\cc: \cs \times \ca \rightarrow \RR $ is a cost function. Given a cost function $\cc$, the motion planner finds $(v^*, w^*)$ that minimizes the expected cost. The cost function takes the following form:
\begin{equation}\label{eq:cost}
\begin{aligned}
    \cc(\s,\a)=\alpha\cdot \cc_\textrm{goal}+\beta\cdot \cc_\textrm{obst}+\gamma\cdot \cc_\textrm{social},
\end{aligned}
\end{equation}
where $\cc_\textrm{goal}$ encourages movement toward the goal, $\cc_\textrm{obst}$ discourages collisions with obstacles, and $\cc_\textrm{social}$ encourages the robot to follow the social norms. $\alpha$, $\beta$, and $\gamma$ are a non-negative weight for each cost term. 

The social cost term $\cc_\textrm{social}$ encompasses various factors that govern human-robot interactions in shared environments. Defining them mathematically poses challenges. For VLM-Social-Nav, we define $\cc_\textrm{social}$ as:
\begin{equation}\label{eq:social}
\begin{aligned}
    \cc_\textrm{social}= \|\cb - \cb_{h} \|,
\end{aligned}
\end{equation}
where $\cb$ is a navigation behavior, and $\cb_{h}$ is a navigation behavior humans would adopt in accordance with social conventions. Minimizing the deviation between them will encourage the robot to emulate socially acceptable human behaviors. While $\cb_{h}$ can be obtained through various methods, including large datasets~\cite{thorDataset2019, karnan2022scand, nguyenmusohu}, we leverage the power of a VLM to compute appropriate behavior based on its rich contextual understanding and nuanced interpretations from perceived images and given prompts. We elaborate further in Section~\ref{sec:vlm}.


\subsection{VLM-based Social Navigation Architecture}

Figure~\ref{fig:overview} highlights the overview of VLM-Social-Nav. Our formulation is built upon an autonomous navigation system comprising a perception layer and an optimization-based motion planner. 
The motion planner considers the sensor inputs and outputs a robot action that minimizes the cost function $C$. 
%The global path planner generates a high-level trajectory for the robot to follow, considering obstacles and the goal location. The local planner refines this trajectory by considering the sensor inputs and outputs a robot action that minimizes the cost function $C$.

While LiDAR detects geometric information useful for obstacle avoidance, RGB images provide contextual details of the current environment. They contain rich information crucial for social navigation. 
To enhance navigation capabilities within social contexts, we propose a VLM-based scoring module. VLMs excel in contextual understanding, interpreting scenes not solely based on visual features but also considering social dynamics. VLMs generate socially appropriate robot actions based on current observations and input instructions. Our VLM-based scoring module then calculates a cost term to be used by the motion planner. 

While VLMs can generate navigation behaviors that comply with social norms, continuously querying large VLMs for new responses is prohibitively computationally expensive for real-time navigation. To address this challenge, we incorporate a real-time perception model. This model identifies social entities such as humans, gestures, and doors as the robot navigates its environment. Our VLM-based scoring module activates only when significant social cues are detected, ensuring that the social cost term is integrated only when necessary, \ie when there is any human interaction involved. This approach reduces the VLM queries and facilitates real-time navigation efficiency for our approach. Algorithm~\ref{alg:vsn} summarizes an overview of our VLM-Social-Nav process. 



\begin{algorithm}[tb]
    \caption{VLM-Social-Nav}\label{alg:vsn}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{RGB image $\ci$, LiDAR point cloud $\cl$, prompt $\cp$, goal position $\p_g$}
    % \Output{robot action $\a = (v, w)$}
    %\KwData{$h$ $\leftarrow$ heights of the surface}
    %\hline
    % \BlankLine
    Initialize robot position $\p_r$\;
    \While{not at goal position $\p_r \neq \p_g$}{
      $\ci \gets$ Read image sensor data\;
      $\cl \gets$ Read LiDAR sensor data\;
      $e \gets \text{Perception Model}(\ci)$\;
      $(\delta_s, \delta_d) \gets 0$\;
      \uIf{social entities detected in $e$}{
        $\cb_h = \text{VLM}(\ci, \cp ,\a)$\;
        $\cb_h \mapsto (\delta_s, \delta_d)$\;
      }
      \ForAll{possible actions $\a$}{
        $\cc_\textrm{social} = \text{VLM-based scoring}(\delta_s, \delta_d)$\;
        Calculate the total cost $\cc = \alpha\cdot \cc_\textrm{goal}+\beta\cdot \cc_\textrm{obst}+\gamma\cdot \cc_\textrm{social}$\;
      }
      Find action $\a$ with minimal cost $\cc$ and execute\;
      Update robot position $\p_r$\;
    }
\end{algorithm} %\vspace{-1.0em}


\begin{figure}[tb]
\begin{GrayBox}      
    \footnotesize{\textbf{Input Image:} \vspace{0.5em} \\ }
    %\hspace{0.2\linewidth}
    \includegraphics[width=0.65\linewidth]{figures/frames_65.jpg} \\ \\
    \footnotesize{\textbf{Input Prompt:} \vspace{0.5em}} \\
    \footnotesize{
    \textit{Task:} \\
    How will you navigate concerning the person in your view? You will need to follow general walking etiquette. \vspace{0.5em} 
    
    \textit{Ego state:} \\
    - heading direction: \textcolor{blue}{straight}\\
    - linear velocity: \textcolor{blue}{0.28} \vspace{0.5em}

    \textit{Remember:} \\
    - Move to the right when passing by a person.\\
    - Do not obstruct others' paths.\\
    % - Pass on the left when overtaking a person.\\
    - ... \vspace{0.5em}

    \textit{Answer Format:} \\
    Move \textcolor{red}{DIRECTION} with \textcolor{red}{SPEED} \\
    - options for DIRECTION: left, straight, right \\
    - options for SPEED: slow down, speed up, constant, stop
    }
\end{GrayBox}
\caption{An example input image ($\ci^t$) and prompt ($\cp$) used in VLM-Social-Nav. Parameterized inputs ($\a^t$) are highlighted in blue. Formatted output specifying the heading ($\delta_d$) and the speed ($\delta_s$) are highlighted in red. The example input data is one of the frontal approach scenarios from MuSoHu~\cite{nguyenmusohu}. The output of VLM is \textit{Move right with slow down}, which was the same action the collected data took. 
} \vspace{-1.0em}
\label{fig:prompt}
\end{figure} 

\subsection{VLM-based Scoring Module} \label{sec:vlm}


VLM plays a crucial role in VLM-Social-Nav to infer immediate socially compatible navigation behavior $\cb_h^{t+1}$ based on its pre-trained large internet-scale dataset:
\begin{equation}\label{eq:vlm}
\begin{aligned}
    \cb_h^{t+1} = \text{VLM}(\ci^t, \cp ,\a^t) ,
\end{aligned}
\end{equation}
where $\ci^t$ is an RGB image from the robot view at time $t$, $\cp$ is a textual prompt, $\a^t$ is a current robot action at time $t$. Inspired by In-Context Learning (ICL), our prompt $\cp$ is designed to leverage the VLM's reasoning abilities through zero-shot examples. This approach offers an interpretable interface, mirroring human reasoning and decision-making processes, without extensive training~\cite{min2022rethinking}.

Our VLM-based scoring module starts from the insight that the action space of a mobile robot can be readily mapped to linguistic terms. For example, the action ``move forward at a constant speed'' can be linked to a linear velocity of $v^{t}$ m/s and an angular velocity of 0. %, where $v^{t}$ represents the current linear velocity. 
The heading direction on the left indicates a positive value of $w^{t}$, while the direction on the right indicates a negative value. Leveraging this understanding, we structure the output of the VLM into a linguistic format comprising the heading and the speed. Subsequently, our scoring module extracts $\cb_h^{t+1} \mapsto (v_h^{t+1}, w_h^{t+1}) \in \ca$ from these tokens; $v_h^{t+1} = v^t + \delta_{s}$, where $\delta_{s}$ is derived from the response for the speed; $w_h^{t+1} = \delta_{d}$, where $\delta_{d}$ is derived from the response for the heading. 
Thus, the social cost term for the next time step can be calculated:
\begin{equation}\label{eq:social_vlm}
\begin{aligned}
    \cc_\textrm{social}^{t+1}= w_{l} \cdot \| v - v_h^{t+1} \| + w_{a} \cdot \| w - w_h^{t+1} \|,
\end{aligned}
\end{equation}
where $w_{l}$ and $w_{a}$ are non-negative weight values. Given all the cost terms, our low-level optimization-based motion planner finds the robot action $(v^*, w^*)$ that minimizes the cost. 

Figure~\ref{fig:prompt} shows an example prompt $\cp$ used in our experiment. We provide a high-level task description along with an image $\ci^t$ captured from the robot's perspective. Furthermore, the current robot action $\a^t=(v^t,w^t) \in \ca$ is provided. %, which will be employed in calculating $\cc_\textrm{social}$. 
Supplementary instructions regarding walking etiquette are also included. Although the VLM demonstrates proficient navigation abilities even in the absence of explicit instructions, offering reasoning guidelines enhances its decision-making processes~\cite{min2022rethinking}. These guidelines not only facilitate comprehensive reasoning and judgment within the VLM but also enable the robot to adapt to specific rules more effectively. For example, in a country where it's customary to walk on the left, we can rephrase the prompt as ``Move to the left when passing by another person''. 




% $\cb_h \rightarrow \ca$
% We map \cb^{t} \rightarrow \ca

\begin{figure*}[htb]
\centering
\begin{tabular}{rc}
\spheading{(a) Frontal Approach} \hspace{-0.6em} & \includegraphics[width=0.82\linewidth]{figures/result/result_frontal.pdf} \\
\spheading{(b) Frontal Approach with Gesture}  \hspace{-0.6em} & \includegraphics[width=0.82\linewidth]{figures/result/result_gesture.pdf} \\
\spheading{(c) Intersection} \hspace{-0.6em}  &  \includegraphics[width=0.82\linewidth]{figures/result/result_intersection.pdf} \\
\spheading{(d) Narrow Doorway} \hspace{-0.6em} &  \includegraphics[width=0.82\linewidth]{figures/result/result_door.pdf} \\
\end{tabular}
\caption{Qualitative Results: the robot navgation behaviors with VLM-Social-Nav for four social navigation scenarios: (a) Frontal Approach, (b) Frontal Approach with Gesture, (c) Intersection, and (d) Narrow Doorway. The gray solid arrow shows the participant's path. The red solid arrow shows the robot's path. The red and gray dashed arrows show the robot's and participant's path respectively after a stop motion. A caption on the top left shows the result from the VLM.} \vspace{0.4em}
\label{fig:qualitative}
\end{figure*}  

\begin{table*}[htb] 
\centering
\caption{Quantitative Results: performance comparisons using BC~\cite{pomerleau1988alvinn} trained on SCAND dataset~\cite{karnan2022scand}, DWA~\cite{fox1997dynamic}, and VLM-Social-Nav. VLM-Social-Nav achieves at least $27.38\%$ improvement in average success rate and $19.05\%$ improvement in average collision rate in the four social navigation scenarios. The user study score shows that VLM-Social-Nav generates the most socially compliant navigation behavior. }
\begin{tabular}{rccccc} 
 \midrule
 \multirow{2}{*}{\textbf{Metric}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{Scenario}} \\
 \cmidrule(lr{0.5em}){3-6}
 & &  (a) Frontal Approach & (b) Frontal Approach  w/ Gesture & (c) Intersection & (d) Narrow Doorway \\
 \midrule
 \multirow{3}{*}{Success Rate (\%)} & BC & 38.10 & 0 & 33.33 & 42.86\\
 & DWA & \textbf{100} & \textbf{0} & 90.48 & \textbf{100}\\
 & VLM-Social-Nav & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100}\\
 \midrule
 \multirow{3}{*}{Collision Rate (\%)} & BC & 42.86 & 66.67 & 28.57 & 38.10\\
 & DWA & 28.57 & 19.05 & 19.05 & 38.10 \\
 & VLM-Social-Nav & \textbf{14.29} & \textbf{0} & \textbf{4.76} & \textbf{9.52} \\
 \midrule
 \multirow{3}{*}{User Study Score} & BC & 2.80 $\pm$ 1.45 & 2.23 $\pm$ 1.54 & 2.80 $\pm$ 1.40 & 2.60 $\pm$ 1.33\\
 & DWA & 3.99 $\pm$ 0.80 & 3.38 $\pm$ 0.64 & 3.57 $\pm$ 0.62 & 3.59 $\pm$ 0.83\\
 & VLM-Social-Nav & \textbf{4.31 $\pm$ 0.72} & \textbf{4.28 $\pm$ 0.56} & \textbf{4.35 $\pm$ 0.70} & \textbf{4.04 $\pm$ 0.74}\\
 \midrule
\end{tabular}
\label{table:quantitative}
\end{table*} 



\section{Experiments}\label{sec:exp}

In this section, we detail the implementation of VLM-Social-Nav and describe our qualitative and quantitative results including comparisons with other methods and the user study. 

\subsection{Implementation Details}

VLM-Social-Nav is tested on a Turtlebot 2 equipped with a Velodyne VLP16 LiDAR, and a Zed 2i camera, and a laptop with intel i7 CPU and an Nvidia GeForce RTX 2080 GPU. 
We use YOLO~\cite{YOLO2023} as our real-world perception model to detect key objects (\eg humans, door, and gesture) and Generative Pre-trained Transformer 4 with Vision (GPT-4V)~\cite{gpt4v2023} as our VLM to comprehend the social dynamics and output the immediate preferred robot action. 
We combined our method with a low-level motion planner DWA~\cite{fox1997dynamic}. We compare VLM-Social-Nav with DWA without social cost $\cc_\textrm{social}$ and BC~\cite{pomerleau1988alvinn} method trained on a state-of-the-art, large-scale social navigation dataset, SCAND~\cite{karnan2022scand}. The dataset contains various examples of socially compliant navigation behaviors teleoperated by humans including sticking to the right of the road, waiting for a human to pass, and following a crowd.

%multi-modal fusion method~\cite{panigrahi2023study}, which adopts an imitation learning approach to learn navigation behavior by fusing an RGB image and a point cloud.

To validate VLM-Social-Nav, we carefully follow the social robot navigation studies~\cite{francis2023principles, pirk2022protocol} that have set up the benchmark scenarios and the metrics for measuring social compliance. 
We present qualitative, quantitative, and user study results in four different indoor social navigation scenarios:
\begin{itemize}
  \item \textbf{Frontal Approach:} A robot and a human approach each other from two ends of a straight trajectory.  
  \item \textbf{Frontal Approach with Gesture:} A robot and a human approach each other from two ends of a straight trajectory. The human recognizes the robot and then gestures for it to stop.
  \item \textbf{Intersection:} A robot and a human cross each other on perpendicular trajectories. 
  \item \textbf{Narrow Doorway:} A robot and a human cross each other's paths by moving through a narrow doorway. 
\end{itemize}

\subsection{Qualitative Result}

Based on the protocols and principles set by other studies~\cite{francis2023principles, pirk2022protocol}, the robot is expected to behave in a socially compliant way as follows: 
\begin{itemize}
  \item \textbf{Frontal Approach:} The robot is expected to yield or slow down and modify its original trajectory not to obstruct the human path. Similar to driving rules in North America, it is conventional to keep on the right. 
  \item \textbf{Frontal Approach with Gesture:} The robot is expected to yield by interpreting the human gesture. 
  \item \textbf{Intersection:} The robot is expected to drive slowly when it approaches the human. It may come to a complete stop or modify its original trajectory to go behind the human to not obstruct the path. 
  \item \textbf{Narrow Doorway:} The robot is expected to wait outside the door and yield to the human. 
\end{itemize}

Fig.~\ref{fig:qualitative} shows snapshots of the resulting robot motion using VLM-Social-Nav in four different scenarios. We demonstrate that VLM-Social-Nav follows the social convention and navigates toward its goal as expected. 
Fig.~\ref{fig:cover} illustrates the resulting trajectories of VLM-Social-Nav in comparison to those of DWA and BC methods. A notable observation is that while DWA also effectively avoids collisions with individuals, VLM-Social-Nav generates trajectories that align more closely with social norms. For instance, in the frontal approach scenario, while DWA tends to maneuver around the person either to the right or left, VLM-Social-Nav predominantly bypasses them on the right side. Similarly, in the intersection scenario, whereas DWA occasionally obstructs the person's path by veering to avoid collision directly in front, VLM-Social-Nav endeavors to pass behind the individual. Additionally, BC successfully avoids humans but fails to recover and follow the original path. This leads to many failures in reaching the goal. 
The accompanying supplementary video shows the resulting robot motions. 


\begin{table}[htb] 
\centering
\caption{Social Compliance Questionnaire}
\begin{tabularx}{\linewidth}{l|l} 
 \midrule
 \multicolumn{2}{l} {\textbf{Scenario 1: Frontal Approach}} \\
 \midrule
 1 & The robot moved to avoid me.\\
 2 & The robot obstructed my path.$^{*}$\\
 3 & The robot maintained a safe and comfortable distance at all times.\\
 4 & The robot nearly collided with me.$^{*}$\\
 5 & It was clear what the robot wanted to do.\\
 \midrule
  \multicolumn{2}{l} {\textbf{Scenario 2: Frontal Approach with Gesture}} \\ 
 \midrule
 6 & The robot maintained a safe and comfortable distance at all times.\\
 7 & The robot slowed down and stopped.\\
 8 & The robot followed my command\\
 9 & I felt the robot paid attention to what I was doing.\\
 \midrule
 \multicolumn{2}{l} {\textbf{Scenario 3: Intersection}} \\ 
 \midrule
 10 & The robot let me cross the intersection by maintaining a safe and \\ & comfortable distance.\\
 11 & The robot changed course to let me pass.\\
 12 & The robot paid attention to what I was doing.\\
 13 & The robot slowed down and stopped to let me pass.\\
 \midrule
 \multicolumn{2}{l} {\textbf{Scenario 4: Narrow Doorway}} \\ 
 \midrule
 14 & The robot got in my way.$^{*}$\\
 15 & The robot moved to avoid me.\\
 16 & The robot made room for me to enter or exit.\\
 17 & It was clear what the robot wanted to do.\\
 \midrule
\end{tabularx} 
%}
\label{table:userstudy}
\end{table} 


\subsection{Quantitative Result}

To further validate VLM-Social-Nav, we evaluate the methods using three different 
metrics~\cite {francis2023principles}. The success rate describes whether the robot reaches the goal. For the frontal approach with gesture scenario, we mark it as successful when the robot reacts to the gesture. The collision rate describes whether the robot collided with the human or other objects in the environment. We also mark it as collision when we manually intervene to avoid an imminent collision with the human subject or surroundings. The user study score is an average score we obtained from the user study detailed in Section~\ref{sec:userstudy}.

Table~\ref{table:quantitative} reports the results averaged over 21 runs for each method and scenario. The results demonstrate that VLM-Social-Nav, DWA with social cost, outperforms other methods in every metric. DWA excels at following a path smoothly, yet it faces challenges in collision avoidance as it relies solely on the LiDAR sensor and it does not consider socially compliance. Most of the collisions occurred when DWA navigated in a way that interfered with a person's path, for example, going in front of the person when intersecting.
%This occurs 
%when the participant walks too quickly and the robot attempts to make an abrupt change in motion. 
We also observe that the outcomes of BC varied. At times, when attempting to avoid collisions, it failed to return to its original path and failed to reach the goal. Conversely, there were instances where it didn't attempt collision avoidance at all, resulting in collisions with the participants. For gesture recognition, only our proposed method successfully responded to the participants' gestures. In total, VLM-Social-Nav achieves at least $27.38\%$ improvement in the average success rate and $19.05\%$ improvement in the average collision rate in the four social navigation scenarios.


\subsection{User Study}\label{sec:userstudy}

To validate the social compliance of VLM-Social-Nav, we conduct a user study. 
We ask the participants to walk along the predefined trajectory and ask them to answer questionnaires about the robot motion~\cite{pirk2022protocol} (Table~\ref{table:userstudy}). * denotes negatively formulated questions, for which we reverse-code the ratings to ensure comparability with the positively formulated ones. The three methods are randomly shuffled and repeated three times. Each scenario is tested on seven participants. 
We use a five-level Likert scale to ask participants to rate their agreement toward these statements. 

Fig.~\ref{fig:userstudy} and the user study scores on Table~\ref{table:quantitative} show the study result. The plot shows the per-question average scores for the three methods in each scenario. 
Based on the results, it's evident that VLM-Social-Nav receives the highest level of agreement from participants across all questions, indicating its strong adherence to social norms. The standard error of the BC method was large, indicating that the performance of the BC method was not consistent. The score difference between VLM-Social-Nav and DWA was not large in the narrow doorway scenario. This is because, when attempting to enter the narrow doorway, the robot motion results from DWA indicated that it failed to find a plan and froze, resembling almost a complete stop as the result of VLM-Social-Nav. 
%From the result, we can see that \todo{}.


\begin{figure}[t]
\centering
\includegraphics[width=0.49\linewidth]{figures/plot/frontal.pdf}
\includegraphics[width=0.49\linewidth]{figures/plot/gesture.pdf}
\includegraphics[width=0.49\linewidth]{figures/plot/intersection.pdf}
\includegraphics[width=0.49\linewidth]{figures/plot/door.pdf}
\includegraphics[width=0.35\linewidth]{figures/plot/legend.pdf}
\caption{User Study Average Scores: the per-question average scores for the three methods in each scenario. The results indicate that VLM-Social-Nav (Ours) garnered the highest level of agreement from participants across all questions, highlighting its robust alignment with social norms.} \vspace{-1.0em}
\label{fig:userstudy}
\end{figure} 

\subsection{Discussion} \label{sec:discussion}
%From our experiments, we conclude that integrating VLM into robot navigation helped improve the robot navigation behavior in a socially compliant manner. We would like to discuss further regarding our design choices.

\textbf{Real-time navigation with VLM: }
GPT-4 and similar large VLMs require several seconds to respond to prompts, making continuous querying impractical for real-time navigation tasks. To address this, we optimized VLM-Social-Nav in two ways: first, by formatting prompts and providing predefined choices, which resulted in reduced response times. Second, we minimize queries by using a perception model to detect social cues, allowing for timely VLM queries only when necessary. These choices enable and allow average response times of 2-3 seconds, sufficient for human interaction and navigation. While such a limitation can be problematic in more dynamic scenarios that require frequent interactions, future advancements in fast large language models promise further extensions of our approach.
% The large VLMs like GPT4-V require several seconds to respond to the prompt. Thus, continuously querying VLMs for making an instant decision could be infeasible for real-time navigation task. We made multiple design choices to make VLM-Social-Nav feasible for real-time navigation. First, by having the VLM select from multiple choices and respond in a specific answer format, we were able to reduce its response time compared to when it answers without a given format and choices. Second, we query VLM only it is necessary by utilizing the perception model to detect social cues. This not only reduces the frequency of querying VLM but also enables our approach to query VLM in advance. We can adjust the timing with the size of the bounding box detected by the perception model.
% Due to these design choices, our approach was able to receive answers from the VLM within an average of 2-3 seconds, which was sufficient for interacting with humans and navigating. Although, at present, this may not be suitable for dense crowd scenarios or more dynamic situations, as the performance of large language models grows, our approach can be further extended.

\textbf{Socially aware navigation with VLM: } 
We observe that VLMs can analyze and reason about social interactions from single images. Using various single images, including those collected by ourselves and from social robot navigation datasets~\cite{nguyenmusohu,karnan2022scand}, VLMs accurately describe scenes and suggest socially compliant navigation strategies with reasons. For instance, with the image shown in Fig.~\ref{fig:prompt}, GPT-4V describes the scene as \textit{a person is walking towards the camera along a sidewalk}. To navigate this situation, GPT-4V advises to \textit{yield the right of way} because \textit{it is generally customary to keep to the right side of the path when encountering someone coming from the opposite direction, similar to driving rules}.
However, despite their powerful capabilities, VLMs can still make mistakes. Therefore, it is not yet safe to rely solely on VLMs for navigation decisions. Instead, we incorporate their output as a cost term in our overall decision-making process.
%Additionally, utilizing multiple sequential images to gain a better understanding of human trajectories could enhance the model's accuracy. However, this approach will only become feasible when the computation time for large language models is significantly improved.

\textbf{A single action output of VLM: }
Our approach begins with the observation that the robot action can be effectively translated into linguistic terms. VLMs still face challenges in accurate spatial understanding. Mapping spatial information from images to a textual format for communicating with VLMs is another challenging research problem, unless we use a fine-tuned foundation model. Therefore, adopting an approach where robot actions can be easily translated into textual output offers a straightforward yet robust solution that effectively mitigates VLM hallucination. However, this may have the drawback of not being able to plan long-range trajectories ahead. This is our immediate interest for future work; to extend our approach to planning longer-range trajectories for outdoor navigation.


\textbf{More challenging scenarios: } 
Although our robot experiments were conducted only indoors, according to the example in Fig.~\ref{fig:prompt}, VLM-Social-Nav can be extended to outdoor scenarios in more complex environments. VLMs successfully retrieve environment information  that is significant for outdoor social robot navigation, such as sidewalks, zebra crossings, and cars. This will be our immediate focus for future work, \emph{i.e.}, to advance our work into global outdoor navigation. 
We also aim to extend our approach to complex scenarios involving multiple individuals. When tested with the scenario with a group of people, VLM-Social-Nav successfully outputs socially compatible actions, realizing the group of people in the scene. However, when multiple groups are present, simple directions like left or right may not suffice to describe effective robot navigation. We plan to explore multiple trajectory generation methods~\cite{liang2023mtg, liang2024dtg} and selection techniques using visual prompts~\cite{nasiriany2024pivot, liu2024moka} to enhance interaction precision.



\section{Conclusion}\label{sec:con}
We propose a novel social navigation approach based on VLMs, focusing on real-time, socially compliant decision-making in human-centric environments. We utilize the perception model to detect important social entities and prompt a VLM to generate guidance for socially compliant behavior. VLM-Social-Nav features a VLM-based scoring that ensures socially appropriate and effective robot actions. This minimizes the dependence on extensive training datasets and eliminates the necessity for explicit rules or rewards typically associated with reinforcement learning methods. By furnishing textual instructions to VLM, we can instruct the robot to adhere to specific navigation rules, such as navigating on the right or left according to cultural norms. %Consequently, our approach significantly boosts the adaptability and scalability of decision-making in autonomous social robot navigation. 
In practice, it results in improved socially compliant navigation in human-shared environments. We demonstrate and evaluate our system in four different real-world social navigation scenarios with a Turtlebot robot.

%The current version of VLM-Social-Nav faces certain limitations, as discussed in Section~\ref{sec:discussion}. The latency of large VLMs can impact real-time navigation performance, restricting our approach in dynamic scenarios that demand instantaneous interactions. However, these limitations can potentially be mitigated with future advancements in large VLMs. While our decision to output a single robot action in a textual format has benefits, it may constrain our approach in more complex scenarios requiring precise robot trajectories. We plan to address this issue and aim to explore more on providing VLMs with better navigation options beyond simple direction and speed adjustments. 


%Although our robot experiments were conducted only indoors, according to the example in Fig.~\ref{fig:prompt}, VLM-Social-Nav can be extended to outdoor scenarios. 
% Our immediate focus will be on advancing our work into a global outdoor navigation system. 
% Currently, VLM-Social-Nav is taking certain social entities to query VLM. To make it work on more general cases, when or where to query VLM will be a critical consideration. 
% Our experiments revealed that VLM excels when presented with choices rather than being tasked with making singular, decisive actions. We aim to explore more on the methods to provide VLM with better navigation options beyond simple direction and speed adjustments. 


% Our approach uses a VLM-based scoring module that computes a cost term that ensures socially appropriate and effective robot actions generated by the underlying planner. 
% This not only reduces the reliance on extensive training data but also alleviates the need to define rules or rewards for every possible scenario explicitly, thereby enhancing the adaptability and scalability of the decision-making process in autonomous robot navigation. 
% Our overall approach reduces reliance on large datasets (for training) and enhances adaptability in decision-making. 
% In practice, it results in improved socially compliant navigation in human-shared environments. We demonstrate and evaluate our system in four different real-world social navigation scenarios with a Turtlebot robot.

