\appendix

\begin{figure*}[t]
\centering
\vspace{-10pt}
\includegraphics[width=1\textwidth]{figs/ParadigmComparison.pdf}
\vspace{-10pt}
\caption{Overview of the knowledge model paradigm (left) and the retrieval-and-integration paradigm (right). The knowledge model paradigm pretrains LMs with specially designed objectives, and then further finetunes them to adapt to downstream tasks for prediction. The retrieval-and-integration paradigm retrieves relevant subgraphs of the story context and then makes predictions according to the retrieved subgraphs.}
\vspace{-10pt}
\label{fig:paradigm}
\end{figure*}


\section{Detailed experimental results}
\label{sec:appendix_detail_results}
We present the detail performance comparison for SCT-v1.0 and SCT-v1.5 (in Table \ref{tab:supplementary_sct}), as well as MCNC (in Table \ref{tab:supplementary_mcnc}).
Performance of the significant baselines in the corresponding tasks is presented.

\begin{table}[h]
\centering
\small
\scalebox{0.99}{
\begin{tabular}{l|cc}
\hline
\textbf{Method} & \textbf{SCT-v1.0} & \textbf{SCT-v1.5} \\
\hline
Random & 50.00 & 50.00 \\
\cite{chaturvedi2017story} & 77.60 & - \\
\cite{mostafazadeh2016corpus} & 58.50 & - \\
\cite{srinivasan2018simple} & 76.50 & - \\
\cite{yu2020cocolm} & 97.70 & - \\
\cite{zhou2022eventbert} & - & 91.33 \\
\cite{zhou2022claret} & - & 91.18 \\
\cite{li2019story} & 91.80 & 90.30 \\
\hline
RoBERTa-base  & 92.75\small{$\pm$0.24} & 87.14\small{$\pm$0.39} \\
RoBERTa-large  & 96.74\small{$\pm$0.08} & 92.34\small{$\pm$0.06} \\
DeBERTa-large & 98.13\small{$\pm$0.34} & 94.67\small{$\pm$0.25}  \\
\hline
\methodname\tiny{-RB} &  93.30\small{$\pm$0.11} & 87.65\small{$\pm$0.13} \\
\methodname\tiny{-RL} &    97.10\small{$\pm$0.13} & 92.86\small{$\pm$0.05} \\
\methodname\tiny{-DL} &    98.29\small{$\pm$0.16} & 95.01\small{$\pm$0.32} \\
\hline
\end{tabular}
}
\caption{Results on SCT v1.0 and v1.5. Numbers are the mean and standard deviation of accuracy (\%) over three runs.}
\label{tab:supplementary_sct}
\end{table}


\begin{table}[h]
\centering
\scalebox{0.93}{
\begin{tabular}{l|c}
\hline
\textbf{Method} & \textbf{MCNC}  \\
\hline
Random & 20.00 \\
\cite{chambers2008unsupervised} & 30.52 \\
\cite{granroth2016happens} & 49.57 \\
\cite{li2018constructing} & 52.45 \\
\cite{ding2019event} & 56.03 \\
\cite{lv2020integrating} & 58.66 \\
\cite{zhou2021modeling} & 63.62 \\
\cite{zhou2022eventbert} &  63.50\\
\cite{lee2020weakly} &           63.59 \\
\cite{lee2019multi} &           63.67 \\
\cite{zhou2022claret} & 64.61 \\
\hline
RoBERTa-base  & 61.28\small{$\pm$0.14} \\
RoBERTa-large  &  63.01\small{$\pm$0.12}\\
DeBERTa-large & 65.67\small{$\pm$0.13} \\
\hline
\methodname\tiny{-RB} &  62.11\small{$\pm$0.07} \\
\methodname\tiny{-RL} &   63.96\small{$\pm$0.15}  \\
\methodname\tiny{-DL} &    66.05\small{$\pm$0.12} \\
\hline
\end{tabular}
}
\caption{Results on MCNC. Numbers are the mean and standard deviation of accuracy (\%) over three runs.}
\label{tab:supplementary_mcnc}
\end{table}



\section{Results and statistics of event extraction and grounding}
Table \ref{tab:statistics_event_grounding} shows the detailed statistics of the event grounding and subgraph retrieval stage.
It is clear that our proposed event extraction, normalization and multi-level extraction method help alleviate the event sparsity to a large extent.
This not only reflects on the hit rate and mean L-2 distance during event grounding stage, but also in their retrieved graphs statistics.

Table \ref{tab:statistics_event_matching} shows the performance comparison between semantic similarity based matching (which we used) and the token-level similarity matching.
It is clear from the table that the token-level based similarity matching, such as tf-idf, fails to perform as good as the semantic based matching.
\label{sec:appendix_grounding}

Note that, the information extraction here is fundamentally different from the entity-centric line of work \cite{cui2021refining, cui2021incorporating, chen2022learning}, as our setting involves decomposition and semantic similarity computations over text snippets.
\begin{table}[h]
\centering
\scalebox{0.85}{
\begin{tabular}{l|cc}
\hline
& RoBERTa & BERT \\
\hline
Baseline (w/o know.) & 92.75\small{$\pm$0.24} & 83.63\small{$\pm$1.16} \\
Token-level similarity (tf-idf) & 92.84\small{$\pm$0.27} & 84.27\small{$\pm$0.73} \\
Semantic similarity (SBERT) & 93.30\small{$\pm$0.11} & 85.78\small{$\pm$0.74} \\
\hline
\end{tabular}
}
\caption{Performance comparison between baseline, token-level similarity based event matching, and semantic similarity based event matching. }
\label{tab:statistics_event_matching}
\end{table}

\begin{table*}[h]
\centering
\begin{tabular}{l|cc|cccc}
\hline
& \multicolumn{2}{c|}{\textbf{Event grounding}} & \multicolumn{4}{c}{\textbf{Subgraph retrieval}} \\
& hit rate (\%) & mean L2 distance $\bar d$ & \small $\overline{|\mathcal{V}_{sub}|}$ & \small $\overline{|\mathcal{E}_{sub}|}$ & \small $\overline{|\mathcal{V}_{joint}|}$ & \small $\overline{|\mathcal{E}_{joint}|}$ \\
\hline
w/o extract. & 1.43 & 0.9566 & 0.1235 & 0.1951 & 5.12 & 8.35  \\
\hline
\multirow{2}{*}{w/o PIE} & 88.28 & 0.3853 & 13.37 & 36.33 & 21.60 & 67.17 \\
                    & {\color{gray}12.50} & {\color{gray}0.8351} & & & & \\
\multirow{2}{*}{- ARGM} & 93.22 & 0.2819 & 22.34 & 74.12 & 30.53 & 109.64  \\
                    & {\color{gray}21.43} & {\color{gray}0.7801} & & & & \\
\multirow{2}{*}{- ARG2,3,4} & 94.38 & 0.1818 & 28.03 & 93.94 & 36.20 & 134.09  \\
                    & {\color{gray}45.44} & {\color{gray}0.6477} & & & & \\
\multirow{2}{*}{- ARG1} & 97.12 & 0.1150 & 63.27 & 281.32 & 71.41 & 330.73   \\
                    & {\color{gray}41.97} & {\color{gray}0.6968} & & & & \\
\hline
\end{tabular}
\caption{Results and statistics of event grounding and subgraph retrieval. The {\color{gray} gray numbers} are the statistics for ``w/o norm.'' experiments. }
\label{tab:statistics_event_grounding}
\end{table*}

% Different event matching performance curves to threshold
\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{figs/Precision-threshold.png}
\caption{The Precision to threshold curves. }
\label{fig:ablation_precision}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{figs/Recall-threshold.png}
\caption{The Recall to threshold curves. }
\label{fig:ablation_recall}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{figs/PR-curve.png}
\caption{The Precision-Recall curve.}
\label{fig:ablation_pr}
\end{figure}




\section{Supplementary case studies}
Apart from the case study provided in Section \ref{sec:case}, we additionally provide another two examples in Figure \ref{fig:appendix_case_1} and \ref{fig:appendix_case_2}.

\label{sec:appendix_case}
\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{figs/case_study_appendix_1.pdf}
\caption{Supplementary case 1.}
\label{fig:appendix_case_1}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{figs/case_study_appendix_2.pdf}
\caption{Supplementary case 2.}
\label{fig:appendix_case_2}
\end{figure*}


\section{Annotation details}
We show the annotation interface presented to the expert annotators in \ref{fig:ablation_annotation}. 
Users are prompted to compare the event and its matched anchor, and then to give an evaluation of the quality (Successful-1 or Not-0).
Since the annotation requires domain-specific knowledge, we recruited 3 student researchers within our area who volunteered to help us conduct the evaluation. 
The payment to annotators is higher than the local minimum wage.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figs/annotation_interface.jpg}
\caption{Annotation interface in command line.}
\label{fig:ablation_annotation}
\end{figure}


\section{Obtaining ChatGPT Performance}
\begin{table}[!t]
% \vspace{-0.3cm}
\small
\centering
\begin{tabular}{l c c}
\hline
\multicolumn{1}{c}{\textbf{Model}}
& \textbf{SCT-v1.0 (\%)} & \textbf{SCT-v1.5 (\%)}\\
\hline
Random                          & 50.00 & 50.00\\
ChatGPT$_\text{Prompt}$         & 77.80 & 77.00 \\
ChatGPT$_\text{w/ proscript DOT}$ & 67.80 & 69.00 \\
ChatGPT$_\text{w/ node}$         & 72.00 & \textbf{78.00} \\
ChatGPT$_\text{w/ node \& edge}$ & \textbf{79.60} & \textbf{78.00}\\
\hline
\end{tabular}
\vspace{-0.2cm}
\caption{The performance of ChatGPT performs on the SCT-v1.0 test set (sampled 500 instances) and the SCT-v1.5 validation set. The submission upload for the SCT-v1.5 leaderboard (\url{https://competitions.codalab.org/competitions/15333}) is no longer available. Therefore, we test ChatGPT performance on the validation set. The ChatGPT template is displayed in Figure~\ref{fig:ChanGpt_Template}.}
\label{tab:ChatGPT_Performance}
\vspace{-0.7cm}
\end{table}

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{figs/ChanGpt_Template.pdf}
\vspace{-0.8cm}
\caption{ChatGPT Template}
\label{fig:ChanGpt_Template}
\end{figure*}


In addition to GNNs \cite{kipf2016semi, xu2018powerful, schlichtkrull2018modeling, liu2022boosting}, we also evaluated large language models as graph reasoning modules.
Recently, large language models (e.g.,  ChatGPT~\cite{openai2022chatgpt} and GPT-4~\cite{DBLP:journals/corr/abs-2303-08774}) have shown promising performance on various tasks, and have raised concerns and discussions on topics such as factuality and privacy~\cite{wang2023survey, DBLP:journals/corr/abs-2303-12712,
DBLP:journals/corr/abs-2302-10724,
DBLP:journals/corr/abs-2304-14827,
DBLP:journals/corr/abs-2305-12870,
DBLP:journals/corr/abs-2310-10383,
DBLP:journals/corr/abs-2311-04044}.
In this paper, we test ChatGPT~\footnote{The evaluation is performed in September 2023 by calling ChatGPT Model (\textit{gpt-3.5-turbo}) API .} in narrative reasoning tasks with additional grounded knowledge. 
The zero-shot performance of large language models, which relies on the sophisticated design of templates, has shown variance across various tasks~\cite{DBLP:conf/naacl/MaZGTLZH22,
DBLP:journals/corr/abs-2309-08303,
DBLP:conf/acl/ChanLCLSWS23,
DBLP:conf/icmlc2/ChanC23}.
To obtain replicable and representative results, we follow~\citet{DBLP:conf/iclr/RobinsonW23, cheng2021question} to formulate the task as a multiple choice question answering problem.





