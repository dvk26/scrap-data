\setstretch{1.2} 
\title{Policy Learning for Optimal Dynamic Treatment Regimes with Observational Data} 
%\thanks{The full version of the paper is available at \url{https://arxiv.org/abs/2106.12886}.}


\author{Shosei Sakaguchi\thanks{Faculty of Economics, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan. Email: sakaguchi@e.u-tokyo.ac.jp.} }
\date{\today}
%\date{\today \medskip \\ \textit{Preliminary Draft}}

\maketitle
\thispagestyle{empty}

\vspace{-1cm}

\begin{abstract}
\begin{spacing}{1.2}
   Public policies and medical interventions often involve dynamics in their treatment assignments, where individuals receive a series of interventions over multiple stages. 
    We study the statistical learning of optimal dynamic treatment regimes (DTRs) that guide the optimal treatment assignment for each individual at each stage based on the individual’s evolving history. We propose a doubly robust, classification-based approach to learning the optimal DTR using observational data under the assumption of sequential ignorability. This approach learns the optimal DTR through backward induction. At each step, it constructs an augmented inverse probability weighting (AIPW) estimator of the policy value function and maximizes it to learn the optimal policy for the corresponding stage. 
    We show that the resulting DTR can achieve an optimal convergence rate of $n^{-1/2}$ for welfare regret under mild convergence conditions on estimators of the nuisance components.
    \bigskip \\
\noindent 
\textbf{Keywords:} Dynamic treatment regime; Policy learning; Double robustness; Double machine learning; Backward induction. \\
%\textbf{JEL codes:} C22, C44, C54
\end{spacing}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}
\clearpage
\addtocounter{page}{-1}
%\newpage
\setstretch{1.4} 


\section{Introduction} \label{sec:introduction}


Public policies and medical interventions often involve dynamics in their treatment assignments. For example, some job training programs offer participants training sessions over multiple stages \citep[e.g.,][]{Lechner_2009,Rodriguez_et_al_2022}. In clinical medicine, physicians sequentially administer treatments, adapting to patients' evolving medical conditions \citep[e.g.,][]{Wang_et_al_2012,Pelham_et_al_2016}. Multi-stage treatment assignments are also common in educational programs spanning multiple grades \citep[e.g.,][]{krueger_1999,Ding_Lehrer_2010} and dynamic marketing strategies \citep[e.g.,][]{Liu_2023}, among other examples.

This study focuses on the optimal allocation of sequential treatments \citep{Robins_1986}, where individuals receive interventions over multiple stages. In this context, treatment effects at each stage are often heterogeneous, depending on prior treatments and related states. Therefore, adapting treatment allocation based on evolving information can substantially enhance the welfare gains of multi-stage interventions.

We study statistical learning for optimal sequential treatment assignment using data from quasi-experimental or observational studies. Throughout this paper, we assume sequential ignorability \citep{Robins_1997}, meaning that treatment assignment at each stage is independent of potential outcomes, conditional on the history of prior treatments and observed states. Under this assumption, we develop a method to learn the optimal Dynamic Treatment Regime (DTR), a sequence of stage-specific policies that determines the optimal treatment for each individual at each stage, based on their history up to that point.


In developing our approach, we build on recent advances in doubly robust policy learning \citep{Athey_Wager_2020, zhou2022offline} and extend them to dynamic settings. We propose a doubly robust, classification-based method to learn the optimal DTR using backward induction, which sequentially estimates the optimal policy from the final to the first stage. At each step of the backward induction, the method constructs an augmented inverse probability weighting (AIPW) estimator for the policy value function by combining estimators of propensity scores and action value functions (Q-functions) while using cross-fitting.\footnote{The Q-function for each stage of a fixed DTR is the conditional mean outcome given the treatment and history, assuming future treatments follow the fixed DTR.} The Q-functions can be estimated via fitted Q-evaluation \citep{Munos_et_al_2008, Fonteneau_et_al_2013, Le_et_al_2019}, a method for offline policy evaluation in reinforcement learning. At each step, the optimal policy for the corresponding stage is estimated by maximizing the estimated policy value function over a pre-specified class of stage-specific policies. This procedure produces the estimated DTR as a sequence of the estimated policies across all stages.

The proposed approach is computationally feasible due to step-wise backward optimization, which is a considerable advantage given the computational challenge of optimizing DTRs over multiple stages. Additionally, it does not rely on structural assumptions, such as Markov decision processes. 
Leveraging a classification-based framework \citep{Kitagawa_Tetenov_2018a}, the approach can enhance the interpretability of DTRs by incorporating interpretable policy classes, such as decision trees, for each stage.
Moreover, the approach can encompass various dynamic treatment problems, including optimal stopping/starting problems, by appropriately constraining the class of feasible DTRs.\footnote{An important example of an optimal stopping problem in economics is unemployment insurance programs that reduce benefit levels after a certain duration \citep[e.g.,][]{Meyer_1995,Kolsrud_et_al_2018}}


We study the statistical properties of the proposed approach in terms of welfare regret, defined as the average outcome loss of the estimated DTR relative to the optimal one. This is, however, a nontrivial task because stage-specific policies within the DTR are estimated sequentially, rather than simultaneously, and state variables are influenced by past treatments.
The main contribution of this paper is to establish the convergence rate for welfare regret, linking it to the convergence rates of the nuisance component estimators (propensity scores and Q-functions) in terms of the mean-squared-error (MSE). Our key result identifies conditions on the nuisance component estimators and the class of DTRs under which the resulting DTR attains the minimax optimal convergence rate of $n^{-1/2}$ for regret.
For example, if all nuisance components are estimated with an MSE convergence rate of $n^{-1/4}$, which is attainable by many machine learning methods under structured assumptions, the resulting DTR can achieve regret convergence to zero at the minimax optimal rate of $n^{-1/2}$. This result is comparable to those of \cite{Athey_Wager_2020} and \cite{zhou2022offline}, who study doubly robust policy learning in single-stage settings, and aligns with the spirit of double machine learning \citep{chernozhukov_et_al_2018}.

We illustrate the proposed method with an empirical application to data from Project STAR \citep[e.g.,][]{krueger_1999}, where we learn the optimal DTR for assigning each student to either a regular-size class with a teacher aide or a small-size class without one during their early education (kindergarten and grade 1). The estimated DTR uses information on students’ intermediate academic performance to determine the optimal class type for a subsequent grade. Our empirical results show that the optimal DTR leads to better academic outcomes for students compared with uniform class-type allocations.

%%%%%%%%%%

\subsection*{Related Literature \label{sec:related literature}}

Although numerous studies have investigated the statistical decision/learning of treatment choice, most have focused on the single-stage problem.%There is a large body of literature on the statistical decision/learning of treatment choice, but most focus on the single-stage problem.
\footnote{A partial list includes \cite{Manski_2004}, \cite{Hirano_Porter_2009}, \citeauthor{Stoye_2009} (\citeyear{Stoye_2009,Stoye_2012}), \cite{Qian_Murphy_2011}, \cite{Bhattacharya_Dupas_2012}, \cite{Tetenov_2012}, \cite{Zhao_et_al_2012}, \cite{Kitagawa_Tetenov_2018a}, \cite{Athey_Wager_2020}, \cite{Mbakop_Tabord-Meehan_2019}, \cite{kitagawa_et_al_2021}, \cite{zhou2022offline}, and \cite{viviano_2024}, among others.} 
Among these works, this study is most relevant to  \cite{Athey_Wager_2020} and \cite{zhou2022offline}. They propose doubly robust policy learning in single-stage settings and show that the $n^{-1/2}$-upper bound on the regret can be achieved even in the observational data setting. This study seeks to extend their approach and results to the dynamic treatment problem.\footnote{To be precise, this study extends the results of \cite{zhou2022offline} rather than those of \cite{Athey_Wager_2020}, as it involves multiple treatments at each stage and relies on a similar complexity condition on the policy class as in \cite{zhou2022offline}.} 

There is an expanding literature on the estimation of optimal DTRs, with many methods proposed.\footnote{\cite{Chakraborty_Murphy_2014}, \cite{Laber_et_al_2014}, \cite{Kosorok_et_al_2019}, and \cite{Li_et_al_2023} provide reviews of this literature.} Offline Q-learning \citep{Watkins_Dayan_1992} is arguably the most popular method for estimating optimal DTRs \citep[e.g.,][]{Murphy_2005,Moodie_et_al_2012,Zhang_et_al_2018}.\footnote{Q-learning is a sequential approach that estimates the optimal Q-functions and optimal policies from the final stage to the first stage through backward induction.} \cite{Murphy_2005} shows that the performance of the DTR obtained via Q-learning depends on the accuracy of the Q-function estimates; if the estimated Q-functions are not close to the true ones, the resulting DTR can be far from optimal. %Our approach also estimates Q-functions as part of its procedure. %However, it is more robust and accurate than Q-learning because it leverages propensity score models.
Our approach also estimates Q-functions as part of its procedure. However, leveraging propensity score models, it is more robust and accurate than Q-learning.

This study is also related to the classification-based, inverse probability weighting approach for estimating optimal DTRs \citep[e.g.,][]{Zhao_et_al_2015, Sakaguchi_2021}. This approach uses inverse propensity weighted outcomes to estimate the value function of a DTR and maximizes it to estimate the optimal DTR over a pre-specified class of DTRs. However, the use of inverse probability weighted outcomes sometimes leads to excessively high variance, resulting in suboptimal DTRs \citep{Doroudi_et_al_2018}. Our approach enhances the power of this method by incorporating models of Q-functions.  


Doubly robust estimation for optimal DTRs has also been proposed by \cite{Zhang_et_2013}, \cite{Wallace_Moodie_2015}, and \cite{Ertefaie_et_al_2021}. \cite{Zhang_et_2013} suggest estimating the optimal DTR by maximizing an AIPW estimator of the welfare function over an entire class of DTRs. However, this approach faces computational challenges for two reasons: (i) the nuisance components to be estimated depend on each specific DTR, and (ii) the method maximizes the estimated welfare function simultaneously over the entire class of DTRs. Our approach resolves these issues because (i) the nuisance components depend only on the estimated policies for future stages, and (ii) the optimal DTR is estimated through stage-wise backward optimization. Section \ref{sec:existing_approach} provides further details on this comparison.
\cite{Wallace_Moodie_2015} develop a doubly robust estimation method based on Q-learning and G-estimation \citep{Robins_2004}. \cite{Ertefaie_et_al_2021} propose a doubly robust approach for Q-learning and investigate the statistical properties of the estimated parameters in the Q-functions. By contrast, our focus is on the statistical properties of welfare regret for the estimated DTR.
In the context of policy learning for optimal stopping/starting problems, \cite{Nie_et_al_2021} develop a doubly robust learning approach with computational feasibility and show upper bounds on the associated regret. Our framework encompasses this problem as a specific case.


This study also relates to the literature on (offline) reinforcement learning in terms of multi-stage decision problems.\footnote{This study differs from the multi-armed bandit problem, as the latter involves online learning, whereas this study focuses on offline learning.}  However, most works assume a Markov decision process, which this study does not rely on.
In the context of non-Markov decision processes, \cite{Jiang_Li_2016}, \cite{Thomas_et_al_2016}, and \cite{Kallus_Uehara_2020} propose doubly robust methods for evaluating DTRs, but they do not focus on optimizing DTRs.

Finally, in the econometric literature on dynamic treatment analysis, \cite{Heckman_Navarro_2007} and \cite{Heckman_et_al_2016} use exclusion restrictions to identify the average dynamic treatment effects, but their focus is not on the identification of optimal DTRs. \cite{Han_2023} proposes a method to characterize the sharp partial ordering of the counterfactual welfares of DTRs in an instrumental variable setting. \cite{Ida_et_al_2024} empirically show that the optimal DTR outperforms optimal static targeting policies in the context of energy-saving rebate programs.

\subsection*{Structure of the Paper }
The remainder of the paper is organized as follows. Section \ref{sec:setup} outlines the dynamic treatment choice problem. Section \ref{sec:doubly_robust_policy_learning} presents the doubly robust approach for learning the optimal DTRs through backward induction. Section \ref{sec:statistical_properties} shows the statistical properties of the proposed approach. Section \ref{sec:existing_approach} compares the proposed approach with that of \cite{Zhang_et_2013}. Section \ref{sec:simulation} presents a simulation study to evaluate the finite sample performance of the proposed approach. Section \ref{sec:empirical_illustration} shows the the empirical application results. Appendix \ref{app:main_proof} includes the proof of the main theorem along with some auxiliary lemmas. %All proofs of auxiliary lemmas are provided in the supplement to this paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Setup\label{sec:setup}}

Section \ref{sec:dynamic treatment framework} introduces the dynamic treatment framework, following the dynamic counterfactual outcome framework of \citet{Robins_1986,Robins_1997} and \cite{Murphy_2003}. Subsequently, we define the dynamic treatment choice problem in Section \ref{sec:dynamic treatment choice problem}. 

\subsection{Dynamic Treatment Framework \label{sec:dynamic treatment framework}}

We consider a fixed number of stages, $T$ $(< \infty)$, for multiple treatment assignments.
Let $\MA_{t} \equiv \{0,\ldots,d_t -1\}$ ($t=1,\ldots,T$) denote the set of possible treatment arms at stage $t$, where $d_t$ ($\geq 2$) represents the number of treatment arms at stage $t$ and may vary across stages. We observe the assigned treatment $A_{t} \in \MA_{t}$ for each individual in each stage $t$. Let $S_{t}$ be a vector of the state variables observed prior to treatment assignment in stage $t$, which may depend on the past treatments. 
In each stage $t$, we observe the outcome $Y_{t}$ after the treatment intervention. $S_{t}$ ($t \geq 2$) may contain the previous outcomes $(Y_{1},\ldots,Y_{t-1})$. 
Throughout this paper, for any time-dependent object $V_{t}$, we denote by $\text{\ensuremath{\text{\ensuremath{\underline{V}}}}}_{t}\equiv \left(V_{1},\ldots,V_{t}\right)$
the history of the object up to stage $t$, and by $\text{\ensuremath{\underline{V}}}_{s:t} \equiv \left(V_{s},\ldots,V_{t}\right)$, for $s\leq t$, the partial history of the object from stage $s$ up to stage $t$.
For example, $\underline{A}_{t}$ denotes the treatment history up to stage $t$.

Let $Z\equiv(\underline{A}_{T},\underline{S}_T,\underline{Y}_{T})$ represent the vector of all observed variables. We define the history at stage $t$ as $H_t \equiv (\underline{A}_{t-1},\underline{S}_t)$, which is the information available when the policymaker selects the treatment intervention at stage $t$. Note that $H_1 = (S_1)$, where $S_1$ represents individual characteristics observed prior to the beginning of the sequential treatment intervention. We denote the supports of $H_{t}$ and $Z$ by ${\cal H}_{t}$ and $\MZ$, respectively.

To formalize our results, we employ the framework of dynamic potential outcomes \citep{Robins_1986,Hernan_et_al_2001,Murphy_2003}. Let $\underline{\MA}_t \equiv \MA_1 \times \cdots \times \MA_t$. We define $Y_{t}\left(\text{\ensuremath{\underline{a}}}_{t}\right)$ as the potential outcome of $\underline{a}_t \in \underline{\MA}_t$ for stage $t$, which represents the outcome realized at stage $t$ when the treatment history up to that stage corresponds to $\underline{a}_{t}$. We assume that the outcome at each stage is not influenced by treatments in future stages. Since the state variables $S_t$ may also depend on past treatments, we define the potential state variables as $S_{t}(\underline{a}_{t-1})$ for each $t\geq 2$ and $\underline{a}_{t-1} \in \underline{\MA}_{t-1}$. For $t = 1$, we set $S_{1}(\underline{a}_{0}) = S_1$. The observed outcomes and state variables are thus defined as $Y_{t} \equiv Y_{t}(\underline{A}_t)$ and $S_t \equiv S_t(\underline{A}_{t-1})$, respectively. 

Let $\underline{S}_t(\underline{a}_{t-1}) \equiv (S_1,S_2(\underline{a}_{1}),\ldots,S_t(\underline{a}_{t-1}))$. We define the vector $H_{t}\left(\underline{a}_{t-1}\right) \equiv \left(\underline{a}_{t-1},\underline{S}_t(\underline{a}_{t-1})\right)$  as the potential history realized when the prior treatments are $\underline{a}_{t-1}$. For $t=1$, we set $H_{1}\left(\underline{a}_{0}\right) = H_1$. The observed history is then defined as $H_t \equiv H_t\left(\underline{A}_{t-1}\right)$.
We denote by $P$ the distribution of all underlying variables $\left(\underline{A}_T,\left\{\underline{S}_T(\underline{a}_{T-1})\right\}_{\underline{a}_{T-1} \in \underline{\MA}_{T-1}},\{\underline{Y}_{T}(\underline{a}_T)\}_{\underline{a}_T \in \underline{\MA}_T}\right)$, where  $\underline{Y}_T(\underline{a}_T) \equiv (Y_1(a_1),Y_2(\underline{a}_{2}),\ldots,Y_T(\underline{a}_{T}))$.

From an observational study, we observe $Z_{i}\equiv \left(\underline{A}_{i,T},\underline{S}_{i,T},\underline{Y}_{i,T}\right)$ for individuals $i=1,\ldots,n$, where $\underline{A}_{i,T}=(A_{i,1},\ldots,A_{i,T})$, $\underline{S}_{i,T}=(S_{i,1},\ldots,S_{i,T})$, and $\underline{Y}_{i,T}=(Y_{i,1},\ldots,Y_{i,T})$. The observed outcome $Y_{i,t}$ and state variables $S_{i,t}$ are defined as $Y_{i,t}  \equiv Y_{i,t}(\underline{A}_{i,t})$ and $S_{i,t}  \equiv S_{i,t}(\underline{A}_{i,t-1})$, respectively, with $Y_{i,t}\left(\underline{a}_{t}\right)$ and $S_{i,t}\left(\underline{a}_{t-1}\right)$ being the
potential outcome and state variables, respectively, for individual $i$ at stage $t$. Let $\underline{S}_{i,T}(\underline{a}_{T-1}) \equiv (S_{i,1},S_{i,2}(a_1),\ldots,S_{i,T}(\underline{a}_{T-1}))$ and $\underline{Y}_{i,T}(\underline{a}_{T}) \equiv (Y_{i,2}(a_1),\ldots,Y_{i,T}(\underline{a}_{T}))$. We assume that the vectors of random variables $\left(\underline{A}_{i,T},\left\{\underline{S}_{i,T}\left(\text{\ensuremath{\underline{a}}}_{T-1}\right)\right\}_{\text{\ensuremath{\underline{a}}}_{T-1}\in\underline{\MA}_{T-1}}, \left\{ \underline{Y}_{i,T}\left(\text{\ensuremath{\underline{a}}}_{T}\right)\right\} _{\text{\ensuremath{\underline{a}}}_{T}\in\underline{\MA}_{T}}\right)$, for $i=1,\ldots,n$, are independent and identically distributed (i.i.d) under the distribution $P$. 
We denote by $H_{i,t}\equiv (\underline{A}_{i,t-1},\underline{S}_{i,t})$ the history of the $i$-th individual at stage $t$.

Define $e_{t}\left(h_{t},a_{t}\right)\equiv \Pr\left(A_{t}=a_{t}\mid H_{t}=h_{t}\right)$ as the propensity score of treatment $a_t$ at stage $t$ with the history $h_t$. In the observational data setting we study, the propensity scores are unknown to the analyst. This contrasts with the experimental data setting, in which the propensity scores are known from the experimental design.\footnote{Even when the data are obtained from a sequential randomized trial, the propensity scores may be unknown due to non-compliance with assigned treatments or attrition over stages.}

Throughout the paper, we suppose that the underlying distribution $P$ satisfies the following assumptions.

\medskip

\begin{assumption}[Sequential Ignorability]\label{asm:sequential independence} For any $t=1,\ldots,T$ and $\text{\ensuremath{\underline{a}}}_{T}\in\underline{\MA}_{T}$,
\begin{align*}
\{Y_{t}\left(\underline{a}_{t}\right),\ldots,Y_{T}\left(\underline{a}_{T}\right),S_{t+1}(\underline{a}_{t}),\ldots,S_{T}(\underline{a}_{T-1})\} \indep A_{t} \mid H_{t}\ \ \mbox{a.s.}  
\end{align*}
\end{assumption}

%\smallskip

\begin{assumption}[Bounded Outcomes]\label{asm:bounded outcome}
There exists $M <\infty$ such that the support of $Y_{t}(\underline{a}_{t})$ is
contained in $\left[-M/2,M/2\right]$ for all $t \in \{1,\ldots,T\}$ and $\underline{a}_{t} \in \underline{\MA}_{t}$.
\end{assumption}

%\smallskip

\medskip


Assumption \ref{asm:sequential independence} is what is called a dynamic unconfoundedness assumption or sequential conditional independence assumption elsewhere, and is commonly used in the literature of dynamic treatment effect analysis \citep{Robins_1997,Murphy_2003}. This assumption implies that the treatment assignment at each stage is independent of both current and future potential outcomes and future state variables, conditional on the history up to that point. In observational studies, this assumption holds when a sufficient set of confounders is controlled at each stage. Assumptions \ref{asm:bounded outcome} is a common assumption in the literature on estimating optimal DTRs.

\medskip

\subsection{Dynamic Treatment Choice Problem\label{sec:dynamic treatment choice problem}}

The aim of this study is  to develop a method for learning optimal DTRs using data from an observational study. We define a policy for each stage $t$ as $\pi_{t}:\MH_{t}\mapsto \MA_{t} $, a map from the history space for stage $t$ to the treatment space for stage $t$. A policy $\pi_t$ determines which treatment is assigned to each individual at stage $t$ based on their history $h_t$. We define a DTR as $\pi \equiv (\pi_{1}, \ldots, \pi_{T})$, a sequence of stage-specific policies. The DTR guides the treatment choice for each individual from the first to the final stage, based on their evolving history up to each stage.

Given a fixed DTR $\pi$, we define the welfare of $\pi$ as
\begin{align*}
W\left(\pi\right) &\equiv   
E\left[\sum_{t=1}^{T}\sum_{\underline{a}_t \in \underline{\MA}_t}\left( Y_{t}(\underline{a}_t)\cdot \prod_{s=1}^{t}1\{\pi_s(H_{s}(\underline{a}_{s-1}))=a_s\}\right)\right].
\end{align*}
This represents the expected total outcome realized when the sequential treatment assignments follow the DTR $\pi$.

We consider choosing a DTR from a pre-specified class of DTRs denoted by $\Pi \equiv \Pi_{1}\times\cdots\times \Pi_{T}$, where $\Pi_{t}$ represents a class of policies for stage $t$ (i.e., a class of measurable functions $\pi_{t}:\MH_{t}\rightarrow \MA_{t}$). For example, \cite{Laber_Zhao_2015}, \cite{Tao_et_al_2018}, \cite{Sun_Wang_2021},  \cite{Blumlein_et_al_2022}, and \cite{zhou_et_al_2023} use a class of decision trees \citep{Breiman84book} for $\Pi_t$, and \cite{Zhang_et_al_2018} use a class of list-form policies for $\Pi_t$. These policy classes are employed to enhance the interpretability of the resulting DTRs.%\footnote{\cite{Ahmad_et_al_2018} discuss the importance of interpretability of machine learning models in healthcare.} 

Our framework can also accommodate cases where only a subset of the history, $\tilde{h}_t \subseteq h_t$, is used for treatment choice by constraining $\Pi_t$ to be a class of functions of $\tilde{h}_t$. This is particularly important given the increasing dimension of the entire history $h_t$ over time, where only a sub-history may be informative for optimal treatment choice. Additionally, our framework can encompass the optimal stopping/starting problem by constraining the class $\Pi$ of DTRs as follows.

\medskip

\begin{example}[Optimal Starting/Stopping Problem]\label{eg:optimal_start}
Suppose that the number of treatment arms $d_t$ is time-invariant ($d_t = K$ for some $K$) and that arm $0$ represents no treatment. Our framework can accommodate the optimal starting problem, where the decision-maker determines when to start assigning one of the arms $a_t \in \{1,\ldots,K-1\}$ for each unit. This problem can be encompassed in our framework by constraining the class of DTRs $\Pi$ such that for any $t \in \{2,\ldots,T\}$ and $(\pi_{t}, h_{t}) \in \Pi_{t} \times \MH_{t}$, $\pi_{t}(h_t) = a_{t-1}$ if $a_{t-1} \neq 0$. The optimal stopping problem can also be specified in a similar manner.
\end{example}
\medskip

An important example of an optimal stopping problem in economics is unemployment insurance programs that reduce benefit levels after a certain duration \citep[e.g.,][]{Meyer_1995,Kolsrud_et_al_2018}. In this context, a DTR determines the timing of benefit reductions for each unemployed individual.

Given a pre-specified class $\Pi$ of DTRs, we impose an overlap condition on $\underline{D}_{T}$, associated with the structure of $\Pi$, as follows.

\medskip

\begin{assumption}[Overlap Condition]\label{asm:overlap} 
There exists $\eta \in (0,1)$ for which $\eta \leq e_{t}(h_{t},a_{t})$ holds for any $t \in \{1,\ldots,T\}$ and any pair  $(h_{t},a_{t}) \in \MH_{t} \times \MA_{t}$ such that there exists $\pi_{t} \in \Pi_{t}$ that satisfies $\pi_{t}(h_t) = a_{t}$. 
\end{assumption}

\medskip

When $\Pi$ is structurally constrained, Assumption \ref{asm:overlap} is weaker than the common overlap condition that requires the overlap $e_{t}(h_t, a_t) \in (0, 1)$ for all $(h_t, a_t) \in \MH_{t} \times \MA_{t}$ and $t$. For example, in the optimal starting problem, Assumption  \ref{asm:overlap} does not require $e_{t}(h_t, 0) > 0$ for any $h_t$ such that $a_s$ in $h_t$ differs from 0 for some $s < t$.

The ultimate goal of our analysis is to choose an optimal DTR that maximizes the welfare $W\left(\cdot\right)$ over $\Pi$. 
We are especially interested in learning the optimal DTR from observational data that satisfies the sequential ignorability assumption (Assumption \ref{asm:sequential independence}).
The following section presents a step-wise approach to learning the optimal DTR.



\section{Learning of the Optimal DTR}\label{sec:doubly_robust_policy_learning}

In this section, we propose a doubly robust approach to learning the optimal DTRs through backward induction. Section \ref{sec:fitted_Q} first introduces Q-function (action-value function) and the fitted Q-evaluation, a method to estimate the Q-functions. This section also discusses identifiability of the optimal DTR through the backward-induction procedure. 
Section \ref{sec:learning_DTR} then presents our proposed approach to learning the optimal DTRs.

\subsection{Fitted Q-evaluation and Backward Induction}
\label{sec:fitted_Q}

For any DTR $\pi$ and the class $\Pi$ of DTRs, we denote their partial sequences by $\pi_{s:t} \equiv (\pi_s,\ldots,\pi_t)$ and $\Pi_{s:t} \equiv \Pi_s \times \cdots \times \Pi_t$, respectively, for $s\leq t$.\footnote{For any object $v_{s:t}$ and $\underline{w}_{s:t}$ ($s\leq t$), $v_{t:t}$ and $\underline{w}_{t:t}$ correspond to $v_t$ and $w_t$, respectively.}
We define the policy value of $\pi_{t:T}$ for stage $t$ as 
\begin{align*}
    V_{t}\left(\pi_{t:T}\right) \equiv E\left[\sum_{s=t}^{T}\sum_{\underline{a}_{s} \in \underline{\MA}_{s}}\left( Y_{s}(\underline{a}_{s})\cdot1\{\underline{A}_{t-1} = \underline{a}_{t-1}\} \cdot \prod_{\ell=t}^{s}1\{\pi_{\ell}\left(H_{\ell}(\underline{a}_{\ell-1})\right)=a_\ell\}\right)\right],
\end{align*}
where we assume $1\{\underline{A}_{0} = \underline{a}_{0}\}=1$ for $t=1$. $V_{t}\left(\pi_{t:T}\right)$
represents the average total outcome from stage $t$ to stage $T$ that is realized when the treatment assignments before stage $t$ follow $\underline{A}_{t-1}$ (i.e., assignments in the observational data), and those from stage $t$ follow $\pi_{t:T}$.
With some abuse of terminology, we refer to $V_{t}\left(\pi_{t:T}\right)$ as the policy value function of $\pi_{t:T}$.
Note that $V_{1}(\pi_{1:T}) = W(\pi)$.

Given a fixed DTR $\pi$, we define Q-functions (state-action-value functions), recursively, as follows:
\begin{align}
    Q_T(h_T,a_T) &\equiv E\left[Y_{T}|H_T = h_T, A_T = a_T\right], \label{eq:action_value_func_1}\\
     Q_{T-1}^{\pi_{T}}(h_{T-1},a_{T-1}) &\equiv E\left[Y_{T-1} + Q_T(h_T,\pi_T(H_{T}))|H_{T-1} = h_{T-1}, A_{T-1} = a_{T-1}\right],
     \end{align}
     and, for $t=T-2,\ldots,1$,
     \begin{align}
     Q_{t}^{\pi_{(t+1):T}}(h_t,a_t) &\equiv E\left[Y_{t} + Q_{t+1}^{\pi_{(t+2):T}}(H_{t+1},\pi_{t+1}(H_{t+1}))|H_t = h_t, A_t = a_t\right]. \label{eq:state_value_func_2}
\end{align}
We refer to $Q_{t}^{\pi_{(t+1):T}}(h_t,a_t)$ as the Q-function for $\pi_{(t+1):T}$. 
The Q-function $Q_{t}^{\pi_{(t+1):T}}(h_t,a_t)$ represents the average total outcome when the history $H_t$ and treatment $A_t$ correspond to $h_t$ and $a_t$ in stage $t$, and the future treatment assignments follow $\pi_{(t+1):T}$. 
When $t=T$, we denote $Q_{T}^{\pi_{(T+1):T}}(\cdot,\cdot) = Q_{T}(\cdot,\cdot)$.  Note that $E[Q_{t}^{\pi_{(t+1):T}}(H_{t},\pi_{t}(H_{t}))] = V_{t}(\pi_{t:T})$ holds under Assumption \ref{asm:sequential independence}.\footnote{See, for example, \citeauthor{Tsiatis_et_al_2019}(\citeyear{Tsiatis_et_al_2019}, Section 6.4) or Lemma \ref{lemma:identification} in Appendix \ref{app:proofs_of_lemmas}.} 
In what follows, for any function $f(\cdot,\cdot):\MH_{t} \times \MA_{t} \rightarrow \Real$ and policy $\pi_{t}(\cdot):\MH_{t} \rightarrow \MA_{t}$, we denote $f(h_{t},\pi_{t}(h_{t}))$ shortly by $f(h_{t},\pi_{t})$ (e.g., $Q_{t}^{\pi_{(t+1):T}}(h_t,\pi_t(h_t))$ is denoted by $Q_{t}^{\pi_{(t+1):T}}(h_t,\pi_t)$).



Given a fixed DTR $\pi$, we can use the sequential definitions in equations (\ref{eq:action_value_func_1})--(\ref{eq:state_value_func_2}) to estimate the sequence $\{Q_{t}^{\pi_{(t+1):T}}(\cdot,\cdot)\}_{t=1,\ldots,T}$ of the Q-functions for $\pi$. This approach is referred to as the fitted Q-evaluation \citep{Munos_et_al_2008,Fonteneau_et_al_2013,Le_et_al_2019} in the  reinforcement learning literature and comprises multiple steps as follows:
\begin{itemize}
    \item Regress $Y_{T}$ on $(H_T,A_T)$ to obtain $\widehat{Q}_{T}(\cdot,\cdot)$ as the estimated regression function for $Q_T(\cdot,\cdot)$;
    \item Regress $Y_{T-1} + \widehat{Q}_{T}(H_{T},\pi_{T})$ on $(H_{T-1},A_{T-1})$ to obtain $\widehat{Q}_{T-1}^{\pi_{T}}(\cdot,\cdot)$ as the estimated regression function for $Q_{T-1}^{\pi_{T}}(\cdot,\cdot)$;
   \item Recursively, for $t=T-2,\ldots,1$, regress $Y_{t} + \widehat{Q}_{t+1}^{\pi_{(t+2):T}}(H_{t+1},\pi_{t+1})$ on $(H_t,A_t)$ to obtain $\widehat{Q}_{t}^{\pi_{(t+1):T}}(\cdot,\cdot)$ as the estimated regression function for $Q_{t}^{\pi_{(t+1):T}}(\cdot,\cdot)$.
\end{itemize}
We can apply flexible/nonparametric regression methods, including machine learning methods (e.g., random forests, lasso, neural networks), to the regression in each step. 


Given the definitions of the Q-functions, we can optimize the DTR through backward induction. To present the idea, we here assume that the generative distribution function $P$ is known and that the pair  $(P,\Pi)$ satisfies Assumptions \ref{asm:sequential independence}, \ref{asm:bounded outcome}, and \ref{asm:overlap}. The backward-induction approach in the population problem is a step-wise process that proceeds as follows. In the first step, the approach optimizes the final-stage policy over $\Pi_T$ by solving 
\begin{align*}
        \pi_{T}^{\ast,B} = \argmax_{\pi_{T} \in \Pi_{T}} E\left[Q_{T}(H_{T},\pi_{T})\right].
\end{align*}
Then, recursively, from $t=T-1$ to $1$, the approach optimizes the $t$-th stage policy over $\Pi_{t}$ by solving
\begin{align*}
        \pi_{t}^{\ast,B} = \argmax_{\pi_{t} \in \Pi_{t}} E\left[Q_{t}^{\pi_{(t+1):T}^{\ast,B}}(H_{t},\pi_{t})\right].
\end{align*}
Note that the objective function $E\left[Q_{t}^{\pi_{(t+1):T}^{\ast,B}}(H_{t},\pi_{t})\right]$ corresponds to the policy value $V_{t}(\pi_{t},\pi_{(t+1):T}^{\ast,B})$ under Assumption \ref{asm:sequential independence}.
The entire procedure yields the DTR $\pi^{\ast,B} \equiv (\pi_{1}^{\ast,B},\cdots,\pi_{T}^{\ast,B})$.

We denote the optimal DTR over $\Pi$ by $\pi^{\ast,opt} = \argmax_{\pi \in \Pi} W(\pi)$. Importantly, the DTR $\pi^{\ast,B}$ obtained through backward induction does not necessarily correspond to the optimal DTR $\pi^{\ast,opt}$ when $\Pi$ is structurally constrained, as noted by \cite{Sakaguchi_2021}. 
To ensure that backward induction yields the optimal DTR, the policy class $\Pi_t$ for each stage $t$ ($\geq 2$) needs to be correctly specified. The following is a sufficient condition for backward optimization to attain optimality.

\medskip

\begin{assumption}[Correct Specification] 
\label{asm:first-best} 
There exists $\pi_{2:T}^{\ast} = (\pi_{2}^{\ast},\ldots,\pi_{T}^{\ast})\in \Pi_{2:T}$ such that for any $t=2,\ldots,T$,
\begin{align*}
Q_{t}^{\pi_{(t+1):T}^{\ast}}\left(H_t,\pi_{t}^{\ast}\right) \geq \sup_{\pi_{t} \in \Pi_t}Q_{t}^{\pi_{(t+1):T}^{\ast}}\left(H_t,\pi_{t}\right) \mbox{\ a.s.}
\end{align*} 
\end{assumption}

\medskip

Assumption \ref{asm:first-best} requires that, for each stage $t \geq 2$, $\Pi_t$ includes a policy that can select the optimal arm among all feasible arms in $\{\pi_{t}(h_t):\pi_{t} \in \Pi_t\}$ $(\subseteq \MA_{t})$ for any history $h_t$. This assumption is satisfied when $\Pi_t$ (for $t=2,\ldots,T$) is flexible enough or correctly specified in relation to treatment effect heterogeneity.\footnote{ \cite{Zhang_et_2013} discuss how to correctly specify $\Pi_t$ based on the model for treatment effect heterogeneity. \cite{Zhao_et_al_2015} use a reproducing kernel Hilbert space for each $\Pi_t$ as a flexible class of polices.} We suppose that $\Pi$ satisfies Assumption \ref{asm:first-best}. Note that Assumption \ref{asm:first-best} is a sufficient but not a necessary condition.\footnote{\citeauthor{Sakaguchi_2021} (\citeyear{Sakaguchi_2021}, Remark 3.1) provides two examples demonstrating cases where backward induction leads to optimality in one instance and sub-optimality in another when Assumption \ref{asm:first-best} does not hold.}

A stronger version of Assumption \ref{asm:first-best} is that each $\Pi_t$ ($t \geq 2$) contains the first-best policy; that is, there exists $\pi_{2:T}^{\ast,FB} = (\pi_{2}^{\ast,FB},\ldots,\pi_{T}^{\ast,FB})\in \Pi_{2:T}$ such that for any $t=2,\ldots,T$,
\begin{align*}
    Q_{t}^{\pi_{(t+1):T}^{\ast,FB}}\left(H_t,\pi_{t}^{\ast,FB}\right) \geq \max_{a_{t} \in \MA_t}Q_{t}^{\pi_{(t+1):T}^{\ast,FB}}\left(H_t,a_t\right) \mbox{\ a.s.}
\end{align*}
The first-best policy $\pi_{t}^{\ast,FB}$ is the policy that selects the best treatment arm for any history $h_t$. \cite{Sakaguchi_2021} argues that the availability of the first-best policies is a fundamental assumption for the optimality of the backward optimization. However, Assumption \ref{asm:first-best} is practically weaker than this.
For example, in the optimal starting problem (Example \ref{eg:optimal_start}), although $\Pi_t$ may not include the first-best policy $\pi_{t}^{\ast,FB}$ due to structural constraints from the optimal starting problem, Assumption \ref{asm:first-best} does not require its feasibility. %Instead, it requires the optimal policy for the optimal starting problem. 
Similarly, when each policy in $\Pi_t$ depends only on a sub-history $\tilde{h}_t$ (a subset of $h_t$), Assumption \ref{asm:first-best} requires that the optimal policy is available for the sub-history, rather than for the entire history.

%The following lemma shows that the backward induction procedure leads to the optimal DTR under Assumption \ref{asm:first-best}. 

The following lemma formalizes the optimality of the backward-induction procedure under Assumption \ref{asm:first-best}.

\medskip

\begin{lemma}\label{lem:optimality_backward_induction}
Under Assumptions \ref{asm:sequential independence}, \ref{asm:overlap}, and \ref{asm:first-best}, $\pi^{\ast,B}$ is the optimal DTR over $\Pi$; that is, %$W(\pi^{\ast,B}) \geq W(\pi)$ for all $\pi \in \Pi$.
\begin{align*}
    W(\pi^{\ast,B}) \geq W(\pi) \mbox{\ for all\ }\pi \in \Pi.
\end{align*}
\end{lemma}

\begin{proof}
See Appendix \ref{app:preliminary_results}.
\end{proof}


\medskip
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Learning of the Optimal DTRs through Backward Induction}
\label{sec:learning_DTR}

This section presents a backward optimization procedure to learn the optimal DTRs using an AIPW estimator of the policy value function. Following the doubly robust policy learning of \cite{Athey_Wager_2020} and \cite{zhou2022offline}, we employ cross-fitting \citep{Schick_1986,chernozhukov_et_al_2018} to estimate the policy value function and learn the optimal policy independently. We randomly divide the dataset $\{Z_i:i=1,\ldots,n\}$ into $K$ evenly-sized folds (e.g., $K=5$). Let $I_k$ be the set of indices of the data in the $k$-th fold, and $I_{-k}$ denote the set of indices of the data excluded from the $k$-th fold. In what follows, for any statistics $\hat{f}$, we denote by $\hat{f}^{-k}$ the corresponding statistics calculated using the data excluded from the $k$-th fold.


The approach we propose is based on backward induction and thus consists of multiple steps. As a preliminary step, we estimate the propensity scores $\{e_t(\cdot,\cdot)\}_{t=1,\ldots,T}$ for all stages and the Q-function $Q_T(\cdot,\cdot)$ for the final stage using the data excluded in each cross-fitting fold. For each fold index $k$, we denote by $\hat{e}_{t}^{-k}(\cdot,\cdot)$ and $\widehat{Q}_{T}^{-k}(\cdot,\cdot)$, respectively, the estimators of $e_t(\cdot,\cdot)$ and $Q_T(\cdot,\cdot)$ using data not contained in the $k$-th fold. Any regression methods, including machine learning methods (e.g., random forests and neural networks), can be used to estimate $e_t(\cdot,\cdot)$ and $Q_{T}(\cdot,\cdot)$. 

Given $\{\hat{e}_{t}^{-k}(\cdot,\cdot)\}_{t=1,\ldots,T}$ and  $\widehat{Q}_{T}^{-k}(\cdot,\cdot)$ for each $k=1,\ldots,K$, we estimate the optimal DTR sequentially as follows. In the first step, regarding the final stage $T$, we construct a score function of the treatment $a_T$ for stage $T$ as 
\begin{align*}
    \widehat{\Gamma}_{i,T}(a_T) \equiv \frac{Y_{i,T} - \widehat{Q}_{T}^{-k(i)}(H_{i,T},A_{i,T})}{\hat{e}_{T}^{-k(i)}(H_{i,T},A_{i,T})}\cdot 1\{A_{i,T}=a_T\} +
    \widehat{Q}_{T}^{-k(i)}(H_{i,T},a_{T}).
\end{align*}
Given a policy $\pi_T$, the sample mean $(1/n)\sum_{i=1}^{n}\widehat{\Gamma}_{i,T}(\pi_T(H_{i,T}))$ is an AIPW estimator of the policy value $V_{t}(\pi_{T})$ for stage $T$.

We then find the best candidate policy for stage $T$ by solving
\begin{align*}
\hat{\pi}_{T} = \argmax_{\pi_{T}\in \Pi_{T}}\frac{1}{n}\sum_{i=1}^{n}\widehat{\Gamma}_{i,T}\left(\pi_T(H_{i,T})\right). 
\end{align*}


In the next step, we consider stage $T-1$. Given the estimated policy $\hat{\pi}_{T}$ from the previous step, for each $k$-th cross-fitting fold, we estimate the Q-function  $Q_{T-1}^{\hat{\pi}_{T}} (\cdot,\cdot)$ for $\hat{\pi}_{T}$ by regressing $Y_{i,T-1} + \widehat{Q}_{T}(H_{i,t},\hat{\pi}_{T})$ on $(H_{i,T-1},A_{i,T-1})$ using the observations whose indices are not included in $I_k$. This corresponds to the second step of the fitted-value Q-evaluation.
We denote by $\widehat{Q}_{T-1}^{\hat{\pi}_{T},-k}(\cdot,\cdot)$ the resulting estimator of $Q_{T-1}^{\hat{\pi}_{T}}(\cdot,\cdot)$ for each fold $k$. Any regression method can be applied at this step. 

We subsequently construct the score function of $a_{T-1}$ as follows:
\begin{align*}
    \widehat{\Gamma}_{i,T-1}^{\hat{\pi}_{T}}(a_{T-1})  &\equiv \frac{ Y_{i,T-1} + \widehat{\Gamma}_{i,T}(\hat{\pi}_{T}(H_{i,T})) - \widehat{Q}_{T-1}^{\hat{\pi}_{T},-k(i)}(H_{i,T-1},A_{i,T-1}) }{\hat{e}_{T-1}^{-k(i)}(H_{i,T-1},A_{i,T-1})} \cdot 1\{A_{i,T-1} = a_{T-1}\} \\
    &+ 
\widehat{Q}_{T-1}^{\hat{\pi}_{T},-k(i)}(H_{i,T-1},a_{T-1}).
\end{align*}
Given a policy $\pi_{T-1}$, the sample mean $(1/n)\sum_{i=1}^{n}\widehat{\Gamma}_{i,T-1}^{\hat{\pi}_{T}}(\pi_{T-1}(H_{i,T-1}))$ is an AIPW estimator of the policy value $V_{T-1}(\pi_{T-1},\hat{\pi}_{T})$.
We then find the best candidate policy for stage $T-1$ by solving
\begin{align*}
    \hat{\pi}_{T-1} = \argmax_{\pi_{T-1}\in \Pi_{T-1}} \frac{1}{n}\sum_{i=1}^{n}\widehat{\Gamma}_{i,T-1}^{\hat{\pi}_T}\left(\pi_{T-1}(H_{i,T-1})\right).
\end{align*}
%Given the solution $\hat{\pi}_{T-1}$, the sample mean $(1/n)\sum_{i=1}^{n}\widehat{\Gamma}_{i,T-1}^{\hat{\pi}_T}\left(\hat{\pi}_{T-1}(H_{i,T-1})\right)$ becomes an AIPW estimator of the policy value $V_{T-1}(\hat{\pi}_{(T-1):T})$ for the sequence of the estimated policies $\hat{\pi}_{(T-1):T}$.

Recursively, for $t=T-2,\ldots,1$, we learn the optimal policy as follows. For each cross-fitting index $k$, we first estimate the Q-function $Q_{t}^{\hat{\pi}_{(t+1):T}}$ by regressing  $Y_{i,t} + \widehat{Q}_{t+1}^{\hat{\pi}_{(t+2):T}}(H_{i,t+1},\hat{\pi}_{t+1})$ on $(H_{i,t},A_{i,t})$ using the observations whose indices are not in $I_k$ (the fitted Q-evaluation). Any regression method can be applied at this step. 

We next construct the score function of $a_{t}$ as
\begin{align*}
    \widehat{\Gamma}_{i,t}^{\hat{\pi}_{(t+1):T}}(a_t) &\equiv \frac{ Y_{i,t} + \widehat{\Gamma}_{i,t+1}^{\hat{\pi}_{(t+2):T}}(\hat{\pi}_{t+1}(H_{i,t+1})) - \widehat{Q}_{t}^{\hat{\pi}_{(t+1):T},-k(i)}(H_{i,t},A_{i,t}) }{\hat{e}_{t}^{-k(i)}(H_{i,t},A_{i,t})}\cdot 1\{A_{i,t} = a_t\} \\
    &+ 
    \widehat{Q}_{t}^{\hat{\pi}_{(t+1):T},-k(i)}(H_{i,t},a_{t}).
\end{align*}
We then find the best candidate policy for stage $t$ by solving
\begin{align*}
    \hat{\pi}_{t} = \argmax_{\pi_{t}\in \Pi_{t}} \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\hat{\pi}_{(t+1):T}}\left(\pi_{t}(H_{i,t})\right),
\end{align*}
where the objective function $(1/n)\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\hat{\pi}_{(t+1):T}}\left(\pi_{t}(H_{i,t})\right)$ is an AIPW estimator of the policy value $V_{t}(\pi_t,\hat{\pi}_{(t+1):T})$.

Throughout this procedure, we obtain the sequence $\hat{\pi} \equiv (\hat{\pi}_{1},\ldots,\hat{\pi}_{T})$, which serves as the estimator for the optimal DTR. In the following section, we will show its statistical properties with respect to its welfare regret. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Statistical Properties}\label{sec:statistical_properties}

Given a DTR $\pi \in \Pi$, we define the regret of $\pi$ by $R(\pi) \equiv \sup_{\tilde{\pi} \in \Pi}W(\tilde{\pi}) - W(\pi)$, the loss of the welfare of $\pi$ relative to the maximum welfare achievable in $\Pi$.
We study statistical properties of $\hat{\pi}$ with respect to its regret $R(\hat{\pi})$. This section shows the rate of convergence of $R(\hat{\pi})$ depending on the convergence rates of the estimators of the nuisance components, $\{\hat{e}_{t}^{-k}(\cdot,\cdot)\}_{t=1,\ldots,T}$ and  $\{\widehat{Q}_{t}^{\pi_{(t+1):T},-k}(\cdot,\cdot)\}_{t=1,\ldots,T}$, and the complexity of $\Pi$. 

Let $\widehat{Q}_{t}^{\pi_{(t+1):T},(n)}(\cdot,\cdot)$ and $\hat{e}_{t}^{(n)}(\cdot,\cdot)$ denote the estimators of the Q-function $Q_{t}^{\pi_{(t+1):T}}(\cdot,\cdot)$ for $\pi_{(t+1):T}$ and the propensity score $e_t(\cdot,\cdot)$, respectively, using a sample of size $n$ randomly drawn from the distribution $P$. For $t=T$, we denote $\widehat{Q}_{T}^{\pi_{(T+1):T},(n)}(\cdot,\cdot) = \widehat{Q}_{T}^{(n)}(\cdot,\cdot)$.
We suppose that $\{\widehat{Q}_{t}^{\pi_{(t+1):T},(n)}(\cdot,\cdot)\}_{t=1,\ldots,T}$ and $\{\hat{e}_{t}^{(n)}(\cdot,\cdot)\}_{t=1,\ldots,T}$ satisfy the following assumption.

\medskip{}

\begin{assumption}\label{asm:rate_of_convergence_backward}
(i) There exists $\tau >0$ such that the following holds: For all $t=1,\ldots,T$, $s=1,\ldots,t$, and $m \in \{0,1\}$,
\begin{align*}
    \sup_{\underline{a}_{s:t} \in \underline{\MA}_{s:t}} & E\left[ \sup_{\pi_{(t+1):T} \in \Pi_{(t+1):T}}\left(\widehat{Q}_{t}^{\pi_{(t+1):T},(n)}(H_{t},a_{t}) - Q_{t}^{\pi_{(t+1):T}}(H_{t},a_{t})\right)^{2}\right] \\
    &\times E\left[\left(\frac{1}{\prod_{\ell=s}^{t-m}\hat{e}_{\ell}^{(n)}(H_{\ell},a_{\ell})} - \frac{1}{\prod_{\ell=s}^{t-m}e_{\ell}(H_{\ell},a_{\ell})} \right)^{2}\right] = \frac{o(1)}{n^{\tau}}.
\end{align*}
(ii) There exists $n_0 \in \Natural$ such that for any $n \geq n_0$ and $t=1,\ldots,T$, 
\begin{align*}
     \sup_{a_{t} \in \MA_{t}, \pi_{(t+1):T} \in \Pi_{(t+1):T}}\widehat{Q}_{t}^{\pi_{(t+1):T},(n)}(H_{t},a_{t}) < \infty \mbox{\ \ and\ \ }
     \min_{a_t \in \MA_t}\hat{e}_{t}^{(n)}(H_{t},a_{t}) > 0
\end{align*}
hold a.s.
\end{assumption}

\medskip{}


As we will see later, the $\sqrt{n}$-consistency of the regret  $R(\hat{\pi})$ to zero can be achieved when Assumption \ref{asm:rate_of_convergence_backward} (i) holds with $\tau=1$, which is not very strong or restrictive. For example, Assumption \ref{asm:rate_of_convergence_backward} (i) is satisfied with $\tau = 1$ when
\begin{align*}
&\sup_{a_{t} \in \MA_{t}}  E\left[\sup_{\pi_{(t+1):T} \in \Pi_{(t+1):T}}\left(\widehat{Q}_{t}^{\pi_{(t+1):T},(n)}(H_{t},a_{t}) - Q_{t}^{\pi_{(t+1):T}}(H_{t},a_{t})\right)^{2}\right] = \frac{o(1)}{\sqrt{n}}\mbox{\ and\ }\\
    &\sup_{\underline{a}_{s:t} \in \underline{\MA}_{s:t}} E\left[\left(\frac{1}{\prod_{\ell=s}^{t}\hat{e}_{\ell}^{(n)}(H_{\ell},a_{\ell})} - \frac{1}{\prod_{\ell=s}^{t}e_{\ell}(H_{\ell},a_{\ell})} \right)^{2}\right] = \frac{o(1)}{\sqrt{n}}
\end{align*}
hold for all $t=1,\ldots,T$ and $s=1,\ldots,t$. The uniform MSE convergence rate of $\widehat{Q}_{t}^{\pi_{(t+1):T},(n)}(H_{t},a_{t})$ for the fitted Q-evaluation is not a standard result, but some existing results could be applicable with some modifications \citep[e.g.,][]{Zhang_et_al_2018}.\footnote{\cite{Zhang_et_al_2018} derive the rate of convergence for offline Q-learning using Support Vector Machine regression with policy search over the class of list-form policies. While their study focuses on Q-learning, the results can likely be extended to fitted Q-evaluation.}
Note also that Assumption \ref{asm:rate_of_convergence_backward} (i) encompasses the property of doubly robustness; that is, Assumption \ref{asm:rate_of_convergence_backward} (i) holds if either $\widehat{Q}_{t}^{\pi_{(t+1):T},(n)}(\cdot,\cdot)$ is uniformly consistent or $\hat{e}_{t}^{(n)}(\cdot,\cdot)$ is consistent.


We next consider the complexity of the class $\Pi$ of DTRs and the class $\Pi_t$ of stage-specific policies. Following \cite{zhou2022offline}, we use the $\epsilon$-Hamming covering number to measure the complexity of the class of policy sequences $\pi_{s:t} \in \Pi_{s}\times \cdots \times \Pi_{t}$ for each $s$ and $t$ such that $s \leq t$. 

\medskip{}

\begin{definition}
(i) For any stages $s$ and $t$ such that $s \leq t$, given a set of history points $\{h_{t}^{(1)},\ldots,h_{t}^{(n)}\} \subseteq \MH_{t}$, we define the Hamming distance between two sequences of policies  $\pi_{s:t}, \pi_{s:t}^{\prime} \in \Pi_{s:t}$ as $d_{h}(\pi_{s:t},\pi_{s:t}^{\prime}) \equiv n^{-1}\sum_{i=1}^{n}1\{\pi_s(h_{s}^{(i)})\neq \pi_{s}^{\prime}(h_{s}^{(i)}) \vee \cdots \vee \pi_t(h_{t}^{(i)})\neq \pi_{t}^{\prime}(h_{t}^{(i)})\}$,
where $h_{s}^{(i)} \subseteq h_{s+1}^{(i)}\subseteq \ldots \subseteq h_{t}^{(i)} \in \MH_t$ from the definition of the history. 
Let $N_{d_h}\left(\epsilon,\Pi_{s:t},\left\{h_{t}^{(1)},\ldots,h_{t}^{(n)}\right\}\right)$ be the smallest number of sequences of policies $\pi_{s:t}^{(1)},\pi_{s:t}^{(2)},\ldots$ in $\Pi_{s:t}$ such that for any $\pi_{s:t} \in \Pi_{s:t}$, there exists $\pi_{s:t}^{(i)}$ satisfying $d_h(\pi_{s:t},\pi_{s:t}^{(i)}) \leq \epsilon$.
We define the $\epsilon$-Hamming covering number of $\Pi_{s:t}$ as
\begin{align*}
    N_{d_h}(\epsilon,\Pi_{s:t})  \equiv  \sup\left\{N_{d_h}\left(\epsilon,\Pi_{s:t},\left\{h_{t}^{(1)},\ldots,h_{t}^{(n)}\right\}\right)\middle| n \geq 1, h_{t}^{(1)},\ldots,h_{t}^{(n)} \in \MH_t\right\}.
\end{align*}
(ii) We define the entropy integral of $\Pi_{s:t}$ as $\kappa(\Pi_{s:t})= \int_{0}^{1}\sqrt{\log N_{d_h}\left(\epsilon^2,\Pi_{s:t}\right)}d\epsilon$.
\end{definition}

\medskip{}

Note that when $s=t$, $N_{d_h}(\epsilon,\Pi_{s:t}) = N_{d_h}(\epsilon,\Pi_t)$ and $\kappa(\Pi_{s:t}) = \kappa(\Pi_t)$. 
We assume that the policy class $\Pi_t$ for each $t$ is not excessively complex in terms of the covering number.
\medskip{}

\begin{assumption}\label{asm:bounded entropy}
For all $t=1,\ldots,T$, $N_{d_h}(\epsilon,\Pi_{t}) \leq C\exp(D(1/\epsilon)^{\omega})$ holds for any $\epsilon>0$ and some constants $C,D>0$ and $0<\omega<0.5$.
\end{assumption}

\medskip{}

This assumption implies that the covering number of $\Pi_t$ does not grow too quickly, but allows $\log N_{d_h}(\epsilon,\Pi_{t})$ to grow at a rate of  $1/\epsilon$.
This assumption is satisfied, for example, by a class of finite-depth trees (see \citeauthor{zhou2022offline} (\citeyear{zhou2022offline}, Lemma 4)). In the case of a binary action set (i.e., $|\MA_t|=2$), a VC-class of $\MA_{t}$ also satisfies Assumption \ref{asm:bounded entropy}. 
\citeauthor{zhou2022offline} (\citeyear{zhou2022offline}, Remark 4) shows that the entropy integral $\kappa(\Pi_t)$ is finite under Assumption \ref{asm:bounded entropy}. 

Regarding the class $\Pi$ of entire DTRs, its entropy integral $\kappa(\Pi)$ is finite as well under Assumption \ref{asm:bounded entropy}. 

\medskip

\begin{lemma}\label{lem:entropy_integral_bound}
Under Assumption \ref{asm:bounded entropy}, $\kappa(\Pi) < \infty$.
\end{lemma}

\begin{proof}
See Appendix \ref{app:preliminary_results}.
\end{proof}

\medskip

%\cite{zhou2022offline} give some other discussion for the covering number. For example, for the policy class of finite-depth trees (\cite{Breiman_2017}), $\log N_{d_h}(\epsilon,\Pi_{t})$ is only $O(\log(1/\epsilon))$ (see \citeauthor{zhou2022offline} (\citeyearpar{zhou2022offline}, Lemma 4)). 

The following theorem presents the main result of this paper, showing the rate of convergence for the regret of the DTR $\hat{\pi}$ obtained using the proposed approach.

\medskip

\begin{theorem}\label{thm:main_theorem_backward}
Under Assumptions \ref{asm:sequential independence}--\ref{asm:overlap}, \ref{asm:first-best}, \ref{asm:rate_of_convergence_backward}, and \ref{asm:bounded entropy},
\begin{align*}
    R(\hat{\pi}) = O_{p}\left(\kappa(\Pi) \cdot n^{-1/2}\right) + O_{p} (n^{-\min\{1/2,\tau/2\}}). 
\end{align*}
\end{theorem}

\begin{proof}
See Appendix \ref{app:main_proof}.
\end{proof}

\medskip

This theorem shows the convergence rate of the regret $R(\hat{\pi})$ for the proposed approach. When Assumption \ref{asm:rate_of_convergence_backward} (i) holds with $\tau = 1$, the approach achieves the minimax optimal rate $\kappa(\Pi) \cdot n^{-1/2}$ for the regret convergence.\footnote{In the case of binary treatment assignment at each stage, \cite{Sakaguchi_2021} shows that the minimax optimal rate of convergence for the regret is $V_{1:T}\cdot n^{-1/2}$, where $V_{1:T}$ is the VC-dimension of the class of DTRs.} This result is comparable to those of \cite{Athey_Wager_2020} and \cite{zhou2022offline}, who study doubly robust policy learning in single-stage settings.\footnote{In the single-stage binary treatment setting, \cite{Athey_Wager_2020} use the specific growth rate of the entropy in a VC class and obtain a slightly stronger result compared to using the fixed entropy class.}

In the proof of Theorem \ref{thm:main_theorem_backward}, we consider the derivation of the asymptotic upper bound on $R(\hat{\pi})$. This is, however, a nontrivial task because the stage-specific policies in $\hat{\pi} = (\hat{\pi}_1, \ldots, \hat{\pi}_T)$ are estimated sequentially, rather than simultaneously. If the DTR is estimated simultaneously across all stages, we can apply the theoretical analysis of \cite{zhou2022offline} with some modifications. However, since $\hat{\pi}$ is estimated sequentially, we cannot directly apply their approach. While the sequential estimation complicates the analysis of $R(\hat{\pi})$, Appendix \ref{app:main_proof} introduces new analytical strategies for evaluating the regret of the sequentially estimated DTR.

%We conclude this section with a remark comparing the proposed approach to an existing doubly-robust approach, which simultaneously optimizes a DTR across all stages.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Existing Approach}\label{sec:existing_approach}

An alternative doubly robust approach for estimating the optimal DTR is maximizing an AIPW estimator of the welfare function over the entire class of DTRs.  Specifically, we have 
\begin{align}
    \hat{\pi}^{AIPW} = \argmax_{\pi \in \Pi} & \widehat{W}^{AIPW}(\pi) \mbox{\ \ with} \label{eq:simultaneous_maximization}\\
    \widehat{W}^{AIPW}(\pi) = \frac{1}{n}\sum_{i=1}^{n}\sum_{t=1}^{T}&\left(\hat{\psi}_{it}^{-k(i)}\left(\underline{\pi}_{t}\right)\cdot Y_{it} \right. \nonumber \\
      - &\left.\left(\hat{\psi}_{it}^{-k(i)}\left(\underline{\pi}_{t}\right)- \hat{\psi}_{i,t-1}^{-k(i)}\left(\underline{\pi}_{t-1}\right)\right)\cdot \widehat{Q}_{t}^{\pi_{(t+1):T},-k(i)}\left(H_{it},\pi_t(H_{it})\right)\right) \nonumber
\end{align}
and $\hat{\psi}_{it}^{-k(i)}(\underline{\pi}_{t}) \equiv \left(\prod_{s=1}^{t}1\left\{ A_{is}=\pi_{s}(H_{is})\right\}\right)/(\prod_{s=1}^{t}\hat{e}_{t}^{-k(i)}\left(H_{is},A_{is}\right))$, where $\widehat{W}^{AIPW}(\pi)$ is an AIPW estimator of $W(\pi)$. This approach is first proposed by \cite{Zhang_et_2013} without using cross-fitting, but they did not show its statistical properties.\footnote{\cite{Jiang_Li_2016}, \cite{Thomas_et_al_2016}, and \cite{Kallus_Uehara_2020} propose AIPW estimators of welfare functions for evaluating fixed DTRs, but their focus is not on optimizing DTRs.}
One advantage of this method is that it does not require the correct specification of $\Pi$ (Assumption \ref{asm:first-best}) for consistent estimation of the optimal DTR.

However, this approach faces two computational challenges. First, since the nuisance components $\{Q_{t}^{\pi_{(t+1):T}}\}_{t=1,\ldots,T}$ depend on each specific DTR $\pi$, we basically need to estimate $\{Q_{t}^{\pi_{(t+1):T}}\}_{t=1,\ldots,T}$ for every DTR $\pi$ to implement this method. \cite{Nie_et_al_2021} highlight this computational issue.
Second, maximizing $\widehat{W}^{AIPW}(\pi)$ simultaneously across all stages is computationally difficult, particularly when $T$ is not very small. These computational issues render the approach intractable unless the class of DTRs is small (e.g., $\Pi$ consisting of a small number of DTRs).
By contrast, our proposed approach has two key advantages: (i) the nuisance components $\{Q_{t}^{\hat{\pi}_{(t+1):T}}\}_{t=1,\ldots,T}$ depend only on the estimated policies $\hat{\pi}_{(t+1)}$ from the earlier steps of the backward optimization, and (ii) the backward optimization is computationally much more efficient than the simultaneous optimization across all stages.

Though the simultaneous maximization approach (\ref{eq:simultaneous_maximization}) is not our primary proposal, we show its statistical properties as follows.%\footnote{\cite{Zhang_et_2013} did not show statistical properties of their proposed approach.}


\medskip

\begin{theorem}\label{thm:theorem_simlutaneous}
Under Assumptions \ref{asm:sequential independence}--\ref{asm:overlap}, \ref{asm:rate_of_convergence_backward}, and \ref{asm:bounded entropy},
\begin{align}
    R(\hat{\pi}^{AIPW}) = O_{p}\left(\kappa(\Pi) \cdot n^{-1/2}\right) + O_{p} (n^{-\min\{1/2,\tau/2\}}). \label{eq:theorem_result_AIPW}
\end{align}
\end{theorem}

\begin{proof}
See Appendix \ref{app:statistical_property_AIPW}.
\end{proof}

\medskip

This theorem shows that $\hat{\pi}^{AIPW}$ attains the same convergence rate for regret as our proposed sequentially estimated DTR, $\hat{\pi}$, under the same conditions regarding the MSE convergence rate of the nuisance component estimators and the complexity of the DTR class. Since the simultaneous optimization approach does not require the correct specification of the DTR class, Theorem \ref{thm:theorem_simlutaneous} does not rely on Assumption \ref{asm:first-best}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Simulation Study}\label{sec:simulation}


We conduct a simulation study to examine the finite sample performance of the approach presented in Section \ref{sec:doubly_robust_policy_learning}.
We consider two data generating processes (DGPs), labeled DGP1 and DGP2, each of which consists of two stages of binary treatment assignment $(A_{1},A_{2}) \in \{0,1\}^2$, associated potential outcomes $\left\{Y_2\left(a_{1},a_{2}\right)\right\}_{\left\{ a_{1},a_{2}\right\} \in\left\{ 0,1\right\} ^{2}}$ for the second stage,
$20$ state variables $(S_{1}^{(1)},\ldots,S_{1}^{(20)})$ observed at the first stage, and one state variable $S_2$ observed at the second stage. Each DGP is structured as follows:
\begin{align*}
&(S_{1}^{(1)},\ldots,S_{1}^{(20)})^{\prime} \thicksim N(\boldsymbol{0},I_{20});\\
&S_{2}\left(a_{1}\right)=  \sign\left(S_{1}^{(1)}\right)\cdot a_{1}+ S_{1}^{(2)} + \left(S_{1}^{(3)}\right)^2 + S_{1}^{(4)} + \varepsilon_{1} \mbox{\ with\ }\varepsilon_{1} \sim N(0,1);\\
&Y_{2}\left(a_{1},a_{2}\right) =  \phi(a_1,S_2(a_1))\cdot a_{2} 
    + 0.5 \cdot S_{2}(a_1) + S_{1}^{(4)} - \left(S_{1}^{(5)}\right)^2 + S_{1}^{(6)} + \varepsilon_{2} \mbox{\ with\ } \varepsilon_{2} \sim N(0,1);\\
&A_{1} \thicksim Ber\left(1/(1+e^{0.5 S_{1}^{(2)}-0.5S_{1}^{(3)}-S_{1}^{(5)}})\right),\ A_{2} \thicksim Ber\left(1/(1+e^{0.5S_{1}^{(5)}+0.5 S_{2}-0.2A_{1}})\right).
\end{align*}
In DGPs 1 and 2, we specify $\phi(a_1,S_2(a_1))$ as 
\begin{align*}
    \phi(a_1,S_2(a_1)) &= \sign(S_2(a_1)\cdot (a_1 - 1/2)) \ \ \mbox{and}\\
    \phi(a_1,S_2(a_1)) &= S_2(a_1) + (a_1 - 1/2),
\end{align*}
respectively. In each DGP, the first-stage treatment $a_1$ influences the outcome $Y_2$ through both direct and indirect channels: (i) a direct effect on $Y_2$ via treatment effect heterogeneity $Y_2(a_1,1)-Y_2(a_1,0)$ and (ii) an indirect effect on $Y_2$ via the second-stage state variable $S_2(a_1)$. 

We compare the performance of the approach proposed in Section \ref{sec:doubly_robust_policy_learning} (labeled ``DR'') with those of three existing methods:  Q-learning without and with policy search (labeled ``Q-learn'' and ``Q-search,'' respectively) and the IPW classification-based approach with backward optimization (labeled ``IPW'').\footnote{Following \cite{Murphy_2005}, we separately consider Q-learning with and without policy search. In Q-learning with policy search, the optimal policy for each stage $t$ is chosen from a pre-specified policy class $\Pi_t$; specifically, the optimal policy is estimated as $\hat{\pi}_{t} = \argmax_{\pi_{t}\in \Pi_{t}}\sum_{i=1}^{n}\widehat{Q}_{t}^{\hat{\pi}_{(t+1):T}}(H_{it},\pi_{t})$. This approach, for example, is used by \cite{Zhang_et_al_2018} with a class of list-form policies for $\Pi_t$. In contrast, Q-learning without policy search optimizes the policy for each stage $t$ over all measurable policies, such as $\hat{\pi}_{t}(h_t) = \argmax_{a_{t}\in \MA_{t}}\sum_{i=1}^{n}\widehat{Q}_{t}^{\hat{\pi}_{(t+1):T}}(h_{t},a_{t})$ for any $h_t$. This approach consistently estimates the first-best DTR unless the Q-functions are not misspecified.}
For each method, we use generalized random forests \citep{Athey_et_al_2019} to estimate nuisance components. We set $K=5$ for cross-fitting in the proposed approach.



For DR, Q-search, and IPW, we use a class of DTRs $\Pi=\Pi_{1} \times \Pi_{2}$ with $\Pi_1$ being the class of depth-1 decision trees of $H_1$, and $\Pi_2$ being the class of depth-2 decision trees of $H_2$. In DGP1, $\Pi_2$ is correctly specified in the sense of Assumption \ref{asm:first-best}, whereas in DGP2, $\Pi_2$ is misspecified, leading to a loss of optimality in backward optimization. Note that Q-learn consistently estimates the optimal DTR in both DGPs. We solve the optimization problems involving decision trees using the exact learning algorithm for decision trees proposed by \cite{zhou2022offline}.

\smallskip

\begin{center}
[Table~\ref{table:simulatin_results} about here]
\end{center}

\smallskip

Table \ref{table:simulatin_results} presents the results of 500 simulations with sample sizes of $n = 250$, $500$, $1000$, $2000$, and $4000$. In each simulation, welfare is calculated using a test sample of 50,000 observations randomly drawn from the same DGP. The results show that DR consistently outperforms the other methods for both DGPs and all sample sizes in terms of mean welfare. For example, in DGP1, with a small sample size ($n=500$), DR achieves over 40\% higher welfare than all other methods. Notably, DR surpasses Q-learn even in DGP2, where DR does not consistently estimate the optimal DTR, but Q-learn does. %DR also generally exhibits smaller standard deviations; however, for small sample sizes, its standard deviation is higher than that of Q-learn, likely due to the additional uncertainty introduced by the policy search procedure. %As sample size increases, DR achieves a standard deviation equal to or smaller than that of Q-learning.deviation equal to or smaller than that of Q-learn.



%For each of the proposed and IPW-based approach using the classes $\Pi_1$ and $\Pi_2$ of linear treatment rules, the optimization problem at each step can be formulated as Mixed Integer Linear Programming (MILP) problems (see \citeauthor{Sakaguchi_2021} (\citeyear{Sakaguchi_2021}, Appendix B)), for which some efficient softwares (e.g., CPLEX; Gurobi) are available.


%Table ?? shows the results of 500 simulations with sample sizes $n=200$, $500$, $1000$, $1500$, and $2000$, where we calculate the mean welfare achieved by each estimated DTR with 2,0000 observations randomly drawn from the same DGP.
%Figure \ref{fig:simulation_results} shows the results of 250 simulations. The results show that the B-DR outperforms IPW and Q-learn in terms of the mean welfare, especially when the sample size is not large. The proposed method can achieve a mean welfare higher than that of IPW and Q-learn in finite sample scenarios.

\medskip




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Empirical Application}\label{sec:empirical_illustration}

We apply the proposed approach to data from Project STAR 
\citep[e.g.,][]{krueger_1999, Ding_Lehrer_2010}, 
where we study the optimal allocation of students to regular-size classes (22 to 25 students per teacher) with a full-time teacher aide and small-size classes (13 to 17 students per teacher) without one in their early education (kindergarten and grade 1).\footnote{\cite{krueger_1999} reports that the presence of a teacher aide did not have a significant impact on student test scores. However, whether teaching aides have effects on academic attainment has not been examined by accounting for multiple stages of treatment and treatment effect heterogeneity.} We use a dataset of 1,877 students who were not assigned to regular-size classes without a full-time teacher aide in kindergarten.\footnote{We exclude regular-size classes without a teacher aide, as they are unlikely to be preferable to either regular-size classes with an aide or small-size classes without one for any student.} Among these students, 702 were randomly assigned to regular-size classes with a teacher aide, whereas the remaining students were assigned to small-size classes without a teacher aide in kindergarten (labeled ``grade K''). Upon their progression to grade 1, the students were intended to be randomly reassigned between the two class types. However, as some students selected the class types themselves, the allocation process was not entirely random (see, e.g., \cite{Ding_Lehrer_2010} for a detailed discussion). Therefore, we consider this empirical task in the observational data setting.

We investigate the optimal allocation of students to the two class types in grades K and 1, based on their socioeconomic backgrounds, educational environment, and intermediate academic achievement. Each student’s academic achievement is measured by their percentile rank on combined reading and mathematics test scores taken at the end of grade 1. The welfare objective for the policymaker is assumed to be maximizing the population average of this academic achievement measure. 

We define the first and second stages ($t=1$ and $t=2$) as grades K and 1, respectively. 
We define the action set $\MA_{t} = \{\text{aide},\text{small}\}$,
where the treatment variable $A_{t}$ is labeled ``aide'' if a student is in a regular-size class with a teacher aide at stage $t$, and ``small'' if the student is in a small-size class. The outcome variable $Y_2$ denotes the percentile rank of the combined reading and mathematics scores at the end of grade 1.
We do not incorporate the first-stage outcome into the objective function $W(\pi)$.

We include seven variables in $H_1 (=S_{1})$: student gender, student ethnicity (White/Asian or other), eligibility for free or reduced-price school lunch, school location type (rural or non-rural), teacher’s degree (bachelor’s or higher), years of teaching experience, and teacher ethnicity (White or other). For the second-stage state variables $S_{2}$, we include three variables: reading, math, and total test scores at the end of kindergarten. Recall that $H_2 = (A_1, S_1, S_2)$.


The class of DTRs $ \Pi = \Pi_1 \times \Pi_2$ is defied as follows. For the policy class $\Pi_1$ associated with kindergarten class allocation, we employ a class of depth-1 trees that may take splitting variables from teacher degree, teacher experience, and school location type. For the policy class $\Pi_2$ associated with class allocation in grade 1, we use a class of depth-2 trees that may take splitting variables from reading, math, and total test scores at the end of kindergarten, as well as the kindergarten class type. Note that we exclude student gender, student ethnicity, and teacher ethnicity as splitting variables, as using such variables for treatment choice would be discriminatory.

\smallskip

\begin{center}
[Figure~\ref{fig:estimated_decision_trees} about here]
\end{center}

\smallskip

Figure \ref{fig:estimated_decision_trees} displays the DTR estimated using the proposed approach. The policy for kindergarten class allocation uses teacher experience to determine class type, indicating that teachers with 19 years of experience or less should be assigned to small-size classes. The policy for grade 1 class allocation uses total and reading test scores at the end of kindergarten to determine each student's class type. For example, following the estimated policy, students with a total test score below 914 are assigned to the small-size class in grade 1.

\smallskip

\begin{center}
[Table~\ref{table:empirical_results} about here]
\end{center}

\smallskip

Table \ref{table:empirical_results} presents an estimate of the welfare contrast of the optimal DTR $\pi^{\ast,opt}$ relative to the uniform assignment of all students to small classes in both grades (i.e., $W(\pi^{\ast,opt}) - E[Y_2(\text{small},\text{small})]$), along with the share of students in each of the four allocation arms. These estimates are obtained using 5-fold cross-validation. The results show that class-type allocation with the optimal DTR improves academic achievement by an average of 1.27\% compared with the uniform allocation of small-size classes. Additionally, under the DTR allocation, around 23\% of students are placed in regular-size classes with a teacher aide in either grade K or 1. Given the higher cost of small-size classes relative to regular-size classes with a teacher aide on a per-student basis \citep{Word_et_al_1990}\footnote{According to \cite{Word_et_al_1990}, adding a full-time aide in Grades K-3 across Tennessee cost approximately 75 million dollars annually, while reducing class sizes by one-third cost around 196 to 205 million dollars per year.}, this finding suggests that allocation based on the DTR can reduce the costs associated with class-size reduction while simultaneously improving students' academic achievement.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{comment}
\section{Conclusion}\label{sec:conclusion}
We studied the statistical learning of the optimal DTR using observational data and developed a doubly robust approach to learning it under the assumption of sequential ignorability. Based on backward induction, the approach learns the optimal DTR sequentially, ensuring computational tractability. Our main result shows that the DTR obtained through this approach achieves a convergence rate of $n^{-1/2}$ for welfare regret under mild conditions on the MSE convergence rate for estimators of the propensity scores and Q-functions. The simulation study confirms that the proposed approach outperforms others in finite sample settings. Applying the proposed approach to Project STAR data, we estimate the optimal DTR for the sequential allocation of students to regular-size classes with a teacher aide and small-size classes without one in early education.
%\end{comment}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\section*{Tables}

\input{tables/table_simulation}

\input{tables/table_empirical}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\section*{Figure}


\medskip

\begin{figure}[h!]
\caption{Estimated DTR for class assignment in grades K and 1} \label{fig:estimated_decision_trees}
\vspace{.5cm}
\begin{minipage}[t]{0.44\hsize}
\begin{center}

{\small  (a) Policy for grade K} \\
\vspace{0.5cm}
\input{figures/depth1_tree_1st.tikz} 

\end{center}
\end{minipage}
\begin{minipage}[t]{0.44\hsize}
\begin{center}
{\small (b) Policy for grade 1} \\
\vspace{.5cm}
\input{figures/depth2_tree_1st.tikz}
\end{center}
\end{minipage}

\vspace{.3cm}
\begin{tablenotes}\footnotesize
\item[] Notes: This figure illustrates the DTR estimated in Section \ref{sec:empirical_illustration}. Panels (a) and (b) show the estimated policy trees for grades K and 1, respectively.
\end{tablenotes}
\end{figure}
\medskip





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\part*{Appendix}

%This appendix provides the proof of Theorem \ref{thm:main_theorem_backward}. 
\setcounter{equation}{0}
\renewcommand{\theequation}{A.\arabic{equation}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Proof of Theorem \ref{thm:main_theorem_backward}}\label{app:main_proof}

%\begin{comment}
This appendix presents the proof of Theorem \ref{thm:main_theorem_backward}, along with some auxiliary lemmas. We aim to derive an asymptotic upper bound on $R(\hat{\pi})$. This is however a non-trivial task because the components of the DTR, $\hat{\pi} = (\hat{\pi}_1, \dots, \hat{\pi}_T)$, are estimated sequentially rather than simultaneously. We hence cannot directly apply the theoretical analysis of \cite{Athey_Wager_2020} and \cite{zhou2022offline}, which study doubly robust policy learning in single-stage settings. In the following, we present an original analysis to derive an asymptotic upper bound on $R(\hat{\pi})$.

We begin by noting that Lemma \ref{lem:optimality_backward_induction} allows us to analyze the welfare regret, $R(\hat{\pi}) = W(\pi^{\ast,opt}) - W(\hat{\pi})$, by evaluating the welfare difference $W(\pi^{\ast,B}) - W(\hat{\pi})$ between the DTR $\pi^{\ast,B}$ derived from the backward optimization in the population problem and the estimated DTR $\hat{\pi}$. This approach simplifies the analysis of $R(\hat{\pi})$.

\medskip


Given the estimated DTR $\hat{\pi}$, we define $R_{t}^{\hat{\pi}_{t:T}}(\pi_t) \equiv V_{t}(\pi_t,\hat{\pi}_{(t+1):T}) - V_{t}(\hat{\pi}_{t:T})$ for any $\pi_t \in \Pi_{t}$ and $t=1,\ldots,T$. $R_{t}^{\hat{\pi}_{t:T}}(\pi_t)$ measures the deviation of the policy $\pi_{t}$ from the sequence of estimated policies $\hat{\pi}_{t:T}$ at stage $t$ with respect to the value function. We denote $R_{T}^{\hat{\pi}_{T:T}}(\pi_T)= V_T(\pi_T) - V_T(\hat{\pi}_T)$ for $t=T$.

The following lemma provides a useful result for analyzing the regret $R(\hat{\pi})$, relating the regret of the entire DTR to the stage-specific regrets.

\medskip

\begin{lemma}\label{lem:helpful_lemma}
Under Assumptions \ref{asm:sequential independence}, \ref{asm:overlap}, and \ref{asm:first-best}, the regret of $\hat{\pi}$ is bounded from above as
\begin{align}
    R(\hat{\pi}) \leq R_{1}^{\hat{\pi}_{1:T}}(\pi_{1}^{\ast,B}) + \sum_{t=2}^{T} \frac{2^{t-2}}{\eta^{t-1}} R_{t}^{\hat{\pi}_{t:T}}(\pi_{t}^{\ast,B}). \label{eq:decomposition_bound}
\end{align}
\end{lemma}

\begin{proof}
See Appendix \ref{app:proofs_of_main_lemmas}.
\end{proof}

\medskip

The result (\ref{eq:decomposition_bound}) enables us to evaluate $R(\hat{\pi})$ through evaluating stage-specific regrets $R_{t}^{\hat{\pi}_{t:T}}(\pi_{t}^{\ast,B})$ ($t=1,\ldots,T$), which is simpler to analyze as we will see.

Given a fixed DTR $\pi=(\pi_1,\ldots,\pi_T)$, we define
\begin{align*}
\widetilde{V}_{i,T}(\pi_{T}) &\equiv \frac{ Y_{i,T} - Q_{T}(H_{i,T},A_{i,T}) }{e_{T}(H_{i,T},A_{i,T})}\cdot 1\{A_{i,T}=\pi_{T}(H_{i,T})\} + Q_{T}(H_{i,T},\pi_{T}(H_{i,T})),\\
\widehat{V}_{i,T}(\pi_T) &\equiv \frac{ Y_{i,T}- \widehat{Q}_{t}^{\pi_{(t+1):T},-k(i)}(H_{i,T},A_{i,T}) }{\hat{e}_{T}^{-k(i)}(H_{i,T},A_{i,T})}\cdot1\{A_{i,T} = \pi_{T}(H_{i,T})\} \\
&+  \widehat{Q}_{t}^{\pi_{(t+1):T},-k(i)}\left(H_{i,T},\pi_{T}(H_{i,T})\right),
\end{align*}
and recursively for $t=T-1,\ldots,1$:
\begin{align*}
\widetilde{V}_{i,t}(\pi_{t:T}) &\equiv \frac{ Y_{i,t} + \widetilde{V}_{i,t+1}(\pi_{(t+1):T}) - Q_{t}^{\pi_{(t+1):T}}(H_{i,t},A_{i,t}) }{e_{t}(H_{i,t},A_{i,t})}\cdot 1\{A_{i,t}=\pi_{t}(H_{i,t})\} \\
&+ Q_{t}^{\pi_{(t+1):T}}(H_{i,t},\pi_{t}(H_{i,t})),\\
\widehat{V}_{i,t}(\pi_{t:T}) &\equiv \frac{Y_{i,t} + \widehat{V}_{i,t+1}(\pi_{(t+1):T}) - \widehat{Q}_{t}^{\pi_{(t+1):T},-k(i)}(H_{i,t},A_{i,t}) }{\hat{e}_{t}^{-k(i)}(H_{i,t},A_{i,t})}\cdot 1\{A_{i,t}=\pi_{t}(H_{i,t})\} \\
&+ \widehat{Q}_{t}^{\pi_{(t+1):T},-k(i)}(H_{i,t},\pi_{t}(H_{i,t})).
\end{align*}
Note that the sample mean $(1/n)\sum_{i=1}^{n}\widetilde{V}_{i,t}\left(\pi_{t:T}\right)$ is an oracle estimate of the policy value function $V_{t}(\pi_{t:T})$ with oracle access to $\{Q_{s}^{\pi_{s:T}}(\cdot,\cdot)\}_{s=t+1,\ldots,T}$ and $\{e_{s}(\cdot,\cdot)\}_{s=t,\ldots,T}$. Lemma \ref{lemma:identification} in Appendix \ref{app:preliminary_results} shows that $(1/n)\sum_{i=1}^{n}\widetilde{V}_{i,t}\left(\pi_{t:T}\right)$ is an unbiased estimator of the policy value $V_{t}(\pi_{t:T})$ under the assumption of sequential ignorability (Assumption \ref{asm:sequential independence}).


Following the analysis of \cite{zhou2022offline}, for each stage $t=1,\ldots,T$, we define the policy value difference function $\Delta_{t}(\cdot;\cdot):\Pi_{t:T} \times \Pi_{t:T} \rightarrow \Real$, the oracle influence difference function $\widetilde{\Delta}_{t}(\cdot;\cdot):\Pi_{t:T} \times \Pi_{t:T} \rightarrow \Real$, and the estimated policy value difference function $\widehat{\Delta}_{t}(\cdot;\cdot):\Pi_{t:T} \times \Pi_{t:T} \rightarrow \Real$, as follows: For $\pi_{t:T}^{a}=(\pi_{t}^{a},\ldots,\pi_{T}^{a}) \in \Pi_{t:T}$ and $\pi_{t:T}^{b}=(\pi_{t}^{b},\ldots,\pi_{T}^{b}) \in \Pi_{t:T}$,
\begin{align}
\Delta_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})&\equiv V_{t}(\pi_{t:T}^{a}) - V_{t}(\pi_{t:T}^{b}), \label{eq:Delta} \\
 \widetilde{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b}) &\equiv \frac{1}{n}\sum_{i=1}^{n} \widetilde{V}_{i,t}\left(\pi_{t:T}^{a}\right) - \frac{1}{n}\sum_{i=1}^{n} \widetilde{V}_{i,t}\left(\pi_{t:T}^{b}\right), \label{eq:Delta_tilde}\\
 \widehat{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b}) &\equiv  \frac{1}{n}\sum_{i=1}^{n} \widehat{V}_{i,t}\left(\pi_{t:T}^{a}\right) -  \frac{1}{n}\sum_{i=1}^{n} \widehat{V}_{i,t}\left(\pi_{t:T}^{b}\right).\nonumber
\end{align}
%Lemma \ref{lemma:identification} in Appendix \ref{app:preliminary_results} shows that $\widetilde{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})$ is an unbiased estimator of the policy value difference function $\Delta_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})$. 
From the definitions, the stage-specific regret $R_{t}^{\hat{\pi}_{t:T}}(\pi_{t}^{\ast,B})$ is expressed as
\begin{align*}
    R_{t}^{\hat{\pi}_{t:T}}(\pi_{t}^{\ast,B}) = \Delta_{t}\left(\pi_{t}^{\ast,B},\hat{\pi}_{(t+1):T};\hat{\pi}_{t:T}\right).
\end{align*}
%Noting that Lemma \ref{lem:helpful_lemma} enables us to evaluate $R(\hat{\pi})$ through evaluating $\Delta_{t}\left(\pi_{t}^{\ast},\hat{\pi}_{(t+1):T};\hat{\pi}_{t:T}\right)$ for each $t$, 

In what follows, we evaluate $R_{t}^{\hat{\pi}_{t:T}}(\pi_{t}^{\ast,B})$ for each $t$. 
A standard argument of the statistical learning theory  \citep[e.g.,][]{Lugosi_2002} gives
\begin{align}
    R_{t}^{\hat{\pi}_{t:T}}(\pi_{t}^{\ast,B})  
    &=\Delta_{t}\left(\pi_{t}^{\ast,B},\hat{\pi}_{(t+1):T};\hat{\pi}_{t:T}\right) 
    \nonumber\\
    &\leq \Delta_{t}\left(\pi_{t}^{\ast,B},\hat{\pi}_{(t+1):T};\hat{\pi}_{t:T}\right) - \widehat{\Delta}_{t}\left(\pi_{t}^{\ast,B},\hat{\pi}_{(t+1):T};\hat{\pi}_{t:T}\right) \nonumber \\
    &\leq 
    \sup_{\pi_{(t+1):T} \in \Pi_{(t+1):T}} \sup_{\pi_{t}^{a},\pi_{t}^{b} \in \Pi_{t}} |\Delta_{t}(\pi_{t}^{a},\pi_{(t+1):T};\pi_{t}^{b},\pi_{(t+1):T}) - \widehat{\Delta}_{t}(\pi_{t}^{a},\pi_{(t+1):T};\pi_{t}^{b},\pi_{(t+1):T})| \nonumber \\
    & \leq  \sup_{\pi_{(t+1):T} \in \Pi_{(t+1):T}} \sup_{\pi_{t}^{a},\pi_{t}^{b} \in \Pi_{t}}|\Delta_{t}(\pi_{t}^{a},\pi_{(t+1):T};\pi_{t}^{b},\pi_{(t+1):T}) - \widetilde{\Delta}_{t}(\pi_{t}^{a},\pi_{(t+1):T};\pi_{t}^{b},\pi_{(t+1):T})| \nonumber \\
    &    + \sup_{\pi_{(t+1):T} \in \Pi_{(t+1):T}} \sup_{\pi_{t}^{a},\pi_{t}^{b} \in \Pi_{t}} |\widehat{\Delta}_{t}(\pi_{t}^{a},\pi_{(t+1):T};\pi_{t}^{b},\pi_{(t+1):T}) - \widetilde{\Delta}_{t}(\pi_{t}^{a},\pi_{(t+1):T};\pi_{t}^{b},\pi_{(t+1):T})|, \label{eq:standard_inequality}
\end{align}
where the first inequality follows because $\hat{\pi}_{t}$ maximizes  $(1/n)\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\hat{\pi}_{(t+1):T}}\left(\pi_{t}(H_{i,t})\right)$ over $\Pi_t$; hence, $\widehat{\Delta}_{t}\left(\pi_{t}^{\ast,B},\hat{\pi}_{(t+1):T};\hat{\pi}_{t:T}\right) \leq 0$. 

We can now evaluate $R_{t}^{\hat{\pi}_{t:T}}(\pi_{t}^{\ast,B})$ through evaluating
\begin{align}
  \sup_{\pi_{(t+1):T} \in \Pi_{(t+1):T}} \sup_{\pi_{t}^{a},\pi_{t}^{b} \in \Pi_{t}} |\Delta_{t}(\pi_{t}^{a},\pi_{(t+1):T};\pi_{t}^{b},\pi_{(t+1):T}) - \widetilde{\Delta}_{t}(\pi_{t}^{a},\pi_{(t+1):T};\pi_{t}^{b},\pi_{(t+1):T})|  
  \label{eq:former}
\end{align}
and 
\begin{align}
    \sup_{\pi_{(t+1):T} \in \Pi_{(t+1):T}} \sup_{\pi_{t}^{a},\pi_{t}^{b} \in \Pi_{t}} |\widehat{\Delta}_{t}(\pi_{t}^{a},\pi_{(t+1):T};\pi_{t}^{b},\pi_{(t+1):T}) - \widetilde{\Delta}_{t}(\pi_{t}^{a},\pi_{(t+1):T};\pi_{t}^{b},\pi_{(t+1):T})|.
    \label{eq:latter}
\end{align}
As for the former, we apply the uniform concentration result of \citeauthor{zhou2022offline} (\citeyear{zhou2022offline}, Lemma 2) for the oracle influence difference function to obtain the following lemma.

\medskip
\begin{lemma}\label{lem:bound_influence_difference_function}
Suppose that Assumptions \ref{asm:sequential independence}, \ref{asm:bounded outcome}, \ref{asm:overlap}, and \ref{asm:bounded entropy} hold. Then, for any stage $t \in \{1,2,\ldots,T\}$ and $\delta \in (0,1)$, with probability at least $1-2\delta$, the following holds:
\begin{align}
    &\sup_{\pi_{(t+1):T} \in \Pi_{(t+1):T}} \sup_{\pi_{t}^{a},\pi_{t}^{b} \in \Pi_{t}} |\Delta_{t}(\pi_{t}^{a},\pi_{(t+1):T};\pi_{t}^{b},\pi_{(t+1):T}) - \widetilde{\Delta}_{t}(\pi_{t}^{a},\pi_{(t+1):T};\pi_{t}^{b},\pi_{(t+1):T})| \nonumber\\
    &\leq \left(54.4 \sqrt{2}\kappa(\Pi_{t:T}) + 435.2 + \sqrt{2 \log \frac{1}{\delta}}\right)\sqrt{\frac{M_{t:T}^{\ast}}{n}}  + o\left(\frac{1}{\sqrt{n}}\right), \label{eq:bound_influence_difference_function}
\end{align}
where $M_{t:T}^{\ast} \equiv M\cdot \left( 1 +  2\eta^{-T+t-1} + \sum_{s=1}^{T-t} 3\eta^{-s}\right) < \infty$. 
%where 
%    $V_{t:T}^{\ast}  \equiv  \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}}E
%    \left[\left(\Gamma_{i}^{\pi_{(t+1):T}^{a}}(\pi_{t}^{a}(H_{i,t})) - \Gamma_{i}^{\pi_{(t+1):T}^{b}}(\pi_{t}^{b}(H_{i,t})) \right)^2\right] < \infty$.
\end{lemma}

\begin{proof}
See Appendix \ref{app:preliminary_results}.
\end{proof}

\medskip

As for the latter (equation (\ref{eq:latter})), by extending the analytical strategy of \cite{Athey_Wager_2020} and \cite{zhou2022offline}, 
which leverages orthogonal moments and cross-fitting, to our sequential multi-stage setting, we obtain the following lemma. 

\medskip

\begin{lemma}\label{lem:asymptotic_estimated_policy_difference_function}
Suppose that Assumptions \ref{asm:sequential independence}, \ref{asm:bounded outcome}, \ref{asm:overlap}, and \ref{asm:rate_of_convergence_backward} hold. Then, for any stage $t \in \{1,2,\ldots,T\}$, the following holds:
\begin{align*}
     &\sup_{\pi_{(t+1):T} \in \Pi_{(t+1):T}} \sup_{\pi_{t}^{a},\pi_{t}^{b} \in \Pi_{t}} |\widehat{\Delta}_{t}(\pi_{t}^{a},\pi_{(t+1):T};\pi_{t}^{b},\pi_{(t+1):T}) - \widetilde{\Delta}_{t}(\pi_{t}^{a},\pi_{(t+1):T};\pi_{t}^{b},\pi_{(t+1):T})|\\
     &= O_{p}(n^{-\min\{1/2,\tau/2\}}).
\end{align*}
\end{lemma}

\begin{proof}
See Appendix \ref{app:proof_of_main_lemma_2}.
\end{proof}

\medskip

\noindent
\textit{Proof of Theorem \ref{thm:main_theorem_backward}.}
Combining the inequality (\ref{eq:standard_inequality}) with Lemmas \ref{lem:bound_influence_difference_function} and \ref{lem:asymptotic_estimated_policy_difference_function}, we obtain 
\begin{align*}
    R_{t}^{\hat{\pi}_{t:T}}(\pi_{t}^{\ast,B}) = O_{p}\left(\kappa(\Pi_{t:T}) \cdot n^{-1/2}\right) + O_{p} (n^{-\min\{1/2,\tau/2\}}) 
\end{align*}
for all $t=1,\ldots,T$. This result proves Theorem \ref{thm:main_theorem_backward} via the inequality (\ref{eq:decomposition_bound}) in Lemma \ref{lem:helpful_lemma}. 
\hfill {\large $\Box$}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

