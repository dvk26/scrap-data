\documentclass[12pt]{article}

\usepackage[dvipdfmx]{graphicx}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathabx}
\usepackage{ascmac}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathrsfs}
\usepackage{stmaryrd}
\usepackage{comment}
\usepackage[breaklinks=true]{hyperref}
\usepackage{breakcites}
\usepackage{latexsym}
\usepackage{cases}
\usepackage{url}
\usepackage{pifont}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{arydshln}
\usepackage{here}
\usepackage{natbib}
\usepackage[english]{babel}
\usepackage{threeparttable}
\usepackage[a4paper]{geometry}
%\geometry{verbose,tmargin=28mm,bmargin=28mm,lmargin=25mm,rmargin=25mm}
\geometry{verbose,tmargin=28mm,bmargin=28mm,lmargin=25mm,rmargin=25mm}
\usepackage{hyperref}
\usepackage{amsthm}
\hypersetup{
bookmarkstype=none,
colorlinks,
linkcolor=blue,
citecolor=black}
\usepackage{prettyref}
\usepackage{natbib}
\usepackage{threeparttable}
\allowdisplaybreaks[1]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{assumption}{Assumption}[section]
\newtheorem{condition}[theorem]{Condition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}

\floatname{algorithm}{Procedure 1}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand\thealgorithm{}

\newcommand{\MF}{\mathcal{F}}
\newcommand{\MA}{\mathcal{A}}
\newcommand{\MG}{\mathcal{G}}
\newcommand{\MZ}{\mathcal{Z}}
\newcommand{\MX}{\mathcal{X}}
\newcommand{\MY}{\{-1,1\}}
\newcommand{\MP}{\mathcal{P}}
\newcommand{\MR}{\mathcal{R}}
\newcommand{\MH}{\mathcal{H}}
\newcommand{\MB}{\mathcal{B}}
\newcommand{\Bk}{\mathbf{k}}
\newcommand{\Bz}{\mathbf{z}}
\newcommand{\Bb}{\mathbf{b}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Natural}{\mathbb{N}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\argmax}{\mathop{\rm arg~max}\limits}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\setstretch{1.7}

\makeatother

\begin{document}
\setstretch{1.2} 
\title{Sequential Learning of Optimal Dynamic Treatment Regimes with Observational Data}
\author{Shosei Sakaguchi\thanks{Faculty of Economics, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan. Email: sakaguchi@e.u-tokyo.ac.jp.} }
\date{\today \bigskip \\ \textit{Preliminary Draft}}

\maketitle

\vspace{-1cm}

\begin{abstract}
\begin{spacing}{1.2}
    We study statistical decisions for dynamic sequential treatment assignment problems. Many public policies and medical interventions involve dynamics in their treatment assignments where treatments are sequentially assigned to individuals across multiple stages, and the effect of treatment at each stage is usually heterogeneous with respect to history of the prior treatments, past outcomes, and observed characteristics. We consider estimation of optimal dynamic treatment regimes (DTRs) that guide the optimal treatment assignment for each individual at each stage based on the individual’s history. We propose sequential doubly-robust learning approach to estimate the optimal DTR using observational data under the sequential ignorability assumption. The approach solves the treatment assignment problem at each stage through backward induction, which leads to computational advantage over existing methods. The approach consistently estimates the optimal DTR if either propensity scores or stage-specific action value functions are correctly specified.
    Using doubly-robust estimators of treatment scores and cross-fitting, the approach can achieve the minimax optimal convergence rate of welfare regret even when nuisance components are nonparametrically estimated.\bigskip \\
\noindent 
\textbf{Keywords:} Dynamic treatment effect, dynamic treatment regime, double/debiased machine learning, policy learning\\
 \textbf{JEL codes:} C22, C44, C54
\end{spacing}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\setstretch{1.3} 


\section{Introduction} \label{sec:introduction}

%Many policies involve dynamics in their treatment assignments. Some policies assign a series of treatments to each individual across multiple stages, e.g., job training programs that consist of multiple stages (e.g., \citet{Lechner_2009,Rodriguez_et_al_2022}). Some policies are characterized by when to start or stop consecutive treatment assignment, e.g., unemployment insurance programs with reduced benefit level after unemployment duration exceeds a certain length (e.g., \citet{Meyer_1995,Kolsrud_et_al_2018}). Examples of dynamic treatment assignment also include sequential medical interventions, educational interventions, and online advertisements, among others. 

Many public policies and medical interventions involve dynamics in their treatment assignments. In public policy, for example, some job training programs assign a series of trainings to each individual across multiple stages (e.g., \citet{Lechner_2009,Rodriguez_et_al_2022}). In clinical medicine, physicians often sequentially assigns medical treatments to each patient depending on the patient's medical history (e.g., \cite{Wang_et_al_2012}).%Examples of dynamic treatment assignment also include educational interventions and online advertisements among others. 

\input{figures/example_tree.tikz}

We consider the setting of sequential treatment assignment (\cite{Robins_1986}) in which treatments are sequentially assigned to each individual across multiple stages. In this setting, the effect of treatment at each stage is usually heterogenous with respect to the past treatments and associated characteristics. Hence an effective design of sequential treatment assignment should account for such treatment effect heterogeneity at each stage. That is, the optimal decision of treatment to an individual at each stage should depend upon his/her accumulated information at the corresponding stage.

This paper studies statistical decision of sequential treatment assignment using data from an observational study. 
We assume that the assumption of sequential ignorability (\citet{Robins_1997}) holds, meaning that the treatment assignment at each stage is independent of potential outcomes conditioning on the history of treatment assignments and observed characteristics. 
Under this assumption, we construct an approach to learn the optimal Dynamic Treatment Regime (DTR) that is the sequence of the stage-specific policies (treatment rules) and guides the optimal treatment assignment for each individual at each stage based on the individual’s history (\citet{Murphy_2003,Chakraborty_Murphy_2014}).
%\footnote{Specifying a class of DTRs can accommodate exogenous policy constraints (e.g., interpretability) by restricting the class of feasible DTRs. Moreover, it can also specify different types of dynamic treatment choice problems, such as start/stop time decision problems, where the interest is in when consecutive treatment assignment should be started or stopped for each individual.}

We propose two step-wise doubly robust (DR) approaches to learn the optimal DTR. % by extending an augmented inverse probability weighting (AIPW) estimator of \cite{Robins_et_al_1994} beyond the static case. 
Each approach sequentially learns the optimal stage-specific policy from the final to first stages through backward induction. At each step of the backward induction, each approach constructs an augmented inverse probability weighting (AIPW) estimator of the policy value function for the corresponding stage. The first approach does so by combing estimates of the propensity scores and conditional mean of the potential outcomes as well as the estimated policies for the future stages. The second approach uses the estimates of the state-action value functions (called Q-function (\citet{Murphy_2005,Sutton_et_al_2018})) rather than the conditional mean of the potential outcomes. Each approach then estimates the optimal policy for the corresponding stage by maximizing the estimated policy value function over the pre-specified class of stage-specific policies. The optimal DTR is estimated as the sequence of the estimated policies for all stages. Throughout the procedure of each approach, we also use the cross-fitting (\citet{chernozhukov_et_al_2018}) to make the estimation of the policy value function and the learning of the optimal policy independent. These approaches have the DR property in the sense that the optimal DTR is consistently estimated if at least one of the propensity scores and the conditional mean of the potential outcome or the Q-function are consistently estimated, respectively. 


%The two approaches are complementary each other. The backward learning approach is computationally efficient; however, consistency of the learning is ensured only when the pre-specified class of DTRs contains the first-best policy that assigns the best treatment to any individual with any history at all stages (except for the first stage). 
%In contrast, the simultaneous learning approach can consistently estimate the optimal DTR on the pre-specified class of DTRs irrespective of the feasibility of the first-best policy, at the cost of computational efficiency.

We evaluate the statistical performance of each approach in terms of the regret that is the welfare loss of an estimated DTR relative to the optimal one. For each approach, our main result shows the rate of convergence of the regret of the resulting DTR in relation to rates of convergences of the estimators of the nuisance components and complexity of the class of DTRs. The result also reveals conditions on the class of DTRs and estimators of the nuisance components under which the DTR obtained by each approach achieves the minimax optimal convergence rate $O_{p}(n^{-1/2})$ of the regret. 
For example, for each approach, if all the nuisance components are estimated with rate $O_{p}(n^{-1/4})$ of convergence in the root-mean-squared error, which is achievable for many machine learning methods under structured assumptions on the nuisance parameters, then the regret of the resulting DTR converges to zero at the rate $O_{p}(n^{-1/2})$. This result is consistent with those of \cite{Athey_Wager_2020} and \cite{zhou2022offline} who study the policy learning in the static case.

%The proposed approach also has computational advantage over existing methods for doubly robust estimation of the optimal DTRs (e.g., \citet{Zhang_et_2013}) for the following two reasons. First, the nuisance components of the proposed approach do not depend on any specific-policies, making the estimation of the nuisance components free from the choice of policies. Second, the approach divides the optimization problem to find the optimal DTR into sequential stage-specific optimization problems. Solving the optimization problem simultaneously over a whole class of feasible DTRs is computationally demanding especially when the class of feasible DTRs is not very simple. Section \ref{sec:existing_methods} gives more detailed discussion on the computational point.

\subsection*{Related Work \label{sec:related literature}}


There is a large literature on the statistical decision of treatment choice, but many works in that literature focus on the static treatment assignment problem; e.g., \cite{Manski_2004}, \cite{Zhao_et_al_2012}, \cite{Kitagawa_Tetenov_2018a}, \cite{Athey_Wager_2020}, \cite{zhou2022offline}, and so on. Among these works, this paper is most relevant to \cite{Athey_Wager_2020} and \cite{zhou2022offline}. They propose DR policy learning using the cross-fitting in the static case, and and show that  $n^{-1/2}$-upper bound on the regret can be achieved even in the observational data setting. This paper seeks to extend their approaches and results to the dynamic treatment setting.%, which is a non-trivial task in the literature as discussed in \cite{Athey_Wager_2020} and \cite{Nie_et_al_2021}). 

This work is related to the literature of estimation of the optimal DTRs.\footnote{ \cite{Laber_et_al_2014}, \cite{Chakraborty_Murphy_2014}, and \cite{Kosorok_et_al_2019} review this literature.} The problem of estimating dynamic sequential decision rules is also called adaptive strategies (\cite{Lavori_et_al_2000}) or the batch offline policy learning in the reinforcement learning literature (\cite{Sutton_et_al_2018}).\footnote{Most works in the literature of the batch offline reinforcement learning assume Markov decision process whereas this work does not. Many dynamic treatment cases in social and biomedical sciences are non-Markovian process.} In terms of the statistical setting, this paper is close to classification-based methods of \cite{Zhao_et_al_2015} and \cite{Sakaguchi_2021} who use propensity weighted outcomes to estimate the value function of a DTR and then estimate the optimal DTR by maximizing it over a pre-specified class of DTRs. \citet{Zhao_et_al_2015} develop methods to estimate the optimal DTR using the Support Vector Machine with propensity weighted outcomes, but their focus are on to use experimental data (i.e., the propensity scores are known). \citet{Sakaguchi_2021} proposes backward and simultaneous estimating approaches using inverse weights of estimated propensity scores in the observational data setting.
These approaches are simple to implement; however, such approach sometimes has problematically high variance and leads to inferior DTRs (see, .e.g., \cite{Doroudi_et_al_2018}). Our approach improves power of propensity weighting type methods by leveraging outcome regression.  
%they have sometimes been observed exhibit problematically high variance, especially in dynamic settings \cite{Devroye_et_al_1996} 

%to estimate the optimal DTRs using inverse weights of estimated propensity scores in the observational data setting. 


%The performance of this approach depends on the performance of the estimators of the propensity scores. If the estimated propoensity scores are not close to the true ones, the resulting DTR can be far from the optimal one.%If the propensity scores are not consistent, the resulting DTR is not consistent to the true one. The rate of convegence of the welfare regret depends on that of the estimated propensity scores as well. 
%Our approach solves this problem using the concept of the double/debiased machine learning (\citet{chernozhukov_et_al_2018}).

The batch offline Q-learning (\citet{Watkins_Dayan_1992}) is a dominant approach for estimating the optimal DTRs (e.g., \citet{Murphy_2005,Moodie_et_al_2012,Zhang_et_al_2018}). This approach sequentially estimates the  Q-function (i.e., optimal state-action value function) for each stage, and then estimates the optimal policy from the final to first stage through backward induction. %\citet{Zhang_et_al_2018} propose estimating the optimal DTR with stage-specific policies having list forms by converting unconstrained policies estimated by the batch offline Q-learning (\cite{Murphy_2005}). 
\cite{Murphy_2005} shows that the performance of the DTR obtained by the Q-learning depends on how accurately the Q-function is estimated. The resulting DTR may lead to much smaller welfare than the optimal one if the estimated Q-functions are not close to the true ones. 
Our approaches also use the regression model for the outcome variable.%; especially, the second approach uses the regression model for the Q-function. 
%Our approach also adopts the Q-learning to estimate the nuisance components. 
However, leveraging the propensity score models in the AIPW estimations of the policy value functions, our approaches improve the robustness and accuracy of the approaches based on the outcome regression.
%However, using the AIPW estimators of the value functions, our approach improves robustness of the Q-learning, and moreover has faster rate of convergence of the welfare regret than the Q-learning methods themselves.%The resulting DTR is not consistent to the optimal one if the Q-function is missspecified. Moreover, the welfare of the resulting DTR poorly converges to the optimal welfare if the estimated Q-functions slowly converge to the true ones. Our approach also uses the Q-learning to estimate the nuisance components. However, our approach is robust to the missspecification of the Q-function if the propoensity scores are correctly specified, and moreover it has faster rate of convergence of the welfare regret than the Q-learning methods themselves.

%Whereas the approach proposed in this paper partly use the Q-learning, the resulting DTR is more robust to the missspecification and faster rate of convergence when being combined with consistent estimators of the propensity scores.
%The Q-learning are also suffer from the missspecification of the stage-specific action value function. Moreover, the welfare regret of the resulting DTR converges slowly to zero if the estimators of the Q-functions converges slowly to the true ones. Our proposed approach also uses methods from the Q-learning to estimate the nuisance components in the welfare function. However, it is robust to the missspecification of the Q-function if the propoensity scores are correctly specified, and moreover it has faster convergence rate of the welfare regret than the Q-learning methods themselves.



%Another but popular approach to estimate the optimal DTRs is using the batch off-line Q-learning (\citet{Robins_1986,Murphy_2005,Zhang_et_al_2018}). 
%The Q-learning estimates the stage-specific action value function (called Q-function) and sequentially estimate the optimal policy from the final to first stage (via dynamic programming). %\citet{Zhang_et_al_2018} propose estimating the optimal DTR with a list form by converting an unconstrained policy estimated by the batch Q-learning (\cite{Murphy_2005}) to the list form of the decision rule at each stage.  Methods from the Q-learning are also suffer from the missspecification of the stage-specific action value function. Moreover, the welfare regret of the resulting DTR converges slowly to zero if the estimators of the Q-functions converges slowly to the true ones. Our proposed approach also uses methods from the Q-learning to estimate the nuisance components in the welfare function. However, it is robust to the missspecification of the Q-function if the propoensity scores are correctly specified, and moreover it has faster convergence rate of the welfare regret than the Q-learning methods themselves.

DR estimations of the optimal DTRs are also proposed by \cite{Zhang_et_2013}, \cite{Wallace_Moodie_2015}, and \cite{Nie_et_al_2021}. \cite{Zhang_et_2013} propose estimating the optimal DTR by maximizing the DR estimator of the welfare function of a DTR over a pre-specified class of DTRs. However, this approach is computationally challenging for two reasons: (i) nuisance components are to be estimated for each specific DTR; (ii) the approach maximizes the estimated welfare function over the whole class of DTRs. Our approach solve these computationally obstacles because (i) nuisance components to be estimated do not depend on any specific DTR and (ii) the optimal DTR is estimated through stage-wise optimization steps rather than simultaneous optimization across all stages. \cite{Wallace_Moodie_2015} develops DR estimation of the optimal DTRs building on the Q-learning and G-estimation (\cite{Robins_2004}). In the problem of learning DTRs for when to start and stop treatment, \cite{Nie_et_al_2021} propose DR learning approach with feasible computation property. %G-estimation (\cite{Robins_2004}) is also a doubly robust approach using propensity scores and treatment-free outcome models. 
\citet{Jiang_Li_2016} and \citet{Thomas_et_al_2016} propose DR evaluation of a fixed DTR, rather than learning of the optimal DTRs.%, and show rates of convergence of the welfare. %\footnote{\cite{Lewis_Sygkanis_2021} and \cite{Bodory_et_al_2022} propose double machine learning methods to estimate dynamic treatment effects.} %Their focus in on the evaluation of a fixed DTR rather than finding the optimal one.

%The problem of optimal dynamic sequential decision rules is also called adaptive strategies or batch off-policy learning in the reinforcement learning (\cite{Sutton_et_al_2018}). The literature on dynamic decision rules are 

%This paper is closely related to \citet{Nie_et_al_2021}. They propose a doubly-robust policy learning approach to learn when to start which treatment in a computationally feasible way. This paper covers wider classes of sequential treatment decisions. 
%\citet{Jiang_Li_2016} and \citet{Thomas_et_al_2016} propose doubly-robust estimators of the welfare function of a fixed DTR, and show these rates of convergence. 
%Whereas their focus on the evaluation of a fixed DTR, our focus in on learning of the optimal DTRs. %\citet{Nie_et_al_2021} argue that the evaluation methods of \citet{Jiang_Li_2016} and \citet{Thomas_et_al_2016} are not immediately applicable to learning of the optimal DTRs in computational feasible ways.
%\cite{Zhang_et_2013} also propose estimating the optimal DTR by maximizing the doubly robust estimator of the welfare function and maximizing it over the pre-specified class of DTRs. However, their method is computationally demanding, especially when the class of DTRs is complex. This is because their method optimizes the welfare function simultaneously over the whole class of DTRs, rather than sequentially across stages, and nuisance components in the welfare function depend on a DTR. We discuss in more detail in Section about \ref{sec:existing_methods} of the proposed to existing methods. 

%In the dynamic treatment framework, \citet{Han_2018} relaxes the sequential randomization assumption, allowing for noncompliance, and studies point identification of the average dynamic treatment effects and optimal non-additive DTR, in which treatment assignment at each stage does not depend on the history. %His identification result depends on the binary instrumental variables and other exogenous variables excluded from the treatment-selection process. 
%\citet{Han_2019} proposes a way to characterize the sharp partial ordering of the counterfactual welfares of DTRs and the sharp set of the optimal DTRs in an instrumental variable setting by using a set of linear programs. 
%He also proposes a computationally attractive approach to derive the sharp partial welfare ordering and set of the optimal DTRs. 
%\citet{Heckman_Navarro_2007} and \citet{Heckman_et_al_2016} use exclusion restrictions to identify the dynamic treatment effect by extending the literature on the marginal treatment effect, but do not study identification of the optimal DTRs.


%Estimation of the optimal DTR has been widely studied in the biostatistics and statistics literatures under the labels of dynamic treatment regime, adaptive strategies, and adaptive interventions. \citet{Chakraborty_Moodie_2013}, \citet{Chakraborty_Murphy_2014}, \citet{Laber_et_al_2014}, and \cite{Tsiatis_et_al_2019} review the developments in this field. There are some dominant approaches: G-estimation (\citet{Robins_1989,Robins_et_al_1992}) and Q-learning (\citet{Murphy_2005,Moodie_et_al_2012}). The G-estimation method specifies the Structural Nested Mean Model (SNMM) that is the stage-specific difference of the conditional mean of outcomes between two treatments. The G-estimation method estimates the parameters of the SNMM, and then predicts the optimal DTR from the estimated model (see, e.g., \citet{Murphy_2003}, \citet{Vansteelandt_Goetghebeur_2003}, and \citet{Robins_2004} for further details about G-estimation). The Q-learning method models the so-called Q-function, a function of the stage-specific expected reward with respect to the current treatment and history given that the optimal treatment is assigned in subsequent stages.
%The Q-learning method estimates the parameters of the Q-function via backward induction, and then estimates the optimal DTR as the sequence of stage-specific treatment assignment rules that maximize the estimated Q-functions. A potential drawback of these approaches is the risk of misspecification of the models relevant to the counterfactual outcomes. In contrast, the DEWM approach does not need to specify any model relevant to the counterfactual outcomes; instead, it specifies the propensity scores, which are known in the experimental data setting.



%Finally, we note that the dynamic framework studied in this paper is different from the framework of the bandit problem studied by, for example, \cite{Kock_Thyrsgaard_2018}. In the bandit problem framework, different individuals arrive at each stage and each of them receives treatment only once. In contrast, in the dynamic framework studied in this paper, the same individuals arrive at each stage and each of them receives multiple treatments across stages.
%We also note that recent works of \citet{Han_2018} and \citet{Han_2019} relax the sequential randomization assumption, by using instrumental variables, and studies point identification of the average dynamic treatment effects and characterizes the sharp partial ordering of the counterfactual welfares of DTRs, respectively. 

\subsection*{Structure of the Paper }
The remainder of the paper is structured as follows. Section \ref{sec:setup} describes the dynamic treatment framework and defines the dynamic treatment choice problem. Section \ref{sec:doubly_robust_policy_learning}
presents the two step-wise approaches to learn the optimal DTRs. Section \ref{sec:statistical_properties} shows statistical properties of each approach. Section \ref{sec:existing_methods} makes comparison of the proposed approaches to existing ones. Section \ref{sec:simulation} presents a simulation study to evaluate their finite sample performances.  Section \ref{sec:conclusion} concludes this paper. Appendix presents proofs of main theorems of this paper and auxiliary lemmas.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Setup \label{sec:setup}}

We introduce the dynamic treatment framework, following Robins’s dynamic counterfactual outcomes framework (\citet{Robins_1986,Robins_1997}), in Section \ref{sec:dynamic treatment framework}. Subsequently, we
define the dynamic treatment choice problem in Section \ref{sec:dynamic treatment choice problem}. 

\subsection{Dynamic Treatment Framework \label{sec:dynamic treatment framework}}

We suppose that there are $T$ $(< \infty)$ stages of multiple treatment assignment.
Let $\MA_{t} \equiv \{a_{t}^{(1)},\ldots,a_{t}^{(d_t)}\}$ ($t=1,\ldots,T$) denote a set of possible treatment arms in stage $t$, where $d_t$ denotes the number of possible treatment arms in stage $t$ and is allowed to vary across stages. Let $A_{t} \in \MA_{t}$ be the treatment in stage $t$. Throughout this paper, for any variable $V_{t}$,
we denote by $\text{\ensuremath{\text{\ensuremath{\underline{V}}}}}_{t}\equiv \left(V_{1},\ldots,V_{t}\right)$
the history of the variable up to stage $t$, and denote by $\text{\ensuremath{\underline{V}}}_{s:t} \equiv \left(V_{s},\ldots,V_{t}\right)$,
for $s\leq t$, the partial history of the variable from stage $s$ up to stage $t$. Let $Y\left(\text{\ensuremath{\underline{a}}}_{T}\right)$ be the potential outcome that is realized when  $\underline{A}_T = \underline{a}_{T}\in \underline{\MA}_{T}$, where $\underline{\MA}_{T}\equiv \MA_{1} \times \cdots \times \MA_{T}$. Depending on the history $\underline{A}_T$ of treatments up to the last stage $T$, we observe an outcome $Y \equiv Y(\underline{A}_{T})$ after the end of the sequential treatment assignment.
Let $X_{t}$ be a vector of covariates that are observed prior to treatment assignment in stage $t$. The distribution of $X_{t}$ may depend on the past treatments and past covariates.\footnote{In line with \cite{Zhang_et_2013} and \cite{Nie_et_al_2021}, we can define potential covariates $X_{t}(\underline{a}_{t-1})$ that is realized when $\underline{A}_{t-1}=\underline{a}_{t-1}$. Then the observed covariates $X_t$ correspond to $X_{t}(\underline{A}_{t-1})$. We here assume that the covariates are not influenced by the future treatment (i.e., no-anticipation assumption). } 
We define the history in stage $t$ by $H_{t}\equiv(\underline{A}_{t-1},\underline{X}_{t})$, which is available information for the policy-maker when she chooses a treatment assignment in stage $t$. %\footnote{We can also define $H_t = (\underline{A}_{t-1},\underline{X}_{t})$. In this case, $H_t$ contains all of the observed past information as well as the current covariates.} 
Let $Z\equiv(\underline{A}_{T},\underline{X}_T,Y)$ be the vector of all the observed variables.
We denote
the support of $H_{t}$ and $Z$ by ${\cal H}_{t}$ and $\MZ$, respectively. Let $P$ denote the distribution of all the defined variables $\left(\underline{A}_T,\underline{X}_T,\{Y(\underline{a}_T)\}_{\underline{a}_T \in \underline{\MA}_T}\right)$.

From an observational study, we observe $Z_{i}\equiv \left(\underline{A}_{i,T},\underline{X}_{i,T},T\right)$ for individuals $i=1,\ldots,n$, where $Y_{i}  \equiv \sum_{\text{\ensuremath{\underline{a}}}_{T}\in\underline{\MA}_{T}}\left(1\left\{ \text{\ensuremath{\underline{A}}}_{i,T}=\text{\ensuremath{\underline{a}}}_{T}\right\}\cdot Y_{i}\left(\text{\ensuremath{\underline{a}}}_{T}\right)\right)$ with $Y_{i}\left(\text{\ensuremath{\underline{a}}}_{T}\right)$ being a
potential outcome for individual $i$ that is realized when $\underline{A}_{i,T} = \underline{a}_{T}$. We suppose that the vectors of random variables $\left(\underline{A}_{i,T},\left\{ Y_{i}\left(\text{\ensuremath{\underline{a}}}_{T}\right)\right\} _{\text{\ensuremath{\underline{a}}}_{T}\in\underline{\MA}_{T}},\underline{X}_{i,T}\right)$,
$i=1,\ldots,n$, are independent and identically distributed (i.i.d) under the distribution $P$. We denote by $P^{n}$ the joint distribution of $\left\{\left(\underline{A}_{i,T},\left\{ Y_{i}\left(\text{\ensuremath{\underline{a}}}_{T}\right)\right\} _{\text{\ensuremath{\underline{a}}}_{T}\in\underline{\MA}_{T}},\underline{X}_{i,T}\right):i=1,\ldots,n\right\}$ generated by $P$.

Define $e_{t}\left(h_{t},a_{t}\right)\equiv \Pr\left(A_{t}=a_{t}\mid H_{t}=h_{t}\right)$, the propensity score of the treatment $a_t$ in stage $t$ given the history $h_t$. In the observational data setting we study, the propensity scores are unknown to the analyst and may need to be estimated. This is contrast to 
the experimental data setting studied by \cite{Zhao_et_al_2015} where the propensity scores are known to the analyst.  %\footnote{Even when the date are from a sequential randomized trial, the propensity scores may be unknown when attrition occurs.}

Throughout the paper, we suppose that the underlying distribution $P$ satisfies the following assumptions.

\bigskip{}

\begin{assumption}[Sequential Ignorability]\label{asm:sequential independence} For any $t=1,\ldots,T$ and $\text{\ensuremath{\underline{a}}}_{T}\in\underline{\MA}_{T}$,
$A_{t}\perp Y\left(\text{\ensuremath{\underline{a}}}_{T}\right)\mid H_{t}$ holds a.s.
\end{assumption}
%\bigskip{}
\begin{assumption}[Bounded Outcomes]\label{asm:bounded outcome}
There exists $M <\infty$ such that the support of $Y(\underline{a}_{T})$ is
contained in $\left[-M/2,M/2\right]$ for all $\underline{a}_{T} \in \underline{\MA}_{T}$.
\end{assumption}

\begin{assumption}[Overlap Condition]\label{asm:overlap} 
There exists $\eta \in (0,1)$ such that $\eta \leq e_{t}(H_{t},A_{t})$ a.s. for all $t \in \{1,\ldots,T\}$. 
\end{assumption}

\bigskip


Assumption \ref{asm:sequential independence} is what is called a dynamic unconfoundedness assumption or sequential ingnorability assumption elsewhere, and is commonly used in the literature of dynamic treatment effect analysis (\cite{Robins_1997}; \cite{Murphy_2003}). This assumption means that the treatment assignment at each stage is independent of the contemporaneous and future potential outcomes conditional on the history up to that point. In observational studies, this assumption is satisfied when a sufficient set of confounders is controlled at each stage. Assumptions \ref{asm:bounded outcome} and \ref{asm:overlap} are standard assumptions in the literature of treatment analysis.

\bigskip

\subsection{Dynamic Treatment Choice Problem\label{sec:dynamic treatment choice problem}}

The aim of this paper is to provide methods to learn the optimal DTRs using data from an observational study. We define a policy at each stage $t$ as $\pi_{t}:\MH_{t}\mapsto \MA_{t} $, a map from the history space at stage $t$ to the treatment space at stage $t$. A policy $\pi_t$ decides which treatment to be assigned to each individual at stage $t$ based on the history $H_t$. We then define the DTR by $\pi \equiv\left(\pi_{1},\ldots,\pi_{T}\right)$,
a sequence of stage-specific policies. The DTR sequentially guides a treatment choice for each individual from the first to last stage depending on the individual’s history up to that point. 

We define the welfare of a DTR $\pi$ by 
\begin{align*}
W\left(\pi\right) &\equiv  E\left[Y(\pi_{1}(H_1),\ldots, \pi_{T}(H_T))\right],
\end{align*}
which is the population mean of the outcome that is realized when the sequential treatments are assigned subject to $\pi$.
We consider to choose a DTR from a pre-specified class of DTRs, which we denote by $\Pi \equiv \Pi_{1}\times\cdots\times \Pi_{T}$, where $\Pi_{t}$ is a class of policies at stage
$t$ (i.e., a pre-specified class of measurable functions $\pi_{t}:\MH_{t}\rightarrow \MA_{t}$). \cite{Zhang_et_al_2018} uses the class of list forms of policies for $\Pi_t$; \cite{Laber_Zhao_2015}, \cite{Tao_et_al_2018}, \cite{Sun_Wang_2021}, and \cite{Blumlein_et_al_2022} use the class of trees for $\Pi_t$. These simple policy classes enhance interpretability of the resulting DTR.% (\cite{Ahmad_et_al_2018}).


The ultimate goal of this paper is to choose an optimal DTR that maximizes the welfare $W\left(\cdot\right)$ over $\Pi$. 
We are especially interested in learning the optimal DTR from observation data which satisfies the sequential ignorability assumption (Assumption \ref{asm:sequential independence}).
The following section introduces sequential approaches to learn the optimal DTR from the observational data.

%\bigskip

\section{Sequential Learning of the Optimal DTR}\label{sec:doubly_robust_policy_learning}

We discuss two sequential approaches to learn the optimal DTRs through backward-induction (dynamic programming). Given a DTR $\pi$ and the class $\Pi$ of DTRs, we denote their partial sequence by $\pi_{s:t}:=(\pi_s,\ldots,\pi_t)$ and $\pi_{s:t}:=\Pi_s \times \cdots \times \Pi_t$, respectively, for $s\leq t$.\footnote{Throughout the paper, for any object $v_{s:t}$ and $\underline{w}_{s:t}$ ($s\leq t$), $v_{t:t}$ and $\underline{w}_{t:t}$ correspond to $v_t$ and $w_t$, respectively.} For each stage $t$, we define the policy value function of $\pi_{t:T}$ as 
\begin{align*}
  V_t(\pi_{t:T})\equiv E\left[Y\left(\underline{A}_{1:(t-1)},\pi_{t}(H_t),\ldots,\pi_T(H_Y)\right)\right]  
\end{align*}
and the action-value function of $a_t$ for $\pi_{(t+1):T}$ as 
\begin{align*}
    V_{t}^{\pi_{(t+1):T}}(a_{t})\equiv E\left[Y\left(\underline{A}_{1:(t-1)},a_{t},\pi_{t+1}(H_{t+1}),\ldots,\pi_T(H_T)\right)\right].
\end{align*}
 We consider to sequentially estimate the optimal DTR by constructing AIPW estimators of the policy value functions and using cross-fitting. %It is non-trivial to construct AIPW estimators in the dynamic setting, especially when the optimal DTRs are sequentially estimated through backward induction. 
In what follows, for any function $f(\cdot,\cdot):\MH_{t} \times \MA_{t} \rightarrow \Real$ and policy $\pi_{t}(\cdot):\MH_{t} \rightarrow \MA_{t}$, we denote $f(h_{t},\pi_{t}(h_{t}))$ shortly by $f(h_{t},\pi_{t})$ (e.g., $e_{t}(h_t,\pi_t)=e_{t}(h_t,\pi_t(h_t))$).


%\subsection{Backward Policy Learning\label{sec:backward_policy_learning}}

To sequentially identify and estimate the optimal DTRs through backward-induction, we suppose that the policy class $\Pi_t$ for each stage $t=2,\ldots,T$ contains the first-best policy in the following sense.

\bigskip

\begin{assumption}[First-Best Policy] 
\label{asm:first-best} 
For any $t=2,\ldots,T$, there exists $\pi_{t}^{\ast,FB}\in \Pi_{t}$ such that 
\begin{align*}
&E\left[Y\left(\underline{A}_{1:(t-1)},\pi_{t}^{\ast,FB}(H_{t}),\ldots,\pi_{T}^{\ast,FB}(H_{T})\right) \middle| H_{t}\right] \\
&\geq E\left[Y\left(\underline{A}_{1:(t-1)},\pi_{t}(H_{t}),\pi_{t+1}^{\ast,FB}(H_{t+1}),\ldots,\pi_{T}^{\ast,FB}(H_{T})\right) \middle| H_{t}\right]
\end{align*}
holds a.s. for all $\pi_{t} \in \Pi_{t}$. 
\end{assumption}

\bigskip

We call $\pi_{t}^{\ast,FB}$ that satisfies Assumption \ref{asm:first-best} the first-best policy in stage $t$. The first-best policy $\pi_{t}^{\ast,FB}$ always chooses the best treatment arm for any history $h_t$ given that the first-best policies are followed in the future stages.  Assumption \ref{asm:first-best} is satisfied when $\Pi_t$ ($t=2,\ldots,T$) are flexible enough or correctly specified. 

\cite{Sakaguchi_2021} illustrates how the backward induction approach does not work to estimate the optimal DTRs when the fist-best policy is not available in the supposed policy classes.
\cite{Zhang_et_2013} discuss how to correctly specify $\Pi$ so as to satisfy Assumption \ref{asm:first-best} depending on models relevant to the treatment effect heterogeneity. \cite{Zhao_et_al_2015} use a class of policies represented by a reproducing kernel Hilbert space for each $\Pi_t$, which is flexible enough to approximate the first-best policy. Note that the policy class $\Pi_1$ for the first stage does not need to contain the first-best policy. 

We will present two sequential learning approaches of the optimal DTRs in the following two subsections. 

%Let $S_{t} \equiv \underline{X}_{t}$ be a history of the observed covariates up to stage $t$. 
%Define $Q(\underline{x}_{T},\underline{a}_{T}) \equiv E\left[Y\mid \underline{X}_{T}= \underline{x}_{T}, \underline{A}_{T}=\underline{a}_{T}\right]$. This as well as $\{e_{t}(\cdot,\cdot)\}_{t=1}^{T}$ are the nuisance components to be estimated in our leaning approach, as we sill see. We can estimate $Q(\underline{x}_{T},\underline{a}_{T})$  by regressing $Y$ on $(\underline{X}_{T},\underline{A}_{T})$ to obtain $\widehat{Q}_t(\underline{x}_{T},\underline{a}_{T})$ as the regression function with $(\underline{x}_{T},\underline{a}_{T})$. Any semi/nonparametric regression methods can be used. Note that $E\left[Q_t(\underline{X}_{t:T},\underline{a}_{t:T})\right]=E\left[Y(\underline{A}_{1:(t-1)},\underline{a}_{t:T}) \right]$ holds under Assumption \ref{asm:sequential independence}.


%\begin{comment}

\subsection{First Approach}\label{seq:first_approach}

For each stage $t$, we define $Q_t(\cdot,\cdot):\MH_t \times \underline{\MA}_{t:T}\rightarrow \Real$ as
\begin{align*}
    Q_t(h_t,\underline{a}_{t:T}) \equiv E\left[Y(\underline{A}_{1:(t-1)},\underline{a}_{t:T}) \mid H_{t}=h_{t}\right].
\end{align*}
$Q_t(h_t,\underline{a}_{t:T})$ is the mean of the potential outcome of the sequence of treatments $\underline{a}_{t:T}$ conditional on the history $h_t$ in stage $t$. 
%Abusing terminology somewhat, we refer to $Q_{t}(h_t,\underline{a}_{t:T})$ as the conditional treatment effect of $a_{t:T}$ given $h_t$ in stage $t$.
As we will see, $\{Q_t(\cdot,\cdot)\}_{t=1}^{T}$ and $\{e_{t}(\cdot,\cdot)\}_{t=1}^{T}$ are nuisance components to be estimated in the approach presented in this subsection. \par
Several methods to estimate $Q_{t}(h_t,\underline{a}_{t:T})$ under Assumption \ref{asm:sequential independence} have been proposed (see, e.g., \citeauthor{Tsiatis_et_al_2019} (\citeyear{Tsiatis_et_al_2019}, Ch. 5.5)). %(e.g., G-estimation \cite{Robins_2004}). %Under Assumption \ref{asm:sequential independence}, $\{Q_t(\cdot,\underline{a}_{t:T})\}_{t=1}^{T}$ (with a fixed $\underline{a}_{t:T}$) can be estimated in the following sequential way: 
In this paper, we employ the following recursive approach to estimate $\{Q_t(\cdot;\underline{a}_{t:T})\}_{t=1}^{T}$ (with a fixed $\underline{a}_{T}$):
\begin{itemize}
    \item Regress $Y$ on $(H_{T},A_{T})$ to obtain $\widehat{Q}_{T}(\cdot;a_{T})$ as the estimated regression function with $A_{T}=a_{T}$;
    \item Recursively, for $t=T-1,\ldots,1$, regress $\widehat{Q}_{t+1}(H_{t+1},\underline{a}_{(t+1):T})$ on $(H_t,A_t)$ to obtain $\widehat{Q}_{t}(\cdot;\underline{a}_{t:T})$ as the estimated regression function with $A_{t}=a_{t}$.\footnote{This sequential regression approach comes from the following nested expectation property: 
    \begin{align*}
    E\left[Q_{t+1}\left(H_{t+1},\underline{a}_{(t+1):T}\right)\middle|A_{t}=a_t,H_{t}\right]
    &= E\left[E\left[Y\left(\underline{A}_{1:t},\underline{a}_{(t+1):T}\right)\middle|H_{t+1}\right]\middle|A_{t}=a_{t},H_{t}\right]\\
    &= E\left[Y\left(\underline{A}_{1:(t-1)},\underline{a}_{t:T}\right)\middle|A_{t}=a_t,H_{t}\right]\\
    &=E\left[Y\left(\underline{A}_{1:(t-1)},\underline{a}_{t:T}\right)\middle|H_{t}\right]\\
    &=
    Q_{t}\left(H_{t},\underline{a}_{t:T}\right)\middle|H_{t},    
    \end{align*}
    where the third equality folows from Assumption \ref{asm:sequential independence}.
}
\end{itemize}
We can apply a flexible regression method (e.g., random forest, lasso, neural network learning) in the regression at each step. 
%\end{comment}

Following the doubly robust policy learning of \cite{Athey_Wager_2020} and \cite{zhou2022offline}, we employ the cross-fitting (\citet{Schick_1986,chernozhukov_et_al_2018}) to make the estimation of the policy value function and learning of the optimal policy independent; whereby, to reduce the over-fitting. We randomly divide the data set $\{Z_i\}_{i=1,\ldots,n}$ into $K$ evenly-sized folds (e.g., $K=5$). Let $I_k$ be a set of indices of the data in the $k$-th fold and $I_{-k}$ be a set of indices of the data excluded from the $k$-th fold. In what follows, for any statistics $\hat{f}$, we denote by $\hat{f}^{-k}$ the corresponding statistics calculated using data not contained in the $k$-th fold.

The proposed approach consists of multiple steps. As a preliminary step, we estimate the propensity scores $\{e_t(\cdot,\cdot)\}_{t=1,\ldots,T}$ and conditional means of the potential outcome $\{Q_t(\cdot,\cdot)\}_{t=1,\ldots,T}$ by using the data excluded in each fold. For each index $k$ of the fold, we denote by $\hat{e}_{t}^{-k}(\cdot,\cdot)$ and $\widehat{Q}_{t}^{-k}(\cdot,\cdot)$, respectively, the estimators of $e_t(\cdot,\cdot)$ and $Q_t(\cdot,\cdot)$ using data not contained in the $k$-th fold. Flexible regression methods (e.g., random forest, lasso, neural network learning) can be applied to estimate $e_t(\cdot,\cdot)$ and $Q_{t}(\cdot,\cdot)$. %Remark \ref{rem:Q-function_estimate} gives some comments regarding the Q-learning.%Given $\widehat{Q}_{t}^{-k}(\cdot,\underline{a}_{t:T})$, we also define and compute $\hat{V}_{t}^{-k}(\cdot) := \max_{a_t \in \MA_t}\widehat{Q}_{t}^{-k}(\cdot,\underline{a}_{t:T})$, which is an estimate of the optimal state-value function $V_t(\cdot) := \max_{a_t \in \MA_t}Q_{t}(\cdot,a_t)$.

Given the estimators $\{\hat{e}_{t}^{-k}(\cdot,\cdot)\}_{t=1}^{T}$ and  $\{\widehat{Q}_{t}^{-k}(\cdot,\cdot)\}_{t=1}^{T}$ for each $k=1,\ldots,K$, the optimal DTR is estimated in the following sequential way. In the first step, regarding the last stage $T$, we make the score function of the treatment $a_T$ in stage $T$ as follows: 
\begin{align*}
    \widehat{\Gamma}_{i,T}(a_T) \equiv \frac{Y_{i} - \widehat{Q}_{T}^{-k(i)}(H_{i,T},A_{i,T})}{\hat{e}_{T}^{-k(i)}(H_{i,T},A_{i,T})}\cdot 1\{A_{i,T}=a_T\} +
    \widehat{Q}_{T}^{-k(i)}(H_{i,t},a_{T}),
\end{align*}
where $k(i)$ denotes the fold that contains the $i$-th observation. Note that its sample mean $(1/n)\sum_{i=1}^{n}\widehat{\Gamma}_{i,T}(a_T)$ is an AIPW estimator of $E\left[Y_{T}\left(\underline{A}_{1:(T-1)},a_{T}\right)\right]$, implying that $(1/n)\sum_{i=1}^{n}\widehat{\Gamma}_{i,T}(a_T)$ consistently estimates $E\left[Y_{T}\left(\underline{A}_{1:(T-1)},a_{T}\right)\right]$ if either $\widehat{Q}_{T}^{-k(i)}$ or $\hat{e}_{T}^{-k(i)}$ is consistent.

We then find the best candidate policy in stage $T$ by solving
\begin{align}
\check{\pi}_{T} \in \argmax_{\pi_{T}\in \Pi_{T}}\frac{1}{n}\sum_{i=1}^{n}\widehat{\Gamma}_{i,T}^{\dag}\left(\pi_T(H_{i,T})\right), \label{eq:policy_search_T}
\end{align}
where the objective function $(1/n)\sum_{i=1}^{n}\widehat{\Gamma}_{i,T}^{\dag}\left(\pi_T(H_{i,T})\right)$ is an AIPW estimator of the policy value function $V_{T}(\pi_T)$.

In the following step, we consider stage $T-1$. Given $\check{\pi}_{T}$, 
we make the score function of $a_{T-1}$ as
\begin{align*}
    \widehat{\Gamma}_{i,T-1}^{\check{\pi}_{T}}(a_{T-1})  &\equiv \frac{ \left(\widehat{\Gamma}_{i,T}^{\dag}\left(\check{\pi}_{T}(H_{i,T})\right) - \widehat{Q}_{T-1}^{-k(i)}\left(H_{i,T-1},A_{i,T-1},\check{\pi}_{T}(H_{i,T})\right)\right) }{\hat{e}_{T-1}^{-k(i)}(H_{i,T-1},A_{i,T-1})} \cdot 1\{A_{i,T-1} = a_{T-1}\} \\
    &+ 
\widehat{Q}_{T-1}^{-k(i)}\left(H_{i,T-1},a_{T-1},\check{\pi}_{T}(H_{i,T})\right).
\end{align*}
Its sample mean $(1/n)\sum_{i=1}^{n}\widehat{\Gamma}_{i,T-1}^{\check{\pi}_{T}}(a_{T-1})$ is an AIPW estimator of the action value function $V_{T-1}^{\check{\pi}_T}(a_{T-1})$.
We then find the best candidate policy in stage $T-1$ by solving
\begin{align*}
    \check{\pi}_{T-1} \in \argmax_{\pi_{T-1}\in \Pi_{T-1}} \frac{1}{n}\sum_{i=1}^{n}\widehat{\Gamma}_{i,T-1}^{\check{\pi}_T}\left(\pi_{T-1}(H_{i,T-1})\right),
\end{align*}
where the objective function $(1/n)\sum_{i=1}^{n}\widehat{\Gamma}_{i,T-1}^{\check{\pi}_{T}}\left(\pi_{T-1}(H_{i,T-1})\right)$ is an AIPW estimator of the policy value $V_{T-1}(\pi_{T-1},\check{\pi}_{T})$.

Recursively, for $t=T-2,\ldots,1$, we learn the optimal policy $\pi_{t}^{\ast}$ as follows.
We first make the score function of $a_t$ as 
\begin{align}
    \widehat{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}(a_t) &\equiv \frac{ \left(\widehat{\Gamma}_{i,t+1}^{\check{\pi}_{(t+2):T}}\left(\check{\pi}_{t+1}(H_{i,t+1})\right)- \widehat{Q}_{t}^{-k(i)}(H_{i,t},A_{i,t},\check{\pi}_{t+1}(H_{i,t+1}),\ldots,\check{\pi}_{T}(H_{i,T}))\right) }{\hat{e}_{t}^{-k(i)}(H_{i,t},A_{i,t})}\nonumber \\ 
    &\times 1\{A_{i,t} = a_t\} + 
    \widehat{Q}_{t}^{-k(i)}\left(H_{i,t},a_{t},\check{\pi}_{t+1}(H_{i,t+1}),\ldots,\check{\pi}_{T}(H_{i,T})\right). \label{eq:score_function}
\end{align}
We then find the best candidate policy in stage $t$ by solving
\begin{align*}
    \check{\pi}_{t} \in \argmax_{\pi_{t}\in \Pi_{t}} \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}\left(\pi_{t}(H_{i,t})\right),
\end{align*}
where the objective function $(1/n)\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}\left(\pi_{t}(H_{i,t})\right)$ is an AIPW estimator of the policy value $V_{t}\left(\pi_{t},\check{\pi}_{t+1},\ldots,\check{\pi}_{T}\right)$.

Throughout this procedure, we eventually obtain the sequence $\check{\pi} \equiv (\check{\pi}_{1},\ldots,\check{\pi}_{T})$, which is the resulting estimator of the DTR. We will clarify statistical properties of $\check{\pi}$ with respect to its welfare regret in Section \ref{sec:statistical_properties}. The following remark gives some discussion about the definition of the score function (\ref{eq:score_function}).

%\begin{comment}
\bigskip

\begin{remark}
%The score function (\ref{eq:score_function}) can be written as 
\begin{comment}
\begin{align*}
    &\widehat{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}(a_{t})\\
    =&
	\frac{Y_{i}-\widehat{Q}_{T}^{-k(i)}(H_{i,T},A_{i,T})}{\prod_{s=t}^{T}\hat{e}_{s}^{-k(i)}(H_{i,s},A_{i,s})}\cdot\prod_{s=t+1}^{T}1\{A_{i,s}=\check{\pi}_{s}(H_{i,s})\}\cdot1\{A_{i,t}=a_{t}\}\\
	+&\sum_{s=t}^{T-1}\frac{\prod_{\ell=t+1}^{s}1\{A_{i,\ell}=\check{\pi}_{\ell}(H_{i,\ell})\}\cdot1\{A_{i,t}=a_{t}\}}{\prod_{\ell=t}^{s}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},A_{i,\ell})} \\
	&\cdot \left. \left(\widehat{Q}_{s+1}^{-k(i)}(H_{i,s+1},\check{\pi}_{s+1}(H_{i,s+1}),\ldots,\check{\pi}_{T}(H_{i,T}))-\widehat{Q}_{s}^{-k(i)}(H_{i,s},A_{i,s},\check{\pi}_{s+1}(H_{i,s+1}),\ldots,\check{\pi}_{T}(H_{i,T}))\right)\right]\\
	+&\widehat{Q}_{t}^{-k(i)}(H_{i,t},A_{i,t},\check{\pi}_{t+1}(H_{i,t+1}),\ldots,\check{\pi}_{T}(H_{i,T})).
\end{align*}
\end{comment}
One may think that we can more simply define the score function, for example, as 
\begin{align*}
    \widetilde{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}(a_t) &= \frac{ \left(Y_{i}- \widehat{Q}_{t}^{-k(i)}(H_{i,t},\underline{A}_{i,t:T})\right) }{\prod_{s=t}^{T}\hat{e}_{s}^{-k(i)}(H_{i,s},A_{i,s})}\cdot \prod_{s=t+1}^{T}1\{A_{i,s} = \check{\pi}_{s}(H_{i,s})\} \cdot1\{A_{i,t} = a_t\} \\
    &+ 
    \widehat{Q}_{t}^{-k(i)}\left(H_{i,t},a_{t},\check{\pi}_{t+1}(H_{i,t+1}),\ldots,\check{\pi}_{T}(H_{i,T})\right),
\end{align*}
and use $\widetilde{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}(a_t)$ instead of $\widehat{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}(a_t)$ in each step of the sequential approach. The sequential learning approach with $\widehat{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}(a_t)$ replaced by $\widetilde{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}(a_t)$ also has the doubly robust property to estimate the optimal DTR. However, finite-sample performance of using $\widetilde{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}(a_t)$ is usually worse than that of using $\widehat{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}(a_t)$. This is because the performance of $\widetilde{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}(a_t)$ depends more strongly on that of the following statistics:
\begin{align*}
    \frac{1}{n}\sum_{i=1}^{n}\frac{ Y_{i}\cdot \prod_{s=t+1}^{T}1\{A_{i,s} = \check{\pi}_{s}(H_{i,s})\} \cdot1\{A_{i,t} = a_t\} }{\prod_{s=t}^{T}\hat{e}_{s}^{-k(i)}(H_{i,s},A_{i,s})},
\end{align*}
which is purely an inverse probability weighting estimator with sequential propensity scores and likely to have problematically high variance in the dynamic setting as discussed by \cite{Laber_et_al_2014} and recognized by \cite{Sutton_et_al_1999} and \cite{Doroudi_et_al_2018}.
\end{remark}
%\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Second Approach}\label{sec:second_approach}

The second approach we propose is also based on the backward induction and similar to the first approach. Given a DTR $\pi$, for stage $t=1,\ldots,T-1$, we define the Q-function for the sequence of the policies $\pi_{(t+1):T}$ as follows:
\begin{align*}
    Q_{t}^{\pi_{(t+1):T}}\left(h_t,a_t\right) \equiv E\left[Y\left(\underline{A}_{1:t},\pi_{t+1}(H_{t+1}),\ldots,\pi_T(H_T)\right) \middle| H_t = h_t, A_t=a_t\right],
\end{align*}
which is the conditional mean of the potential outcome at stage $t$ when the pair of history and treatment is $(h_t,a_T)$ and the future treatments are subject to $\pi_{(t+1):T}$. For $t=T$, we define $Q_{T}(h_T,a_T)\equiv E\left[Y\left(\underline{a}_{T}\right) \middle| H_T = h_T\right]$, which is equal to $Q_{T}(h_T,a_T)$. 

%Using the sequential relationships in the equations (\ref{eq:Q_learn_1})-(\ref{eq:V_learn_2}), we can estimate $Q_{T}(\cdot,\cdot)$ and $\{Q_{t}^{\pi_{(t+1):T}}(\cdot,\cdot)\}_{t=1,\ldots,T-1}$ by the following sequential way: 
%\begin{itemize}
%    \item Regress $Y_T$ on $(H_T,Q_T)$ to obtain $\widehat{Q}_{T}(\cdot,\cdot)$ as the estimated regression function for $Q_T(\cdot,\cdot)$, and estimate $V_{T}^{\pi_T}(h_T)$ by $\widehat{V}_{T}^{\pi_T}(h_T)=\widehat{Q}_{T}(h_T,\pi_T)$;
%    \item Recursively, for $t=T-1,\ldots,1$, regress $\widehat{V}_{t+1}^{\pi_{(t+1):T}}(H_{t+1})$ on $(H_t,A_t)$ to obtain $\widehat{Q}_{t}^{\pi_{(t+1):T}}(\cdot,\cdot)$ as the estimated regression function for $Q_{t}^{\pi_{(t+1):T}}(\cdot,\cdot)$, and estimate $V_{t}^{\pi_{t:T}}(h_t)$ by $\widehat{V}_{t}^{\pi_{t:T}}(h_t)=\widehat{Q}_{t}^{\pi_{(t+1):T}}(h_t,\pi_t)$.
%\end{itemize}
%Note that our definition of $Q_t(\cdot,\cdot)$ depends on the policy classes $\Pi_t,\ldots,\Pi_{T}$  through the maximization of $Q_{s}(\cdot,\cdot)$ over $\Pi_{s}$ for $s > t$. %\footnote{Batch Q-learning methods usually chooses the optimal actions at each stage without constraints on the candidate of actions, which leads to the first-best policy at each stage. However, under Assumption \ref{asm:first-best} (i.e., the feasibility of the first-best policy), there is little difference between the batch Q-learning with and without the policy search over the pre-specified class of policies. \citeauthor{Murphy_2005} (\citeyear{Murphy_2005}, Section 3) gives detailed discussion for this.}
%We can apply flexible regression methods (e.g., random forest, lasso, neural network learning) to the regressions to estimate $Q_{T}(\cdot,\cdot)$ and $\{Q_{t}^{\pi_{(t+1):T}}(\cdot,\cdot)\}_{t=1,\ldots,T-1}$. Under Assumption \ref{asm:first-best}, we can show that $\pi_{t}^{\ast,FB} = \argmax_{\pi \in \Pi_t}E\left[Q_{t}^{\pi_{(t+1):T}^{\ast,FB}}(H_t,\pi_t)\right]$ (see e.g., \cite{Murphy_2005}). 

The approach presented here also consists of multiple steps. As a preliminary step, we estimate the propensity scores $\{e_t(\cdot,\cdot)\}_{t=1,\ldots,T}$ and Q-function $Q_T(\cdot,\cdot)$ for the last stage by using the data excluded in each fold. For each index $k$ of the fold, we denote by $\hat{e}_{t}^{-k}(\cdot,\cdot)$ and $\widehat{Q}_{T}^{-k}(\cdot,\cdot)$, respectively, the estimators of $e_t(\cdot,\cdot)$ and $Q_T(\cdot,\cdot)$ using data not contained in the $k$-th fold. Any regression method can be used to estimate $e_t(\cdot,\cdot)$ and $Q_{T}(\cdot,\cdot)$. %Remark \ref{rem:Q-function_estimate} gives some comments regarding the Q-learning.%Given $\widehat{Q}_{t}^{-k}(\cdot,\underline{a}_{t:T})$, we also define and compute $\hat{V}_{t}^{-k}(\cdot) := \max_{a_t \in \MA_t}\widehat{Q}_{t}^{-k}(\cdot,\underline{a}_{t:T})$, which is an estimate of the optimal state-value function $V_t(\cdot) := \max_{a_t \in \MA_t}Q_{t}(\cdot,a_t)$.

Given $\{\hat{e}_{t}^{-k}(\cdot,\cdot)\}_{t=1}^{T}$ and  $\widehat{Q}_{T}^{-k}(\cdot,\cdot)$ for each $k=1,\ldots,K$, the optimal DTR is estimated in the following sequentially way. In the first step, regarding the last stage $T$, we make the score function of the treatment $a_T$ for stage $T$ as follows: 
\begin{align*}
    \widehat{\Gamma}_{i,T}^{Q}(a_T) \equiv \frac{Y_{i} - \widehat{Q}_{T}^{-k(i)}(H_{i,T},A_{i,T})}{\hat{e}_{T}^{-k(i)}(H_{i,T},A_{i,T})}\cdot 1\{A_{i,T}=a_T\} +
    \widehat{Q}_{T}^{-k(i)}(H_{i,t},a_{T})
\end{align*}
Note that its sample mean $(1/n)\sum_{i=1}^{n}\widehat{\Gamma}_{i,T}^{Q}(a_T)$ is an AIPW estimator of $E\left[Y_{T}\left(\underline{A}_{1:(T-1)},a_{T}\right)\right]$.

We then find the best candidate policy in stage $T$ by solving
\begin{align}
\check{\pi}_{T} \in \argmax_{\pi_{T}\in \Pi_{T}}\frac{1}{n}\sum_{i=1}^{n}\widehat{\Gamma}_{i,T}^{Q}\left(\pi_T(H_{i,T})\right), \label{eq:policy_search_T}
\end{align}
where the objective function $(1/n)\sum_{i=1}^{n}\widehat{\Gamma}_{i,T}^{Q}\left(\pi_T(H_{i,T})\right)$ is an AIPW estimator of the policy value function $V_{T}(\pi_T)$. Note that this step is not different from the first of the approach presented in Section \ref{sec:first_approach} because $\widehat{Q}_{T}^{-k}(\cdot,\cdot)$ does not substantially　differ from $\widehat{Q}_{T}^{-k}(\cdot,\cdot)$.

In the following step, we consider stage $T-1$. Given $\check{\pi}_{T}$, for each index $k$ of the cross-fitting fold, we estimate $Q_{T-1}^{\check{\pi}_{T}}(\cdot,\cdot)$ by regressing $\widehat{Q}_{T}^{-k}(H_{i,t},\check{\pi}_{T})$ on $(H_{i,T-1},A_{i,T-1})$ using the observations whose indices are not contained in $I_k$.\footnote{This step is similar to the regression step in the batch offline Q-learning (\cite{Murphy_2005}). The difference is that the Q-learning estimates the optimal policy using the regression equation of the outcome whereas our approach estimates the optimal policy by maximizing the AIPW estimator of the policy value function.} We denote by $\widehat{Q}_{T-1}^{\check{\pi}_{T},-k}(\cdot,\cdot)$ the resulting estimator of $Q_{T-1}^{\check{\pi}_{T}}(\cdot,\cdot)$ for each fold $k$. Any regression method can be used in this step. %Note that although $\widehat{Q}_{T-1}^{\check{\pi}_{T},-k}(\cdot,\cdot)$ is used in the corss-fitting procedure, it depends on all the observations because $\check{\pi}_{T}$ depends on all the observations.

We next make the score function of $a_{T-1}$ as
\begin{align*}
    \widehat{\Gamma}_{i,T-1}^{Q,\check{\pi}_{T}}(a_{T-1})  &\equiv \frac{ \left( \widehat{\Gamma}_{i,T}^{Q}(\check{\pi}_{T}(H_{i,T})) - \widehat{Q}_{T-1}^{\check{\pi}_{T},-k(i)}(H_{i,T-1},A_{i,T-1})\right) }{\hat{e}_{T-1}^{-k(i)}(H_{i,T-1},A_{i,T-1})} \cdot 1\{A_{i,T-1} = a_{T-1}\} \\
    &+ 
\widehat{Q}_{T-1}^{\check{\pi}_{T},-k(i)}(H_{i,T-1},a_{T-1}).
\end{align*}
Its sample mean $(1/n)\sum_{i=1}^{n}\widehat{\Gamma}_{i,T-1}^{Q,\check{\pi}_{T}}(a_{T-1})$ is an AIPW estimator of the action value function $V_{T-1}^{\check{\pi}_{T}}(a_{T-1})$.
We then find the best candidate policy in stage $T-1$ by solving
\begin{align*}
    \check{\pi}_{T-1} \in \argmax_{\pi_{T-1}\in \Pi_{T-1}} \frac{1}{n}\sum_{i=1}^{n}\widehat{\Gamma}_{i,T-1}^{Q,\check{\pi}_T}\left(\pi_{T-1}(H_{i,T-1})\right),
\end{align*}
where the objective function $(1/n)\sum_{i=1}^{n}\widehat{\Gamma}_{i,T-1}^{Q,\check{\pi}_{T}}\left(\pi_{T-1}(H_{i,T-1})\right)$ is an AIPW estimator of the policy value $V_t(\pi_{T-1},\check{\pi}_T)$.

Recursively, for $t=T-2,\ldots,1$, we learn the optimal policy as follows. For each cross-fitting index $k$, we first estimate $Q_{t}^{\check{\pi}_{(t+1):T}}$ by regressing  $\widehat{Q}_{t+1}^{\check{\pi}_{(t+2):T}}(H_{i,t+1},A_{i,t+1})$ on $(H_{i,t},A_{i,t})$ using the observations whose indices are not in $I_k$. Any regression method can be used. Define the score function as
\begin{align*}
    \widehat{\Gamma}_{i,t}^{Q,\check{\pi}_{(t+1):T}}(a_t) &\equiv \frac{ \left(\widehat{\Gamma}_{i,t+1}^{Q,\check{\pi}_{(t+2):T}}(\check{\pi}_{t+1}(H_{i,t+1})) - \widehat{Q}_{t}^{\check{\pi}_{(t+1):T},-k(i)}(H_{i,t},A_{i,t})\right) }{\hat{e}_{t}^{-k(i)}(H_{i,t},A_{i,t})}\cdot 1\{A_{i,t} = a_t\} \\
    &+ 
    \widehat{Q}_{t}^{\check{\pi}_{(t+1):T},-k(i)}(H_{i,t},a_{t}).
\end{align*}
We find the best candidate policy in stage $t$ by solving
\begin{align*}
    \check{\pi}_{t} \in \argmax_{\pi_{t}\in \Pi_{t}} \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{Q,\check{\pi}_{(t+1):T}}\left(\pi_{t}(H_{i,t})\right).
\end{align*}
%where the objective function $(1/n)\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{Q,\check{\pi}_{(t+1):T}}\left(\pi_{t}(H_{i,t})\right)$ is an AIPW estimator of the policy value $V_{t}(\pi_t,\check{\pi}_{t+1},\ldots,\check{\pi}_{T})$.

Eventually, we obtain the resulting DTR $\check{\pi}=(\check{\pi}_{1},\ldots,\check{\pi}_T)$. The following section clarifies the statistical property of $\check{\pi}$.

%\bigskip
\begin{comment}
\begin{remark}\label{rem:Q-function_estimate} (Q-learning with/without policy search)
We can apply the Q-learning (\cite{Murphy_2005}) either with or without policy search over $\Pi_1,\ldots,\Pi_T$ in the estimation of the Q-function $Q_t(\cdot,\cdot)$. Regardless of the policy search, in the Q-learning, we firstly estimate $Q_t(\cdot,\cdot)$ by (non/semiparametrically) regressing $Y_T$ on $(H_T,A_T)$ to obtain $\widehat{Q}(\cdot,\cdot)$. Then, in the Q-learning with policy search, sequentially from $t=T-1$ to $1$, we estimate $Q_{t}(\cdot,\cdot)$ by (non/semiparametrically) regressing $Y_t + \max_{\pi_{t+1} \in \Pi_{t+1}}\widehat{Q}_{t+1}(H_{t+1},\pi_{t+1})$ on $(H_{t},A_{t})$ to obtain $\widehat{Q}_t(\cdot,\cdot)$. On the other hand, in the Q-learning without the policy search, we sequentially estimates $Q_t(\cdot,\cdot)$ for $t=T-1,\ldots,1$ by regressing $Y_t + \argmax_{a_{t+1} \in \MA_{t+1}}\widehat{Q}_{t+1}(H_{t+1},a_{t+1})$ on $(H_t,A_t)$ to obtain $\widehat{Q}_t(\cdot,\cdot)$. Note that the difference is in taking maximum of $\widehat{Q}_{t}(h_t,\cdot)$ over the action space $\MA_t$ or policy space $\Pi_t$.

Although the Q-function defined in (\ref{eq:Q_learn_1}) and (\ref{eq:Q_learn_2}) depend on $\Pi$, we can consistently estimate it with the Q-learning without policy search in some common cases: for example, (i) when the first-best policy $\pi_{t}^{\ast,FB}$ is unique and (ii) when the first-best policies are indiscriminate in the sense that $Q_t(h_t,\pi_{t}^{\ast,FB}) = Q_t(h_t,\pi_{t}^{\ast,FB,\prime})$ holds for any $h_t$ and for any first-best policies $\pi_{t}^{\ast,FB}$ and $\pi_{t}^{\ast,FB,\prime}$ satisfying Assumption \ref{asm:first-best}. %Any seminon-parametric regression methods can be used for the regression in the Q-learning.
\end{remark}

\end{comment}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bigskip

\section{Statistical Properties}\label{sec:statistical_properties}

Given a DTR $\pi \in \Pi$, we define the regret of $\pi$ by $R(\pi) \equiv \sup_{\tilde{\pi} \in \Pi}W(\tilde{\pi}) - W(\pi)$, the loss of the welfare of $\pi$ relative to the maximum welfare achievable in $\Pi$.
We study statistical properties of $\check{\pi}$ and $\check{\pi}$ with respect to their regrets $R(\check{\pi})$ and $R(\check{\pi})$, respectively. This section shows the doubly-robust property of the the two approaches and derives the rate of convergences of $R(\check{\pi})$ and $R(\check{\pi})$ depending on the rates of convergence of the estimators of the nuisance components and complexity of $\Pi$. 

We first focus on the DTR $\check{\pi}$ obtained by the approach in Section \ref{sec:first_approach}. Let $\widehat{Q}_{t}^{(n)}(\cdot,\cdot)$ and $\hat{e}_{t}^{(n)}(\cdot,\cdot)$, respectively, denote estimators of the conditional mean of the potential outcome $Q_{t}(\cdot,\cdot)$ and propensity score $e_t(\cdot,\cdot)$ using size $n$ sample randomly drawn from the population $P$.
We suppose that $\{\widehat{Q}_{t}^{(n)}(\cdot,\cdot)\}_{t=1}^{T}$ and $\{\hat{e}_{t}^{(n)}(\cdot,\cdot)\}_{t=1}^{T}$ satisfy the following assumption.

\bigskip{}

\begin{assumption}\label{asm:rate_of_convergence_backward_Q}
(i) There exists $\tilde{\tau}_B >0$ such that the following holds: For all $t=1,\ldots,T$, $s=1,\ldots,t$, and $m \in \{0,1\}$,
\begin{align*}
    \sup_{\underline{a}_{t:T} \in \underline{\MA}_{t:T}} & E\left[\left(\widehat{Q}_{t}^{(n)}(H_{t},\underline{a}_{t:T}) - Q_{t}(H_{t},\underline{a}_{t:T})\right)^{2}\right] \\
    &\times E\left[\left(\frac{1}{\prod_{\ell=s}^{t-m}\hat{e}_{\ell}^{(n)}(H_{\ell},a_{\ell})} - \frac{1}{\prod_{\ell=s}^{t-m}e_{\ell}(H_{\ell},a_{\ell})} \right)^{2}\right] = \frac{o(1)}{n^{\tilde{\tau}_{B}}}.
\end{align*}
(ii) There exists $n_0 \in \Natural$ such that for any $n \geq n_0$ and $t=1,\ldots,T$, $\sup_{h_t \in \MH_t, \underline{a}_{t:T} \in \underline{\MA}_{t:T}}\widehat{Q}_{t}^{(n)}(h_{t},\underline{a}_{t:T}) < \infty$ and $\sup_{h_t \in \MH_t, a_t \in \MA_t}\hat{e}_{t}^{(n)}(h_{t},a_{t}) > 0$ hold a.s.
\end{assumption}

\bigskip{}


As we will see later, the $\sqrt{n}$-consistency of the regret  $R(\check{\pi})$ to zero can be achieved when Assumption \ref{asm:rate_of_convergence_backward_Q} (i) holds with $\tilde{\tau}_{B}=1$. This is not very strong or restrictive. For example, Assumption \ref{asm:rate_of_convergence_backward_Q} (i) is satisfied with $\tilde{\tau}_B = 1$ when
\begin{align*}
&\sup_{\underline{a}_{t:T} \in \underline{\MA}_{t:T}}E\left[\left(\widehat{Q}_{t}^{(n)}(H_{t},\underline{a}_{t:T}) - Q_{t}(H_{t},\underline{a}_{t:T})\right)^{2}\right]  = \frac{o(1)}{\sqrt{n}} \mbox{\ and\ }\\
    &\sup_{\underline{a}_{s:t} \in \underline{\MA}_{s:t}} E\left[\left(\frac{1}{\prod_{\ell=s}^{t}\hat{e}_{\ell}^{(n)}(H_{\ell},a_{\ell})} - \frac{1}{\prod_{\ell=s}^{t}e_{\ell}(H_{\ell},a_{\ell})} \right)^{2}\right] = \frac{o(1)}{\sqrt{n}}
\end{align*}
hold for all $t=1,\ldots,T$ and $s=1,\ldots,t$. Note also that Assumption \ref{asm:rate_of_convergence_backward_Q} (i) encompasses the doubly-robustness property; that is, Assumption \ref{asm:rate_of_convergence_backward_Q} (i) is satisfied if at least one of $\widehat{Q}_{t}^{(n)}(\cdot,\cdot)$ and $\prod_{s=t}^{T}\hat{e}_{s}^{(n)}(\cdot,\cdot)$ is consistent in terms of the mean squared error.

%The result for the rate of convergence of the condinal treatment effect estimator  $\widehat{Q}_{t}^{(n)}(\cdot,\cdot)$ is not a standard result because the population objective function $Q_{t}(\cdot,\cdot)$ is not smooth due to the non-differentiable maximization in its definition (\ref{eq:Q_learn_2}). However, some results may be applicable. For example, \cite{Zhang_et_al_2018} derive the rate of convergence for the batch offline Q-learning using Support Vector Machine regression with the policy search over the class of list forms of policies.\footnote{They do this by using a weaker notion of smoothness.}

We next consider complexity of the class $\Pi$ of DTRs and each policy class $\Pi_t$ ($t=1,\ldots,T$). Following \cite{zhou2022offline}, we use the $\epsilon$-Hamming covering number to measure the complexity of the sequence of policies $\pi_{t:T} \in \Pi_{s}\times \cdots \times \Pi_{t}$ for each $s \leq t$. 

\bigskip{}

\begin{definition}
(i) For any stages $s$ and $t$ such that $s \leq t$, given a set of history points $\{h_{t}^{(1)},\ldots,h_{t}^{(n)}\} \subseteq \MH_{t}$, we define the Hamming distance between two sequences of policies  $\pi_{s:t}, \pi_{s:t}^{\prime} \in \Pi_{s:t}$ as $d_{h}(\pi_{s:t},\pi_{s:t}^{\prime}):=n^{-1}\sum_{i=1}^{n}1\{\pi_s(h_{s}^{(i)})\neq \pi_{s}^{\prime}(h_{s}^{(i)}) \vee \cdots \vee \pi_t(h_{t}^{(i)})\neq \pi_{t}^{\prime}(h_{t}^{(i)})\}$,
%\begin{align*}
%    d_{h}(\pi_{t:T},\pi_{t:T}^{\prime}):=n^{-1}\sum_{i=1}^{n}1\{\pi_s(h_{s}^{(i)})\neq \pi_{s}^{\prime}(h_{s}^{(i)}) \vee \cdots \vee \pi_t(h_{t}^{(i)})\neq \pi_{t}^{\prime}(h_{t}^{(i)})\},
%\end{align*}
where note that $h_{s}^{(i)} \subseteq h_{s+1}^{(i)}\subseteq \ldots \subseteq h_{t}^{(i)} \in \MH_t$ from the definition of the history. We define the $\epsilon$-Hamming covering number of $\pi_{s:t}$ as follows:
\begin{align*}
    N_{d_h}(\epsilon,\pi_{s:t}) := \sup\left\{N_{d_h}\left(\epsilon,\pi_{s:t},\left\{h_{t}^{(1)},\ldots,h_{t}^{(n)}\right\}\right)\middle| n \geq 1, h_{t}^{(1)},\ldots,h_{t}^{(n)} \in \MH_t\right\},
\end{align*}
where $N_{d_h}\left(\epsilon,\pi_{s:t},\left\{h_{t}^{(1)},\ldots,h_{t}^{(n)}\right\}\right)$ is the smallest number of sequences of policies $\pi_{s:t}^{(1)},\pi_{s:t}^{(2)},\ldots$ in $\pi_{s:t}$ such that for any $\pi_{s:t} \in \Pi_{s:t}$, there exists $\pi_{s:t}^{(i)}$ satisfying $d_h(\pi_{s:t},\pi_{s:t}^{(i)}) \leq \epsilon$.\\
(ii) We define the entropy integral of $\pi_{s:t}$ as $\kappa(\pi_{s:t})= \int_{0}^{1}\sqrt{\log N_{d_h}\left(\epsilon^2,\pi_{s:t}\right)}d\epsilon$.
\end{definition}

\bigskip{}

%\begin{definition}
%(i) For any stage $t$, given a set of points $\{h_{t}^{(1)},\ldots,h_{t}^{(n)}\} \subseteq \MH_{t}$, let the Hamming distance between two policies $\pi_t$ and $\pi_{t}^\prime$ be $d_h(\pi_t,\pi_{t}^\prime) := n^{-1}\sum_{i=1}^{n}1\{\pi_{t}(h_{t}^{(i)}) \neq \pi_{t}^{\prime}(h_{t}^{(i)})\}$. We define the $\epsilon$-Hamming covering number of $\Pi_t$ as
%\begin{align}
%    N_{d_h}(\epsilon,\Pi_t) := \sup\left\{N_{d_h}\left(\epsilon,\Pi_t,\left\{h_{t}^{(1)},\ldots,h_{t}^{(n)}\right\}\right)\middle| n \geq1, h_{t}^{(1)},\ldots,h_{t}^{(n)} \in \MH_t\right\}, \label{eq:covering_number}
%\end{align}
%where $N_{d_h}\left(\epsilon,\Pi_t,\left\{h_{t}^{(1)},\ldots,h_{t}^{(n)}\right\}\right)$ is the smallest number of policies $\pi_{t}^{(1)},\pi_{t}^{(2)},\ldots\in \Pi_t$ such that for any $\pi_t \in \Pi_t$, there exists $\pi_{t}^{(i)}$ such that $d_h(\pi_t,\pi_{t}^\prime) \leq \epsilon$.\\
%(ii) We define the entropy integrals of $\Pi_t$ as $\kappa(\Pi_t)= \int_{0}^{1}\sqrt{\log N_{d_h}\left(\epsilon^2,\Pi_t\right)}d\epsilon$.
%\end{definition}


When $s=t$, note that $N_{d_h}(\epsilon,\Pi_t) = N_{d_h}(\epsilon,\Pi_{t:t})$ and $\kappa(\Pi_t)=\kappa(\Pi_{t:t})$. We suppose that $\Pi_t$ for each $t$ is not too complex in terms of the covering number.
\bigskip{}

\begin{assumption}\label{asm:bounded entropy}
For all $t=1,\ldots,T$, $N_{d_h}(\epsilon,\Pi_{t}) \leq C\exp(D(1/\epsilon)^{\omega})$ holds for any $\epsilon>0$ and some constants $C,D>0$ and $0<\omega<0.5$.
\end{assumption}

\bigskip{}

This assumption implies that the covering number of $\Pi_t$ does not grow too quickly, but allows that $\log N_{d_h}(\epsilon,\Pi_{t})$ grows at a rate of $1/\epsilon$.
The assumption is satisfied, for example, by the policy class of finite-depth trees (see \citeauthor{zhou2022offline} (\citeyear{zhou2022offline}, Lemma 4)). 
\citeauthor{zhou2022offline} (\citeyear{zhou2022offline}, Remark 4) shows that the entropy integral $\kappa(\Pi_t)$ is finite under Assumption \ref{asm:bounded entropy}. 
As for the class $\Pi$ of whole DTRs, $\kappa(\Pi)$ is finite as well under Assumption \ref{asm:bounded entropy}. 

\bigskip

\begin{lemma}\label{lem:entropy_integral_bound}
Under Assumption \ref{asm:bounded entropy}, $\kappa(\Pi) < \infty$.
\end{lemma}

%\begin{proof}
%See Appendix.
%\end{proof}

\bigskip

%\cite{zhou2022offline} give some other discussion for the covering number. For example, for the policy class of finite-depth trees (\cite{Breiman_2017}), $\log N_{d_h}(\epsilon,\Pi_{t})$ is only $O(\log(1/\epsilon))$ (see \citeauthor{zhou2022offline} (\citeyearpar{zhou2022offline}, Lemma 4)). 

The following theorem is a main result of this paper and show the rate of convergence for the regret of the approach proposed in Section \ref{sec:firt_approach}.

\bigskip

\begin{theorem}\label{thm:main_theorem_backward}
Under Assumptions \ref{asm:sequential independence}--\ref{asm:overlap}, \ref{asm:first-best}, \ref{asm:rate_of_convergence_backward_Q}, and \ref{asm:bounded entropy},
\begin{align*}
    R(\check{\pi}) = O_{p}\left(\kappa(\Pi) \cdot n^{-1/2}\right) + o_{p} (n^{-\min\{1/2,\tilde{\tau}_{B}/2\}}).
\end{align*}
\end{theorem}

%\begin{proof}
%See Appendix.
%\end{proof}

\bigskip

When Assumption \ref{asm:rate_of_convergence_backward_Q} (i) holds with $\tilde{\tau}_B = 1$, the approach proposed in Section \ref{sec:first_approach} achieves the minimax optimal rate $O_ {p}(1/\sqrt{n})$ of convergence of the regret with respect to the sample size $n$.\footnote{\cite{Sakaguchi_2021} shows that the minimax optimal convergence rate of the regret is $O_ {p}(1/\sqrt{n})$ in the case that treatment assignment at each stage is binary.} This result is consistent with those of \cite{Athey_Wager_2020} and \cite{zhou2022offline} who study static policy learning and that of \cite{Nie_et_al_2021} who study learning problem of when-to-treat policy.

We next consider the regret property of the DTR $\check{\pi}$ obtained by the approach presented in Section \ref{sec:second_approach}. Let $\widehat{Q}_{t}^{\pi_{(t+1):T},(n)}(\cdot,\cdot)$ denote the estimator of the Q-function $Q_{t}^{\pi_{(t+1):T}}$ for the sequence of fixed policies $\pi_{(t+1):T} \in \Pi_{(t+1):T}$ using size $n$ sample randomly drawn from the population $P$. $\widehat{Q}_{t}^{\pi_{(t+1):T},(n)}(\cdot,\cdot)$ is obtained by the sequential regression procedure in Section \ref{sec:second_approach} with $\check{\pi}_s$ replaced by $\pi_s$ for $s=t+1,\ldots,T$. We suppose that $\left\{\widehat{Q}_{t}^{\pi_{(t+1):T},(n)}(\cdot,\cdot):\pi_{(t+1):T} \in \Pi_{(t+1):T}\right\}_{t=1,\ldots,T-1}$, $\widehat{Q}_{T}^{(n)}(\cdot,\cdot)$, and $\{\hat{e}_{t}(\cdot,\cdot)\}_{t=1,\ldots,T}$ satisfy the following assumption.


\bigskip{}

\begin{assumption}\label{asm:rate_of_convergence_backward_Q}
(i) There exists $\tilde{\tilde{\tau}}_B >0$ such that the following hold:
\begin{align*}
    \sup_{\underline{a}_{T} \in \MA_{T}} & E\left[\left(\widehat{Q}_{T}^{(n)}(H_{T},a_{T}) - Q_{T}(H_{T},a_{T})\right)^{2}\right] \\
    &\times E\left[\left(\frac{1}{\prod_{t=1}^{T}\hat{e}_{t}^{(n)}(H_{t},a_{t})} - \frac{1}{\prod_{t=1}^{T}e_{t}(H_{t},a_{t})} \right)^{2}\right] = \frac{o(1)}{n^{\tilde{\tilde{\tau}}_{B}}}
\end{align*}
and, for all $t=1,\ldots,T-1$, $s=1,\ldots,t$, and $m \in \{0,1\}$,
\begin{align*}
    \sup_{\pi_{(t+1):T} \in \Pi_{(t+1):T},\ \underline{a}_{s:T} \in \MA_{s:t}} & E\left[\left(\widehat{Q}_{t}^{\pi_{(t+1):T},(n)}(H_{t},a_{t}) - Q_{t}(H_{t},a_{t})\right)^{2}\right] \\
    &\times E\left[\left(\frac{1}{\prod_{\ell=s}^{t-m}\hat{e}_{\ell}^{(n)}(H_{\ell},a_{\ell})} - \frac{1}{\prod_{\ell=s}^{t-m}e_{\ell}(H_{\ell},a_{\ell})} \right)^{2}\right] = \frac{o(1)}{n^{\tilde{\tau}_{B}}}.
\end{align*}
(ii) There exists $n_0 \in \Natural$ such that for any $n \geq n_0$, the following hold a.s.: 
\begin{align*}
    \sup_{h_T \in \MH_T, a_T \in \MA_T}\widehat{Q}_{T}^{(n)}(h_{T},a_{T}) &< \infty,\  
    \sup_{h_t \in \MH_t, a_t \in \MA_t, \pi_{(t+1):T} \in \Pi_{(t+1):T}}\widehat{Q}_{t}^{\pi_{(t+1):T},(n)}(h_{t},a_{t}) < \infty, \\
    \sup_{h_t \in \MH_t, a_t \in \MA_t}\hat{e}_{t}^{(n)}(h_{t},a_{t}) &> 0
\end{align*}
for all $t \in \{1,\ldots,T-1\}$.
\end{assumption}

\bigskip{}




The result for convergence rate of the estimated Q-function  $\widehat{Q}_{t}^{\pi_{(t+1):T},(n)}(\cdot,\cdot)$ is not a standard result because $Q_{t}^{\pi_{(t+1):T}}(\cdot,\cdot)$ is not smooth due to the existence of the policies $\pi_{(t+1):T}$ in its definition. However, some results might be applicable. For example, \cite{Zhang_et_al_2018} derive the rate of convergence result for the batch offline Q-learning using Support Vector Machine regression with the policy search over the class of list forms of policies.

The following theorem shows the statistical property of the DTR $\check{\pi}$ in terms of the rate of the convergence of  the regret.

\bigskip

\begin{theorem}\label{thm:main_theorem_backward_Q}
Under Assumptions \ref{asm:sequential independence}--\ref{asm:overlap}, \ref{asm:first-best}, \ref{asm:bounded entropy}, and \ref{asm:rate_of_convergence_backward_Q_Q},
\begin{align*}
    R(\check{\pi}) = O_{p}\left(\kappa(\Pi) \cdot n^{-1/2}\right) + o_{p} (n^{-\min\{1/2,\tilde{\tilde{\tau}}_{B}/2\}}).
\end{align*}
\end{theorem}

%\begin{proof}
%See Appendix.
%\end{proof}

\bigskip

The theorem implies that the minimax optimal rate of the regret, which is $O_p(n^{-1/2})$, can be achieved by $\check{\pi}$ when Assumption \ref{asm:rate_of_convergence_backward_Q_Q} holds with $\tilde{\tilde{\tau}}_{B}=1$. This condition holds, for example, when
\begin{align*}
\sup_{a_T \in \MA_T}&E\left[\left(\widehat{Q}_{T}^{(n)}(H_{T},a_{T}) - Q_{T}(H_{T},a_{T})\right)^{2}\right]  = \frac{o(1)}{\sqrt{n}},\\
    \sup_{\pi_{t:T}\in \Pi_{t:T},\ a_t \in \MA_t}&E\left[\left(\widehat{Q}_{t}^{\pi_{(t+1):T},(n)}(H_{t},a_{t}) - Q_{t}^{\pi_{(t+1):T}}(H_{t},a_{t})\right)^{2}\right]  = \frac{o(1)}{\sqrt{n}} \mbox{, and }\\ 
    \sup_{\underline{a}_{t^\prime} \in \underline{\MA}_{t^\prime}} &E\left[\left(\frac{1}{\prod_{\ell=s}^{t^\prime}\hat{e}_{\ell}^{(n)}(H_{\ell},a_{\ell})} - \frac{1}{\prod_{\ell=s}^{t^\prime}e_{\ell}(H_{\ell},a_{\ell})} \right)^{2}\right] = \frac{o(1)}{\sqrt{n}}
\end{align*}
hold for all $t=1,\ldots,T-1$, $t^\prime=1,\ldots,T$, and $s=1,\ldots,t^\prime$. Note also that Assumption \ref{asm:rate_of_convergence_backward_Q_Q} (i) encompasses DR property, meaning that Assumption \ref{asm:rate_of_convergence_backward_Q_Q} (i) holds if at least one of $\widehat{Q}_{t}^{\pi_{(t+1):T}}(\cdot,\cdot)$ and $\hat{e}_t(\cdot,\cdot)$ is consistent in terms of the mean squared error.



%\bigskip
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Comparison to Existing Approaches}\label{sec:existing_methods}
We make comparison of the proposed approaches to several existing approaches to lean the optimal DTRs with observational data. 
One approach to learn the optimal DTRs is to use the inverse probability weighting (IPW) to estimate the welfare function $W(\pi)$ or the policy value function $V_t(\pi_{t:T})$ for each stage $t$, and then maximize the estimated welfare function of policy valued function over $\Pi$.  \cite{Zhao_et_al_2015} and \cite{Sakaguchi_2021} employ this approach. In the sequential learning of this approach, we can learn the optimal DTR by $\check{\pi}^{B} = (\check{\pi}_{1}^{B},\ldots,\check{\pi}_{T}^{B})$ which recursively solves
\begin{align}
&\check{\pi}_{T}^{B} \in \argmax_{\pi_{T} \in \Pi_{T}} \frac{1}{n}\sum_{i=1}^{n}\frac{Y_{i}\cdot1\{A_{i,T}=\pi_{T}(H_{i,T})\}}{\hat{e}_{T}(H_{i,T},A_{i,T})}; \label{eq:IPW_1}\\
& \check{\pi}_{t}^{B}  \in \argmax_{\pi_{t} \in \Pi_{T}} \frac{1}{n}\sum_{i=1}^{n}\frac{Y_{i}\cdot \prod_{s=t+1}^{T}1\{A_{i,s}=\check{\pi}_{s}^{B}(H_{i,s})\cdot 1\{A_{i,t}=\pi_{t}(H_{i,t})\}\}}{\prod_{s=t}^{T}\hat{e}_{s}(H_{i,s},A_{i,s})}. \label{eq:IPW_2}
\end{align}
for $t=T-1,\ldots,1$. 
%Under the same assumptions used in Theorem \ref{thm:main_theorem_backward} except for that Assumption \ref{asm:rate_of_convergence_backward_Q} is replaced with the consistency of $\hat{e}_{t}$ for all $t$, this approach can consistently estimate the optimal DTR. However, 
A drawback of this approach is that the estimated DTR is inconsistent when the propensity scores are inconsistently estimated. One way to avoid this is applying nonparametric estimation methods to estimate the propensity scores. In this case, the rate of convergence of the regret of the resulting DTR, which depends on that of the estimated propensity scores, would be slow. The two approaches proposed in this paper improves this rate of convergence by leveraging the models relevant to the outcome variable. 
%Even when the propensity scores are consistently estimated, the convergence rate of the regret of the estimated DTR depends on those of the propensity score estimators (\cite{Sakaguchi_2021}). This implies that when the propensity score estimators do not fit well with the true ones, the resulting DTR is far from the optimal one. The approach proposed in this paper mitigate the problem of missspecifican and improve the convergence of the estimated DTR using the doubly-robust treatment effect estimation and cross-fitting.

An alternative approach is the batch offline Q-learning (\citet{Murphy_2005,Sutton_et_al_2018}). This approach is also not robust to the missspecification of the optimal state-action value function called optimal Q-function. 
Nonparametric regression methods can be applied to the Q-learning; however, the convergence rate of the  regret of the resulting DTR depends on that of the estimated Q-function and would be slow (see \cite{Murphy_2005}). Our approach improves the convergence property of the Q-learning by leveraging the AIPW estimators of the policy value function. 
%Moreover, the convergence rate of the welfare regret of the resulting DTR is decided by those of the estimators $\widehat{Q}_{t}(\cdot,\cdot)$ of the Q-functions (\cite{Murphy_2005}), meaning that poorly estimated $\widehat{Q}_{t}(\cdot,\cdot)$ leads to the DTR with poor welfare performance. Theorem \ref{thm:main_theorem_backward} show that our doubly-robust learning approach solve these issues of the missspecification and slow convergence.

Another approach is estimating the welfare function $W(\pi)$ of a whole DTR $\pi$ by using an AIPW estimator as follows (\citet{Zhang_et_2013, Jiang_Li_2016, Thomas_et_al_2016}):
\begin{align}
    \widehat{W}^{DR}(\pi)=\frac{1}{n}\sum_{i=1}^{n}\left(\hat{\gamma}_{i,T}\left(\pi\right)Y_{i}-\sum_{t=1}^{T}\left(\hat{\gamma}_{i,t}\left(\pi\right)-\hat{\gamma}_{i,t-1}\left(\pi\right)\right)\hat{q}_{t}^{\pi}\left(H_{i,t}\right)\right) \label{eq:AIPW_simultaneous}
\end{align}
where $\hat{\gamma}_{i,t}(\pi)$ and $\hat{q}_{t}^{\pi}(h_{t})$ are estimators of  $\gamma_{i,t}(\pi)=\left(\prod_{s=1}^{t}1\left\{ A_{i,s}=\pi_{s}(H_{i,s})\right\}\right)/\left( \prod_{s=1}^{t}e_{t}\left(H_{i,s},\pi_{s}\right)\right)$ and $q_{t}^{\pi}\left(h_{t}\right)=E\left[Y\left(\underline{A}_{t-1},\pi_{t}(H_{t}),\ldots,\pi_{T}(H_{T})\right)\middle| H_{t}=h_{t}\right]$, respectively. The estimator (\ref{eq:AIPW_simultaneous})  generalizes the AIPW of \cite{Robins_et_al_1994} beyond the static case, and is consistent to $W(\pi)$ if either the propensity weights $\{\hat{\gamma}_t(\cdot)\}_{t=1}^{T}$ or the conditional value estimators $\{\widehat{q}_{t}^{\pi}(\cdot)\}_{t=1}^{T}$ are consistent. 

Having $\widehat{W}^{DR}(\pi)$, we can find the candidate of the optimal DTRs as a solution of the following maximization problem\footnote{\cite{Jiang_Li_2016} and \cite{Thomas_et_al_2016} study the evaluation of a fixed DTR $\pi$ rather than estimation of the optimal DTR.}:
\begin{align}
    \max_{\pi \in \Pi} \widehat{W}^{DR}(\pi). \label{eq:maximization_simultaneous}
\end{align}
However, this maximization problem is computationally challenging due to the following two reasons. First, the nuisance components $\gamma_{i,t}(\pi)$ and $q_{t}^{\pi}(\cdot)$ depend on the DTR $\pi$, requiring to estimate $\gamma_{i,t}(\pi)$ and $q_{t}^{\pi}(\cdot)$ for each specific $\pi$. Second, the problem (\ref{eq:maximization_simultaneous}) maximizes $\widehat{W}^{DR}(\pi)$ simultaneously over the whole class $\Pi$ of DTRs rather than sequentially maximizing it. The simultaneous optimization is computationally demanding when $\Pi$ is not a very simple class or $T$ is not very small. Our learning approaches solves these optimization issues. First, as for the approach presented in Section \ref{sec:first_approach}, the nuisance components $\{e_t(\cdot,\cdot)\}_{t=1}^{T}$ and $\{Q_t(\cdot,\cdot)\}_{t=1}^{T}$ do not depend on any specific policy, making the estimation of the nuisance components independent from the searching for the optimal policy. As for the approach presented in Section \ref{sec:second_approach}, the nuisance components $\{Q_{t}^{\check{\pi}_{(t+1):T}}(\cdot,\cdot)\}_{t=1}^{T-1}$ depend only on the sequence of the estimated policies $\check{\pi}_{(t+1):T}$; hence, the computation is much easier than estimating $Q_{t}^{\pi_{(t+1):T}}(\cdot,\cdot)$ for each $\pi_{(t+1):T}$. Second, the proposed approaches sequentially solve the optimization problem from the last to first stage, which is computationally easier than optimizing the estimated welfare function over a whole DTR. %In the problem of learning when-to-start policies, \cite{Nie_et_al_2021} propose doubly robust learning approach which solves the first computational issue.

%Aside from this work, \cite{Zhang_et_2013} also propose doubly robust estimation methods for the optimal DTRs. 

%the welfare $W(\pi)$ of a fixed DTR $\pi$. Their doubly-robust estimators $\widehat{W}^{DR}(\pi)$ of $W(\pi)$ depend on estimators $\widehat{Q}_{\pi,t}(\cdot)$ of nuisance components $q_{t}^{\pi}(h_t)=E[Y_t \mid H_t=h_t,A_{t+1}=\pi_{t+1}(H_{t+1}),\ldots,A_{T}=\pi_{T}(H_{T})]$. They focus on the evaluation of a fixed DTR rather than learning of the optimal DTRs. Practically, using their methods to learn the optimal DTRs is computationally challenging because if we try to find the optimal DTRs by maximizing their $\widehat{W}^{DR}(\pi)$, we have to estiate the nuisance components $q_{t}^{\pi}(h_t)$ for all $\pi \in \Pi$. In contrast, in our methods, the nuisance components $Q_t(h_t,a_t)$ , $e_t(h_t,a_t)$, and $Q_t(s_t,\underline{a}_t)$ to be estimated do not depend on $\pi$, leading the computational advantages over the existing doubly-robust methods. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Simulation Study \label{sec:simulation}}


We conduct a simulation study to examine the finite sample performance of the proposed learning approach.
We consider data generating processes (DGPs) that consist of two stages of binary treatment assignment $(A_{1},A_{2}) \in \{0,1\}^2$, associated potential outcomes $\left\{Y_{1}\left(a_{1},a_{2}\right)\right\}_{\left\{ a_{1},a_{2}\right\} \in\left\{ 0,1\right\} ^{2}}$,
and $p$ covariates $(X_{1}^{(1)},\ldots,X_{1}^{(p)})$ observed at the first stage, where $p=5$ or $20$ and one covariate $X_2$ observed at the second stage. The data is generated as follows:
\begin{align*}
&(X_{1}^{(1)},\ldots,X_{1}^{(p)})^{\prime} \thicksim N(\boldsymbol{0},I_{p});\\
&X_{2} = (1-A_{1})X_{2}\left(0\right) + A_{1} X_{2}(1)\\
&\mbox{with\ } X_{2}\left(a_{1}\right)=  \left(-0.5 + 1.5X_{1}^{(1)}\right)a_{1}+1.5 X_{1}^{(1)} + \varepsilon_{1};\\
&Y_{2}\left(a_{1},a_{2}\right)=  (0.5 + 0.5 a_{1} + X_{2}(a_1))\times a_{2} + 0.5 - 0.5X_{2}(a_1) + X_{1}^{(4)} + \varepsilon_{2};\\
&\varepsilon_{1} \sim N(0,1), \ \varepsilon_{2} \sim N(0,1);\\
&A_{1} \thicksim Ber\left(1/(1+e^{-X_{1}^{(1)}+X_{1}^{(3)}-X_{1}^{(4)}})\right),\ A_{2} \thicksim Ber\left(1/(1+e^{X_{1}^{(4)}+X_{2}-0.3A_{1}})\right).
\end{align*}
The treatment $a_1$ in the first stage has influence on the outcome through the multiple paths. 

We compare the performance of the proposed learning approach proposed (labeled ``DR''), the IPW-based approach (labeled ``IPW'') defined as (\ref{eq:IPW_1}) and (\ref{eq:IPW_2}), and Q-learning (labeled ``Q-learn''). For each method, we use the random forest of \citet{Athey_et_al_2019} to estimate the nuisance components. When $p$ is large, the estimations of the nuisance components become noisy.



For the proposed approach and IPW-based approach, we use the following class of DTRs: $\Pi=\Pi_{1} \times \Pi_{2}$ with
\begin{align*}
\Pi_{1}= & \left\{ 1\left\{ \left(1,X_{1}^{(1)}\right)\boldsymbol{\beta}_{1}\geq0\right\} :\boldsymbol{\beta}_{1}\in\mathbb{R}^{2}\right\} ,\\
\Pi_{2}= & \left\{ 1\left\{ \left(1,A_{1},X_{2}\right)\boldsymbol{\beta}_{2}\geq0\right\} :\boldsymbol{\beta}_{2}\in\mathbb{R}^{3}\right\} .
\end{align*}
This class of DTRs contains the first-best policy of the second stage, satisfying Assumption \ref{asm:first-best}. 
For each of the proposed and IPW-based approach, the optimization problem at each step can be formulated as Mixed Integer Linear Programming (see \citeauthor{Sakaguchi_2021}(\citeyear{Sakaguchi_2021}, Appendix B))
(MILP) problems, for which some efficient softwares (e.g., CPLEX; Gurobi) are available .
We set $K=5$ for the proposed approach.

Figure \textcolor{blue}{1} shows the results of 200 simulations with sample sizes $n=$
200, 500, 800, 1100, and 1400, where we calculate the mean welfare achieved by each estimated DTR with 3,000 observations randomly drawn from the same DGP used in the estimation.
The results show that the B-DR performs better than B-DEWM and Q-learning in terms of the mean welfare especially when the sample size is not large.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{figure}\label{fig:simulation_results}
%\centering \caption{Monte Carlo Simulation Results}
%\begin{center}
%\includegraphics[width=12cm,height=8cm]{figures/sim_result_3methods.png}
%\end{center}   
%\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}\label{sec:conclusion}
We studied statistical learning of the optimal DTRs. We proposed two doubly-robust learning approaches using observational data under the assumption of sequential ignorability. The first approach solves the treatment assignment problem at each stage through backward induction, where we combine the batch Q-learning and inverse propensity score estimators to construct the doubly-robust estimator of the treatment score at each stage. 
The second approach solves the whole dynamic treatment assignment problem simultaneously across all stages, where we use the doubly-robust estimator of the welfare function. Our main results show that based on the doubly-robust estimators of the treatment scores or welfare function and using cross-fitting, each of the approaches has the property of the doubly-robust learning of the optimal DTRs. Furthermore, under relatively mild conditions on the MSE convergence rate for the estimators of the nuisance components, we show that the proposed approaches achieve the minimax optimal convergence rate $O_ {p}(1/\sqrt{n})$ of welfare regret even if nuisance components are non-parametrically estimated. The simulation study confirms this theoretical property in finite sample setting.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\part*{Appendix}

%This appendix provides the proof of Theorem \ref{thm:main_theorem_backward}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Proof of Theorem \ref{thm:main_theorem_backward}}\label{app:main_proof}

%\begin{comment}
This appendix presents the proof of Theorem \ref{thm:main_theorem_backward} along with some auxiliary lemmas. %The proof of Theorem \ref{thm:main_theorem_backward_Q} follows the similar argument and is presented in Appendix ?? 
We consider to derive asymptotic upper bound on $R(\check{\pi})$. However, this is non-trivial task because each component of DTR $\check{\pi}=(\check{\pi}_{1},\ldots,\check{\pi}_{T})$ is separately estimated rather than simultaneously estimated. If the DTR is simultaneously estimated across all stages, we can directly apply the theoretical analysis of \cite{Athey_Wager_2020} and \cite{zhou2022offline} for the doubly-robust policy learning. The sequential estimation makes the analysis challenging.%hinders us from directly applying their analytical approach.
%Evaluating the sequentially estimated DTR is not straight forward.

Given the estimated DTR $\check{\pi}$, for any $\pi_t \in \Pi_{t}$, we define $R_{t}^{\check{\pi}_{t:T}}(\pi_t) \equiv V_{t}(\pi_t,\check{\pi}_{(t+1):T}) - V_{t}(\check{\pi}_{t:T})$ for $t=1,\ldots,T$. $R_{t}^{\check{\pi}_{t:T}}(\pi_t)$ measures the deviation of the policy $\pi_{t}$ from the sequence of the estimated policies $\check{\pi}_{t:T}$ in stage $t$ with respect to the policy value function. Note that $R_{T}^{\check{\pi}_{T}}(\pi_t)= V_T(\pi_T) - V_T(\check{\pi}_T)$.  The following lemma gives a useful result to analyze $\check{\pi}$.

\bigskip

\begin{lemma}\label{lem:helpful_lemma}
Under Assumptions \ref{asm:sequential independence}, \ref{asm:overlap}, and \ref{asm:first-best}, the regret of $\check{\pi}$ is bounded from above as
\begin{align}
    R(\check{\pi}) \leq R_{1}^{\check{\pi}_{1:T}}(\pi_{1}^{\ast}) + \sum_{t=2}^{T} \frac{2^{t-2}}{\eta^{t-1}} R_{t}^{\check{\pi}_{t:T}}(\pi_{t}^{\ast}). \label{eq:decomposition_result}
\end{align}
\end{lemma}

\bigskip

\begin{proof}
See Appendix \ref{app:proofs_of_main_lemmas}.
\end{proof}

\bigskip

The result (\ref{eq:decomposition_result}) enables us to evaluate $R(\check{\pi})$ through evaluating $R_{t}^{\check{\pi}_{t:T}}(\pi_{t}^{\ast})$ for each $t$, to which we can apply a standard argument of the statistical learning theory (\cite{Lugosi_2002}) as we will see below.

Given a fixed DTR $\pi=(\pi_1,\ldots,\pi_T)$, let us define
\begin{align*}
\Gamma_{i,T}(a_{T}) &\equiv \frac{ Y_{i} - Q_{T}(H_{i,T},A_{i,T}) }{e_{T}(H_{i,T},A_{i,T})}\cdot 1\{A_{i,T}=a_T\} + Q_{T}(H_{i,T},a_{T}),\\
\widehat{\Gamma}_{i,T}(a_T) &\equiv \frac{ Y_{i}- \widehat{Q}_{T}^{-k(i)}(H_{i,T},A_{i,T}) }{\hat{e}_{T}^{-k(i)}(H_{i,T},A_{i,T})}\cdot1\{A_{i,T} = a_T\} + 
    \widehat{Q}_{T}^{-k(i)}\left(H_{i,T},a_{T}\right),\\
\Gamma_{i,T-1}^{\pi_T}(a_{T-1}) &\equiv \frac{ \Gamma_{i,T}(\pi_{T}(H_{i,T})) - Q_{T-1}(H_{i,T-1},A_{i,T-1},\pi_{T}(H_{i,T})) }{e_{T-1}(H_{i,T-1},A_{i,T-1})}\cdot 1\{A_{i,T-1}=a_{T-1}\} \\
&+ Q_{T-1}(H_{i,T-1},A_{i,T-1},\pi_{T}(H_{i,T})),\\
\widehat{\Gamma}_{i,T-1}^{\pi_T}(a_{T-1}) &\equiv \frac{\widehat{\Gamma}_{i,T}(\pi_T(H_{i,T}))- \widehat{Q}_{T-1}^{-k(i)}(H_{i,T-1},A_{i,T-1},\pi_{T}(H_{i,T})) }{\hat{e}_{T-1}^{-k(i)}(H_{i,T-1},A_{i,T-1})}\cdot1\{A_{i,T-1} = a_{T-1}\} \\
&+    \widehat{Q}_{T-1}^{-k(i)}\left(H_{i,T-1},a_{T-1},\pi_{T-1}(H_{i,T-1})\right),
\end{align*}
and, recursively for $t=T-2,\ldots,1$,
\begin{align*}
\Gamma_{i,t}^{\pi_{(t+1):T}}(a_{t}) &\equiv \frac{ \Gamma_{i,t+1}^{\pi_{(t+2):T}}(\pi_{t+1}(H_{i,t+1})) - Q_{t}(H_{i,t},A_{i,t},\pi_{t+1}(H_{i,t+1}),\ldots,\pi_{T}(H_{i,T})) }{e_{t}(H_{i,t},A_{i,t})}\cdot 1\{A_{i,t}=a_{t}\} \\
&+ Q_{t}(H_{i,t},a_{t},\pi_{t+1}(H_{i,t+1}),\ldots,\pi_{T}(H_{i,T})),\\
\widehat{\Gamma}_{i,t}^{\pi_{(t+1):T}}(a_{t}) &\equiv \frac{ \widehat{\Gamma}_{i,t+1}^{\pi_{(t+2):T}}(\pi_{t+1}(H_{i,t+1})) - \widehat{Q}_{t}^{-k(i)}(H_{i,t},A_{i,t},\pi_{t+1}(H_{i,t+1}),\ldots,\pi_{T}(H_{i,T})) }{\hat{e}_{t}^{-k(i)}(H_{i,t},A_{i,t})}\cdot 1\{A_{i,t}=a_{t}\} \\
&+ \widehat{Q}_{t}^{-k(i)}(H_{i,t},a_{t},\pi_{t+1}(H_{i,t+1}),\ldots,\pi_{T}(H_{i,T})).
\end{align*}
Note that $(1/n)\sum_{i=1}^{n}\Gamma_{i,t}^{\pi_{(t+1):T}}\left(a_t\right)$ is an oracle estimate of the action value function $V_{t}^{\pi_{(t+1):T}}(a_t)$ with oracle access to $\{Q_{s}(\cdot,\cdot)\}_{s=t,\ldots,T}$ and $\{e_{s}(\cdot,\cdot)\}_{s=t,\ldots,T}$.
%\footnote{Note that $V_{t}^{\pi_{(t+1):T}}(a_t)$ is different from the optimal action value $E\left[Q_{t}(H_t,a_t)\right]$ unless $\pi_{(t+1):T} \neq \pi_{(t+1):T}^{\ast}$. Hence, $(1/n)\sum_{i=1}^{n}\Gamma_{i,t}^{\pi_{(t+1):T}}\left(a_t\right)$ is not a doubly-robust estimator of the optimal value function $E\left[Q_{t}(H_t,a_t)\right]$.} 

Following the analysis of \cite{zhou2022offline}, we define the policy value difference function $\Delta_{t}(\cdot;\cdot):\Pi_{t:T} \times \Pi_{t:T} \rightarrow \Real$, the oracle influence difference function $\widetilde{\Delta}_{t}(\cdot;\cdot):\Pi_{t:T} \times \Pi_{t:T} \rightarrow \Real$, and the estimated policy value difference function $\widehat{\Delta}_{t}(\cdot;\cdot):\Pi_{t} \times \Pi_{t:T} \rightarrow \Real$, respectively, as follows: For $\pi_{t:T}^{a}=(\pi_{t}^{a},\ldots,\pi_{T}^{a})$ and $\pi_{t:T}^{b}=(\pi_{t}^{b},\ldots,\pi_{T}^{b})$ in $\Pi_{t:T}$,
\begin{align}
\Delta_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})&\equiv V_{t}(\pi_{t:T}^{a}) - V_{t}(\pi_{t:T}^{b}) \label{eq:Delta}
\end{align}
for $t=1,\ldots,T$, and
\begin{align}
 &\widetilde{\Delta}_{T}(\pi_{T}^{a};\pi_{T}^{b})\equiv \frac{1}{n}\sum_{i=1}^{n} \Gamma_{i,T}^{\dag}\left(\pi_{T}^{a}(H_{i,T})\right) - \frac{1}{n}\sum_{i=1}^{n} \Gamma_{i,T}^{\dag}\left(\pi_{T}^{b}(H_{i,T})\right), \label{eq:Delta_tilde_1}\\
  &\widehat{\Delta}_{T}(\pi_{T}^{a};\pi_{T}^{b}) \equiv  \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,T}^{\dag}\left(\pi_{T}^{a}(H_{i,T})\right) -  \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,T}^{\dag}\left(\pi_{T}^{b}(H_{i,T})\right), \nonumber \\
 &\widetilde{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b}) \equiv \frac{1}{n}\sum_{i=1}^{n} \Gamma_{i,t}^{\dag,\pi_{(t+1):T}^{a}}\left(\pi_{t}^{a}(H_{i,t})\right) - \frac{1}{n}\sum_{i=1}^{n} \Gamma_{i,t}^{\dag,\pi_{(t+1):T}^{b}}\left(\pi_{t}^{b}(H_{i,t})\right), \label{eq:Delta_tilde_2}\\
 & \widehat{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b}) \equiv  \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\dag,\pi_{(t+1):T}^{a}}\left(\pi_{t}^{a}(H_{i,t})\right) -  \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\dag,\pi_{(t+1):T}^{b}}\left(\pi_{t}^{b}(H_{i,t})\right) \nonumber
\end{align}
for $t=1,\ldots,T-1$.
Note that $\widetilde{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})$ is an unbiased estimator of the policy value difference function $\Delta_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})$. From the definitions, $R_t(\pi_{t}^{\ast}) = \Delta_{t}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right)$.

%Noting that Lemma \ref{lem:helpful_lemma} enables us to evaluate $R(\check{\pi})$ through evaluating $\Delta_{t}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right)$ for each $t$, 
In what follows, we evaluate $R_{t}^{\check{\pi}_{t:T}}(\pi_{t}^{\ast})$ for each $t$. A standard argument of the statistical learning theory (\cite{Lugosi_2002}) gives
\begin{align}
    R_{t}^{\check{\pi}_{t:T}}(\pi_{t}^{\ast})  
    &=\Delta_{t}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right) 
    \nonumber\\%& = \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}\left(\pi_{t}^{\ast}(H_{i,t})\right) - \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}\left(\check{\pi}_{t}(H_{i,t})\right) \\
    %&+ \Delta_{t}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right) - \widehat{\Delta}_{t}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right)\\
    &\leq \Delta_{t}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right) - \widehat{\Delta}_{t}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right) \nonumber \\
    &\leq 
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} |\Delta_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b}) - \widehat{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})| \nonumber \\
    & \leq  \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} |\Delta_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b}) - \widetilde{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})| 
    + \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_t} |\widehat{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b}) - \widetilde{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})|, \label{eq:standard_inequality}
\end{align}
where the first inequality follows because $\check{\pi}_{t}$ maximizes  $(1/n)\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}\left(\pi_{t}(H_{i,t})\right)$ over $\Pi_t$; hence, $\widehat{\Delta}_{t}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right) \leq 0$. 

We can now evaluate $R_{t}^{\check{\pi}_{t:T}}(\pi_{t}^{\ast})$ through evaluating $\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} |\Delta_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b}) - \widetilde{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})|$ and $\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} |\widehat{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b}) - \widetilde{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})|$. As for the former, we apply the uniform concentration result of \citeauthor{zhou2022offline} (\citeyear{zhou2022offline}, Lemma 2) for the oracle influence difference function to obtain the following lemma.

\bigskip
\begin{lemma}\label{lem:bound_influence_difference_function}
Suppose that Assumptions \ref{asm:sequential independence}, \ref{asm:bounded outcome}, \ref{asm:overlap}, and \ref{asm:bounded entropy} hold. Then for any stage $t$ and $\delta \in (0,1)$, with probability at least $1-2\delta$, the following holds:
\begin{align}
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} \left|\widetilde{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})-\Delta_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})\right| &\leq \left(54.4 \sqrt{2}\kappa(\Pi_{t:T}) + 435.2 + \sqrt{2 \log \frac{1}{\delta}}\right)\sqrt{\frac{V_{t:T}^{\ast}}{n}} \nonumber \\
    &+ o\left(\frac{1}{\sqrt{n}}\right), \label{eq:bound_influence_difference_function}
\end{align}
where 
    $V_{t:T}^{\ast} := \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}}E
    \left[\left(\Gamma_{i}^{\dag,\pi_{(t+1):T}^{a}}(\pi_{t}^{a}(H_{i,t})) - \Gamma_{i}^{\dag,\pi_{(t+1):T}^{b}}(\pi_{t}^{b}(H_{i,t})) \right)^2\right] < \infty$.
\end{lemma}
\bigskip

\begin{proof}
See Appendix \ref{app:preliminary_results}.
\end{proof}

\bigskip

As for $\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} |\widehat{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b}) - \widetilde{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b})|$, extending the analytical strategy of \cite{Athey_Wager_2020} and \cite{zhou2022offline}, 
which leverages orthogonality conditions and the cross-fitting, to the sequential setting, we can obtain the following lemma. 

\bigskip

\begin{lemma}\label{lem:asymptotic_estimated_policy_difference_function}
Suppose that Assumptions \ref{asm:sequential independence}, \ref{asm:bounded outcome}, \ref{asm:overlap}, and \ref{asm:rate_of_convergence_backward_Q} hold. Then, for any stage $t$, the following holds:
\begin{align*}
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} \left|\widehat{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})- \widetilde{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})\right| = o_{p}(n^{-\min\{1/2,\tilde{\tau}_{B}/2\}}).
\end{align*}
\end{lemma}

\bigskip

\begin{proof}
See Appendix \ref{app:proofs_of_main_lemmas}.
\end{proof}

\bigskip

Combing the inequality (\ref{eq:standard_inequality}) with Lemmas \ref{lem:bound_influence_difference_function} and \ref{lem:asymptotic_estimated_policy_difference_function}, we obtain 
\begin{align}
    R_{t}^{\check{\pi}_{t:T}}(\pi_{t}^{\ast}) = O_{p}\left(\kappa(\Pi_{t:T}) \cdot n^{-1/2}\right) + o_{p} (n^{-\min\{1/2,\tilde{\tau}_{B}/2\}}) \label{eq:result_deviation_regret}
\end{align}
for all $t=1,\ldots,T$. This result eventually shows Theorem \ref{thm:main_theorem_backward} by using Lemma \ref{lem:helpful_lemma}.$\Box$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Preliminary Results and Proof of Lemma \ref{lem:bound_influence_difference_function}}\label{app:preliminary_results}

This section gives preliminary results for the proofs of Theorems \ref{thm:main_theorem_backward} and \ref{thm:main_theorem_backward_Q}.

The following lemma relates the $\epsilon$-Hamming covering numbers of stage-specific policy classes to that of a class of sequence of policies, and will be used in the proof of Lemma \ref{lem:entropy_integral_bound}.

\bigskip{}

\begin{lemma}\label{lem:covering_number_of_product}
Given a class of DTRs $\Pi=\Pi_1 \times \cdots \times \Pi_T$, for any integers $s$ and $t$ such that $1\leq s \leq t \leq T$, the following holds: $N_{d_h}((t-s + 1)\epsilon,\pi_{t:T}) \leq \sum_{\ell=s}^{t}N_{d_h}(\epsilon,\Pi_{\ell})$.
\end{lemma}

\begin{proof}
Fix a set of history points $\{h_{t}^{(1)},\ldots,h_{t}^{(n)}\} \subseteq \MH_h$. For any $\ell$ ($\leq t$), let $h_{\ell}^{(i)} \subseteq h_{t}^{(i)}$ be the partial history up to stage $\ell$. Let $K_\ell := N_{d_h}(\epsilon,\Pi_\ell,\{h_{\ell}^{(1)},\ldots,h_{\ell}^{(n)}\})$. For each $\ell\in \{s,\ldots,t\}$, we denote by $\widetilde{\Pi}_{\ell}:=\left(\pi_{\ell}^{(1)},\ldots,\pi_{\ell}^{(K_{\ell})}\right)$ the set of policies such that for any $\pi_{\ell} \in \Pi_{\ell}$, there exists $\pi_{\ell}^{(i)}$ satisfying $d_{h}(\pi_{\ell},\pi_{\ell}^{(i)}) \leq \epsilon$. Such a set of policies exists from the definition of $N_{d_h}(\epsilon,\Pi_\ell,\{h_{\ell}^{(1)},\ldots,h_{\ell}^{(n)}\})$.

Fix $\pi_{t:T} \in \Pi_{t:T}$, and define $\widetilde{\Pi}_{s:t}:=\widetilde{\Pi}_{s}\times \cdots \times \widetilde{\Pi}_{t}$. Let $\tilde{\pi}_{s:t}=(\tilde{\pi}_s,\ldots,\tilde{\pi}_t) \in\widetilde{\Pi}_{s:t}$ be such that for any $\ell \in \{s,\ldots,t\}$, $d_{h}(\pi_{\ell},\tilde{\pi}_{\ell}) \leq \epsilon$. Then
\begin{align*}
    d_h(\pi_{t:T},\tilde{\pi}_{s:t}) & = \frac{1}{n}\sum_{i=1}^{n}1\{\pi_{s}(h_{s}^{(i)})\neq \tilde{\pi}_{s}(h_{s}^{(i)}) \vee \cdots \vee \pi_{t}(h_{t}^{(i)})\neq \tilde{\pi}_{t}(h_{t}^{(i)})\} \\
    & \leq \sum_{\ell=s}^{t} \left(\frac{1}{n}\sum_{i=1}^{n}1\{\pi_{\ell}(h_{\ell}^{(i)})\neq \tilde{\pi}_{\ell}(h_{\ell}^{(i)})\}\right)\\
    &= \sum_{\ell=s}^{t} d_h(\pi_{\ell},\tilde{\pi}_{\ell}) \leq (t-s+1)\epsilon.
\end{align*}
Therefore, for any $\pi_{t:T} \in \Pi_{t:T}$, there exists $\tilde{\pi}_{s:t}\in \widetilde{\Pi}_{s:t}$ such that $d_h(\pi_{t:T},\tilde{\pi}_{s:t}) \leq (t-s+1)\epsilon$. Since $\left|\widetilde{\Pi}_{s:t}\right| = \prod_{\ell=s}^{t}\left|\widetilde{\Pi}_{\ell}\right|=\prod_{\ell=s}^{t}N_{d_h}(\epsilon,\Pi_\ell,\{h_{\ell}^{(1)},\ldots,h_{\ell}^{(n)}\})$, where $|\cdot|$ denotes the cardinality, we have
\begin{align*}
    N_{d_h}\left((t-s+1)\epsilon,\pi_{t:T},\{h_{t}^{(1)},\ldots,h_{t}^{(n)}\}\right) \leq \prod_{\ell=s}^{t}N_{d_h}\left(\epsilon,\Pi_\ell,\{h_{\ell}^{(1)},\ldots,h_{\ell}^{(n)}\}\right).
\end{align*}
Because this holds for any $n$ and set of history points $\{h_{t}^{(1)},\ldots,h_{t}^{(n)}\}$, the result in the statement holds.
\end{proof}

\bigskip

Using Lemma \ref{lem:covering_number_of_product}, we give the proof of Lemma \ref{lem:entropy_integral_bound} as follows.

\bigskip
\noindent
\textit{Proof of Lemma \ref{lem:entropy_integral_bound}}.
Note that $\Pi=\Pi_{1:T}$. Applying Lemma \ref{lem:covering_number_of_product} to $\Pi$, we have $N_{H}(\epsilon^2,\Pi) \leq \prod_{t=1}^{T} N_{H}(\epsilon^2/T,\Pi_{t})$. Then
\begin{align*}
\kappa(\Pi)&=\int_{0}^{1}\sqrt{\log N_{H}\left(\epsilon^{2},\Pi\right)}d\epsilon 
\leq\int_{0}^{1}\sqrt{\sum_{t=1}^{T}\log N_{H}(\epsilon^{2}/T,\Pi_{t})}d\epsilon\\
&\leq \sum_{t=1}^{T} \int_{0}^{1}\sqrt{\log N_{H}(\epsilon^{2}/T,\Pi_{t})}d\epsilon\\
&\leq T \int_{0}^{1}\sqrt{\log C+D\left(\frac{\sqrt{T}}{\epsilon}\right)^{2\omega}}d\epsilon \\
&\leq T \int_{0}^{1}\sqrt{\log C}d\epsilon+ T\int_{0}^{1}\sqrt{D\left(\frac{\sqrt{T}}{\epsilon}\right)^{2\omega}}d\epsilon\\
&=T\sqrt{\log C}+\sqrt{T^{(2+\omega)}}\sqrt{D}\int_{0}^{1}\epsilon^{-\omega}d\epsilon=T\sqrt{\log C}+\frac{\sqrt{T^{(2+\omega)}D}}{1-\omega} \\
&<\infty,    
\end{align*}
where the third and last lines follow from Assumption \ref{asm:bounded entropy}. 
$\Box$

\bigskip

The following lemma, which directly follows from Lemma 2 in \cite{zhou2022offline} and its proof, plays important roles in the proofs of Theorems 
\ref{thm:main_theorem_backward} and \ref{thm:main_theorem_backward_Q}.

\bigskip{}

\begin{lemma} \label{lem:concentration inequality_influence difference function}
Fix $t \in \{ 1,\ldots,T\}$. For any $\underline{a}_{t:T}\in \underline{\MA}_{t:T}$, let $\{\Gamma_{i}(\underline{a}_{t:T})\}_{i=1}^{n}$ be i.i.d. random variables with bounded supports. For any $\pi_{t:T} \in \Pi_{t:T}$, let $\widetilde{Q}(\pi_{t:T}) := \frac{1}{n} \sum_{i=1}^{n}  \Gamma_{i}(\pi_{t:T})$, where $\Gamma_{i}(\pi_{t:T}) := \Gamma_{i}((\pi_{t}(H_{i,t}),\ldots,\pi_{T}(H_{i,T})))$, and $Q(\pi_{t:T}):= E[\widetilde{Q}(\pi_{t:T})]$. For any $\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}$, denote $\widetilde{\Delta}(\pi_{t:T}^{a},\pi_{t:T}^{b})= \widetilde{Q}(\pi_{t:T}^{a}) - \widetilde{Q}(\pi_{t:T}^{b})$ and $\Delta(\pi_{t:T}^{a},\pi_{t:T}^{b})= Q(\pi_{t:T}^{a}) - Q(\pi_{t:T}^{b})$. Then, under Assumption \ref{asm:bounded entropy}, the following holds: For any $\delta \in (0,1)$, with probability at least $1-2\delta$,
\begin{align*}
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} \left|\widetilde{\Delta}(\pi_{t:T}^{a},\pi_{t:T}^{b})-\Delta(\pi_{t:T}^{a},\pi_{t:T}^{b})\right| &\leq \left(54.4 \sqrt{2}\kappa(\Pi_{t:T}) + 435.2 + \sqrt{2 \log \frac{1}{\delta}}\right)\sqrt{\frac{V_{t:T}^{\ast}}{n}} \\
    &+ o\left(\frac{1}{\sqrt{n}}\right),
\end{align*}
where 
    $V_{t:T}^{\ast} := \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}}E
    \left[\left(\Gamma_{i}(\pi_{t:T}^{a}) - \Gamma_{i}(\pi_{t:T}^{b}) \right)^2\right]$.
\end{lemma}

\bigskip

Lemma \ref{lem:concentration inequality_influence difference function} leads to the proof of Lemma \ref{lem:bound_influence_difference_function}.

\bigskip

\noindent
\textit{Proof of  Lemma \ref{lem:bound_influence_difference_function}.} With some abuse of notation, for any $\underline{a}_{1:T} \in \underline{\MA}_{1:T}$, define recursively 
\begin{align*}
\Gamma_{i,T}^{\dag}\left({a_T}\right) &:= \frac{ Y_{i} - Q_{T}(H_{i,T},a_T) }{e_{T}(H_{i,T},a_T)}\cdot 1\{A_{i,T}=a_T\} + Q_{T}(H_{i,T},a_{T}), \\
\Gamma_{i,t}^{\dag}\left(\underline{a}_{t:T}\right) &:= \frac{ \Gamma_{i,t+1}\left(\underline{a}_{(t+1):T}\right) - Q_{t}(H_{i,t},\underline{a}_{t:T}) }{e_{t}(H_{i,t},a_{t})}\cdot 1\{A_{i,T-1}=a_{t}\} + Q_{t}(H_{i,t},\underline{a}_{t:T}), 
\end{align*}
for $t=T-1,\ldots,1$. For any $t$ and $\pi_{t:T} \in \Pi_{t:T}$, we also define 
$\Gamma_{i,t}(\pi_{t:T}) := \Gamma_{i,t}((\pi_{t}(H_{i,t}),\ldots,\pi_{T}(H_{i,T})))$
for $t=1,\ldots,T$. Then $\widetilde{\Delta}(\pi_{T}^{a},\pi_{T}^{b})$ defined in (\ref{eq:Delta_tilde_1}) corresponds to $n^{-1}\sum_{i=1}^{n}\left(\Gamma_{i,t}(\pi_{t:T}^{a}) - \Gamma_{i,t}(\pi_{t:T}^{b})\right)$, and $\Delta(\pi_{t:T}^{a},\pi_{t:T}^{b})$ defined in (\ref{eq:Delta_tilde_2}) corresponds to $E\left[n^{-1}\sum_{i=1}^{n}\left(\Gamma_{i,t}(\pi_{t:T}^{a})\right] - E\left[n^{-1}\sum_{i=1}^{n}\Gamma_{i,t}(\pi_{t:T}^{b})\right)\right]$ under Assumption \ref{asm:sequential independence}. Note that $\{\Gamma_{i,t}^{\dag}\left(\underline{a}_{t:T}\right)\}_{i=1}^{n}$ are i.i.d. random variables under Assumptions \ref{asm:bounded outcome} and \ref{asm:overlap} for any $t=1,\ldots,T$. Therefore, applying Lemma \ref{lem:concentration inequality_influence difference function} with $\Gamma_{i}(\underline{a}_{t:T})=\Gamma_{i,t}(\underline{a}_{t:T})$ leads to the result (\ref{eq:bound_influence_difference_function}). $V_{t:T}^{\ast}$ is finite under Assumptions \ref{asm:bounded outcome} and \ref{asm:overlap}.
$\Box$

%\bigskip{}

\begin{comment}
\begin{lemma} \label{lem:concentration inequality_influence difference function}
Fix $t \in \{ 1,\ldots,T\}$. Let $\{\Gamma_{i,t}\}_{i=1}^{n}$ be i.i.d. random vectors with bounded support. For any $\pi_{t} \in \Pi_{t}$, let $\widetilde{Q}_{t}(\pi_{t}) := \frac{1}{n} \sum_{i=1}^{n}\left\langle \pi_{t}(H_{i,t}), \Gamma_{i,t}\right\rangle$ and $Q_{t}(\pi_{t}):= E[\widetilde{Q}_{t}(\pi_{t})]$. For any $\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t}$, denote $\widetilde{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b})= \widetilde{Q}_{t}(\pi_{t}^{a}) - \widetilde{Q}_{t}(\pi_{t})$ and $\Delta_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b})= Q_{t}(\pi_{t}^{a}) - Q_{t}(\pi_{t})$. Then, under Assumption \ref{asm:bounded entropy}, the following hold: For any $\delta \in (0,1)$, with probability at least $1-2\delta$,
\begin{align*}
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_t} \left|\widetilde{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b})-\Delta_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right| \leq \left(54.4 \sqrt{2}\kappa(\Pi_t) + 435.2 + \sqrt{2 \log \frac{1}{\delta}}\right)\sqrt{\frac{V_{t}^{\ast}}{n}} + o\left(\frac{1}{\sqrt{n}}\right),
\end{align*}
where 
    $V_{t}^{\ast} := \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_t}E \left|\left\langle \pi_{t}^{a}(H_{i,t}) - \pi_{t}(H_{i,t}),\Gamma_{i,t} \right\rangle^{2} \right|$.
\end{lemma}
\end{comment}

\bigskip{}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proofs of Lemmas \ref{lem:helpful_lemma}  and \ref{lem:asymptotic_estimated_policy_difference_function}}\label{app:proofs_of_main_lemmas}

We provide the proofs of Lemmas \ref{lem:helpful_lemma}, \ref{lem:bound_influence_difference_function}, and \ref{lem:asymptotic_estimated_policy_difference_function} in this section. Thereby, we can prove Theorem \ref{thm:main_theorem_backward} through the discussion in Section \ref{seq:main_proof}.

The following lemma is a general version of Lemma \ref{lem:helpful_lemma}.

\bigskip

\begin{lemma}\label{lem:helpful_lemma_general}
Fix $\pi=(\pi_1,\ldots,\pi_T)\in \Pi$. Let  $R_{t}^{\pi_{t:T}}(\tilde{\pi}_t) := V_{t}(\tilde{\pi}_t,\pi_{(t+1):T}) - V_{t}(\pi_{t:T})$ for any $\tilde{\pi}_t \in \Pi_t$. Then, under Assumptions \ref{asm:sequential independence}, \ref{asm:overlap}, and \ref{asm:first-best}, the regret of $\pi$ is bounded from above as
\begin{align}
    R(\pi) \leq R_{1}^{\pi_{1:T}}(\pi_{1}^{\ast}) + \sum_{t=2}^{T} \frac{2^{t-2}}{\eta^{t-1}} R_{t}^{\pi_{t:T}}(\pi_{t}^{\ast}). \label{eq:decomposition_result}
\end{align}
\end{lemma}

\begin{proof}
For any $t$, define $R_{t}(\pi_{t:T}) := V_{t}(\pi_{t:T}^{\ast}) - V_{t}(\pi_{t:T})$, which is a partial regret of $\pi_{t:T}$ in stage $t$. 

For any integers $s$ and $t$ such that $1 \leq t < s \leq T$,
\begin{align}
    %&\Delta_{t}\left(\pi_{t:T}^{\ast};\pi_{t:(s-1)}^{\ast},\pi_{s:T}\right) %\nonumber \\
    %&= 
    &V_{t}(\pi_{t}^{\ast},\ldots,\pi_{T}^{\ast}) - V_{t}(\pi_{t}^{\ast},\ldots,\pi_{s-1}^{\ast},\pi_{s},\ldots,\pi_{T}) \nonumber \\
    &= E\left[\frac{\prod_{\ell=t}^{s-1}1\{A_{\ell}=\pi_{\ell}^{\ast}(H_{\ell})\}}{\prod_{\ell=t}^{s-1}e_{\ell}(H_{\ell},A_{\ell})}\cdot \left(Q_{s-1}^{\pi_{s:T}^{\ast}}\left(H_{s-1},A_{s-1}\right) - Q_{s-1}^{\pi_{s:T}}\left(H_{s-1},A_{s-1}\right)\right)\right] \nonumber \\
      &\leq \frac{1}{\eta^{s-t}} \left(V_{s}(\pi_{s:T}^{\ast}) - V_{s}(\pi_{s:T})\right) \nonumber \\
    &= \frac{1}{\eta^{s-t}}R_{s}\left(\pi_{t:T}\right), \label{eq:bound_delta}
\end{align}
where the first equality follows from Assumption \ref{asm:sequential independence} and the inequality follows from Assumptions \ref{asm:overlap} and \ref{asm:first-best}.

For $t=T$ and $T-1$, we have
\begin{align*}
R_{T}(\pi_T) &= V_{T}(\pi_{T}^{\ast}) - V_{T}(\pi_{T}^{\at}) = R_{T}^{\pi_{T}}(\pi_{T}^{\ast});  \\
   R_{T-1}\left(\pi_{(T-1):T}\right) &= \left[V_{T-1}\left(\pi_{T-1}^{\ast},\pi_{T}^{\ast}\right) - V_{T-1}\left(\pi_{T-1}^{\ast},\pi_{T}\right) \right]
   + \left[V_{T-1}\left(\pi_{T-1}^{\ast},\pi_{T}\right) - V_{T-1}\left(\pi_{T-1},\pi_{T}\right)\right] \\
   &\leq \frac{1}{\eta}R_T(\pi_{T}) + R_{T-1}^{\pi_{(T-1):T}}\left(\pi_{T-1}^{\ast}\right), 
\end{align*}
where the inequality follows from (\ref{eq:bound_delta}).

Generally, for $k=2,\ldots,T-1$, it follows that
\begin{align*}
    &R_{T-k}\left(\pi_{(T-k):T}\right) \\
    &= V_{T-k}\left(\pi_{T-k}^{\ast},\ldots,\pi_{T}^{\ast}\right) - V_{T-k}\left(\pi_{T-k},\ldots,\pi_{T}\right) \\ &=\sum_{s=T-k}^{T}\left[V_{T-k}\left(\pi_{T-k}^{\ast},\ldots,\pi_{s}^{\ast},\pi_{s+1},\ldots,\pi_{T}\right) - V_{T-k}\left(\pi_{T-k}^{\ast},\ldots,\pi_{s-1}^{\ast},\pi_{s},\ldots,\pi_{T}\right)\right] \\
    &=\sum_{s=T-k+1}^{T}\left[V_{T-k}\left(\pi_{T-k}^{\ast},\ldots,\pi_{s}^{\ast},\pi_{s+1},\ldots,\pi_{T}\right) - V_{T-k}\left(\pi_{T-k}^{\ast},\ldots,\pi_{s-1}^{\ast},\pi_{s},\ldots,\pi_{T}\right)\right]  \\
    &+ R_{T-k}^{\pi_{(T-k):T}}(\pi_{T-k}^{\ast})\\
    %&\leq \sum_{s=T-k}^{T}\left[V_{T-k}\left(\pi_{T-k}^{\ast},\ldots,\pi_{T}^{\ast}\right) - V_{T-k}\left(\pi_{T-k}^{\ast},\ldots,\pi_{s-1}^{\ast},\pi_{s},\ldots,\pi_{T}\right)\right]\\
    &\leq \sum_{s=T-k+1}^{T}\left[V_{T-k}\left(\pi_{T-k}^{\ast},\ldots,\pi_{T}^{\ast}\right) - V_{T-k}\left(\pi_{T-k}^{\ast},\ldots,\pi_{s-1}^{\ast},\pi_{s},\ldots,\pi_{T}\right)\right] + R_{T-k}^{\pi_{(T-k):T}}(\pi_{T-k}^{\ast})\\
    &\leq \sum_{s=T-k+1}^{T}\frac{1}{\eta^{s-T+k}} R_{s}\left(\pi_{t:T}\right)+ R_{T-k}^{\pi_{(T-k):T}}(\pi_{T-k}^{\ast}),
\end{align*}
where the second equality follows from the telescoping sum; the third equality follows from the definition of $R_{T-k}^{\pi_{(T-k):T}}(\pi_{T-k}^{\ast})$; the first inequality follows from Assumption \ref{asm:first-best}; the last line follows from (\ref{eq:bound_delta}).

Then, recursively, the following hold:
\begin{align*}
    R_{T-1}\left(\pi_{(T-1):T}\right) &\leq \frac{1}{\eta}R_{T}\left(\pi_{T}\right)+ R_{T-1}^{\pi_{(T-1):T}}(\pi_{T-1}^{\ast}) = \frac{1}{\eta}R_{T}^{\pi_{T}}\left(\pi_{T}^{\ast}\right)+ R_{T-1}^{\pi_{(T-1):T}}(\pi_{T-1}^{\ast}) \\
    R_{T-2}\left(\pi_{(T-2):T}\right) &\leq \frac{1}{\eta}R_{T-1}\left(\pi_{(T-1):T}\right)+
    \frac{1}{\eta^2}R_{T}\left(\pi_{T}\right)+R_{T-2}^{\pi_{(T-2):T}}(\pi_{T-2}^{\ast}) \\
    &\leq \frac{2}{\eta^2}R_{T}^{\pi_{T}}\left(\pi_{T}^{\ast}\right) + \frac{1}{\eta}R_{T-1}^{\pi_{(T-1):T}}\left(\pi_{T-1}^{\ast}\right) +R_{T-2}^{\pi_{(T-2):T}}(\pi_{T-2}^{\ast})\\
    & \ \ \vdots \\
R_{T-k}\left(\pi_{(T-k):T}\right) & \leq \sum_{s=1}^{k} \frac{2^{k-s}}{\eta^{k-s+1}} R_{T-s+1}^{\pi_{(T-s+1):T}}(\pi_{T-s+1}^{\ast}) + R_{T-k}^{\pi_{(T-k):T}}(\pi_{T-k}^{\ast}).
\end{align*}
Therefore, setting $k=T-1$ and noting that $R_{1}\left(\pi_{1:T}\right) = R(\pi)$, we obtain
\begin{align*}
    R(\pi) &\leq  \sum_{s=1}^{T-1} \frac{2^{T-1-s}}{\eta^{T-s}} R_{T-s+1}^{\pi_{(T-s+1):T}}(\pi_{T-s+1}^{\ast}) + R_{1}^{\pi_{1:T}}(\pi_{1}^{\ast}) \\
    & = R_{1}^{\pi_{1:T}}(\pi_{1}^{\ast}) + \sum_{s=1}^{T-1} \frac{2^{s-1}}{\eta^{s}} R_{s+1}^{\pi_{(s+1):T}}(\pi_{s+1}^{\ast}).
\end{align*}
Setting $t=s+1$ in the above equation leads to the result.
\end{proof}

\bigskip

The following is the proof of Lemma \ref{lem:helpful_lemma}.\\

\bigskip

\noindent
\textit{Proof of Lemma \ref{lem:helpful_lemma}.}
Lemma \ref{lem:helpful_lemma} follows from Lemma \ref{lem:helpful_lemma_general} with setting $\pi=\check{\pi}$.
$\Box$\\

%%%%%

\bigskip

We next gives the proof of Lemma \ref{lem:asymptotic_estimated_policy_difference_function}.
Fix $t \in \{1,\ldots,T\}$ and $\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}$. For any $\underline{a}_{t:T} \in \underline{\MA}_{t:T}$, let
\begin{align*}
    G_{i,\pi_{t:T}^a,\pi_{t:T}^{b}}^{\underline{a}_{t:T}} := \prod_{s=t}^{T}1\{\pi_{s}^{a}(H_{i,s})=a_s\} - \prod_{s=t}^{T}1\{\pi_{s}^{b}(H_{i,s})=a_s\}.
\end{align*} 
Define recursively 
\begin{align*}
\Gamma_{i,T}^{a_T} &\equiv \frac{ Y_{i} - Q_{T}(H_{i,T},a_T) }{e_{T}(H_{i,T},a_T)}\cdot 1\{A_{i,T}=a_T\} + Q_{T}(H_{i,T},a_{T}),\\
\widehat{\Gamma}_{i,T}^{a_T} &\equiv \frac{ Y_{i}- \widehat{Q}_{T}^{-k(i)}(H_{i,T},a_{T}) }{\hat{e}_{T}^{-k(i)}(H_{i,T},a_{T})}\cdot1\{A_{i,T} = a_T\} + 
    \widehat{Q}_{T}^{-k(i)}\left(H_{i,T},a_{T}\right),\\
\Gamma_{i,t}^{\underline{a}_{t:T}} &\equiv \frac{ \Gamma_{i,t+1}^{\underline{a}_{(t+1):T}} - Q_{t}(H_{i,t},\underline{a}_{t:T}) }{e_{t}(H_{i,t},a_{t})}\cdot 1\{A_{i,T-1}=a_{t}\} + Q_{t}(H_{i,t},\underline{a}_{t:T}),\\
\widehat{\Gamma}_{i,t}^{\underline{a}_{t:T}} &\equiv \frac{\widehat{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}- \widehat{Q}_{t}^{-k(i)}(H_{i,t},\underline{a}_{t:T}) }{\hat{e}_{t}^{-k(i)}(H_{i,t},A_{i,t})}\cdot1\{A_{i,t} = a_{t}\} +    \widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right),  
\end{align*}
for $t=T-1,\ldots,1$.

We also define $\widehat{\Delta}_{t}^{\underline{a}_{t:T}}(\cdot,\cdot):\Pi_{t:T}\times \Pi_{t:T} \rightarrow \Real$ and $\widetilde{\Delta}_{t}^{\underline{a}_{t:T}}(\cdot,\cdot):\Pi_{t:T}\times \Pi_{t:T} \rightarrow \Real$ as follows:
\begin{align*}
    \widehat{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) &\equiv \frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^a,\pi_{t:T}^{b}}^{\underline{a}_{t:T}} \widehat{\Gamma}_{i,t}^{\underline{a}_{t:T}}; \\
    \widetilde{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) &\equiv \frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^a,\pi_{t:T}^{b}}^{\underline{a}_{t:T}} \widetilde{\Gamma}_{i,t}^{\underline{a}_{t:T}}.
\end{align*}
Note that $\widehat{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b})$ and $\widetilde{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b})$ can be decomposed as 
\begin{align*}
    \widehat{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b}) &= \sum_{\underline{a}_{t:T} \in \underline{\MA}_{t:T}} \widehat{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b});\\ \widetilde{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b}) &= \sum_{\underline{a}_{t:T} \in \underline{\MA}_{t:T}} \widetilde{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}).
\end{align*}
 Hence, 
\begin{align*}
    \widehat{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b}) - \widetilde{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b}) = \sum_{\underline{a}_{t:T} \in \underline{\MA}_{t:T}} \left(\widehat{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) - \widetilde{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right).
\end{align*}
We will evaluate $\widehat{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) - \widetilde{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})$ for each $t$ and $\underline{a}_{t:T} \in \underline{\MA}_{t:T}$ in the following lemma.


\bigskip


\begin{lemma} \label{lem:convergence_rate_Stilde}
Suppose that Assumptions \ref{asm:sequential independence}, \ref{asm:bounded outcome}, \ref{asm:overlap}, \ref{asm:rate_of_convergence_backward_Q}, and \ref{asm:bounded entropy} hold. For any integers $s$ and $t$ such that $1\leq s \leq t \leq T$, sequences of policies $\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}$, and treatments $\underline{a}_{s:T} \in \underline{\MA}_{s:T}$, let 
\begin{align*}
    \widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) := \frac{1}{n}\sum_{i=1}^{n}&G_{\pi_{t:T}^a,\pi_{t:T}^b}^{\underline{a}_{s:T}}(H_{i,s}) \cdot \frac{\prod_{\ell = s}^{t-1}1\{ A_{i,\ell} = a_{\ell}\} }{\prod_{\ell = s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell}) } 
    \cdot \left(\widehat{\Gamma}_{i,t}^{\underline{a}_{t:T}} - \widetilde{\Gamma}_{i,t}^{\underline{a}_{t:T}}\right),
\end{align*}
where we suppose that $(\prod_{\ell = s}^{t-1}1\{ A_{i,\ell} = a_{\ell}\})/(\prod_{\ell = s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})) = 1$ when $s=t$.
Then the following holds: 
\begin{itemize}
    \item[(i)] For any integers $s$ and $t$ such that $1\leq s \leq t < T$,,
\begin{align}
        \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right| 
        & \leq  o_{p}(n^{-\min\{1/2,\tilde{\tau}_{B}/2\}})  \nonumber\\
        &+ \sup_{\pi_{(t+1):T}^a , \pi_{(t+1):T}^b \in \Pi_{(t+1):T}} \left| \widetilde{S}_{s:(t+1)}^{\underline{a}_{s:T}}(\pi_{(t+1):T}^a; \pi_{(t+1):T}^b)\right|; \label{eq:sequential_bound}
\end{align}
\item[(ii)] For any integer $s$ such that $1\leq s \leq T$,
\begin{align}
        \sup_{\pi_{T}^a , \pi_{T}^b \in \Pi_{T}} \left| \widetilde{S}_{s:T}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b})\right| 
        & =  o_{p}(n^{-\min\{1/2,\tilde{\tau}_{B}/2\}}); \nonumber
\end{align}
\item[(iii)] For any integers $s$ and $t$ such that $1\leq s \leq t \leq T$,
\begin{align*}
      \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{T}} \left|\widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right| = o_{p}(n^{-\min\{1/2,\tilde{\tau}_{B}/2\}});
\end{align*}
\item[(iv)]
For any stage $t$,
\begin{align*}
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} \left|\widehat{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})- \widetilde{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})\right| = o_{p}(n^{-\min\{1/2,\tilde{\tau}_{B}/2\}}).
\end{align*}
%\begin{align}
%      \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{T}} \left|\widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right| = o_{p}(n^{-\min\{1/2,\tilde{\tau}_{B}/2\}}).
%\end{align}
\end{itemize}
\end{lemma}

\bigskip

\begin{proof}

For any integers $s$ and $t$ such that $1\leq s\leq t \leq T$, define
\begin{align}
\widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})&:=\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}　\nonumber \\
&\times\left(\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right); \label{eq:Stilde_A} \\
\widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})&:=\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})} \nonumber \\
	&\times\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right) \left(\frac{1\left\{ A_{i,t}=a_{t}\right\} }{\hat{e}_{t}^{-k(i)}\left(H_{i,t},a_{t}\right)}-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right).\label{eq:Stilde_C}
\end{align}
We also define $\widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})$ as follows: When $t <T$,
\begin{align}
	\widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})&:=\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})} \nonumber \\
	&\times\left(\widetilde{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(\frac{1\left\{ A_{i,t}=a_{t}\right\} }{\hat{e}_{t}^{-k(i)}\left(H_{i,t},a_{t}\right)}-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right); \label{eq:Stilde_B_1}
\end{align}
when $t=T$,
\begin{align}
	\widetilde{S}_{s:T,(B)}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b})&:=\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{T}^{a},\pi_{T}^{b}}^{\underline{a}_{s:T}}\frac{\prod_{\ell=s}^{T-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{T-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})} \nonumber \\
	&\times\left(Y_{i}-Q_{T}\left(H_{i,T},a_{T}\right)\right)\left(\frac{1\left\{ A_{i,T}=a_{T}\right\} }{\hat{e}_{T}^{-k(i)}\left(H_{i,T},a_{T}\right)}-\frac{1\left\{ A_{i,T}=a_{T}\right\} }{e_{T}\left(H_{i,T},a_{T}\right)}\right).  \label{eq:Stilde_B_2}
\end{align}

We will first prove the result (i). Fix $s$ and $t$ such that $1\leq s\leq t <T$. We also fix $\underline{a}_{s:T} \in \underline{\MA}_{s:T}$. 
We can decompose $\widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})$ as
\begin{align*}
    &\widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) \\
    &= \widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) + \widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) + \widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) + \widetilde{S}_{s:(t+1)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}).
\end{align*}
Hence, $\sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right|$ is bounded from above as
\begin{align}
        &\sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right| \nonumber \\
        & \leq  \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right|  + \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right| \nonumber \\
        &+ \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right|  + \sup_{\pi_{(t+1):T}^a , \pi_{(t+1):T}^b \in \Pi_{(t+1):T}} \left| \widetilde{S}_{s:(t+1)}^{\underline{a}_{s:T}}(\pi_{(t+1):T}^a , \pi_{(t+1):T}^b)\right|. \nonumber
\end{align}
Then applying Lemma \ref{lem:convergence_rate_Stilde_A_C} presented below to each of the first three terms in the right-hand side leads to the result (i).

We will subsequently prove the result (ii). When $t=T$, $\widetilde{S}_{s:T}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b})$ can be decomposed as
\begin{align*}
    &\widetilde{S}_{s:T}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b}) = \widetilde{S}_{s:T,(A)}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b}) + \widetilde{S}_{s:T,(B)}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b}) + \widetilde{S}_{s:T,(C)}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b}).
\end{align*}
Hence,
\begin{align}
        &\sup_{\pi_{T}^a , \pi_{T}^b \in \Pi_{T}} \left| \widetilde{S}_{s:T}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b})\right| \nonumber \\
        & \leq  \sup_{\pi_{T}^a , \pi_{T}^b \in \Pi_{T}} \left| \widetilde{S}_{s:T,(A)}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b})\right|  + \sup_{\pi_{T}^a , \pi_{T}^b \in \Pi_{T}} \left| \widetilde{S}_{s:T,(B)}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b})\right| 
        &+ \sup_{\pi_{T}^a , \pi_{T}^b \in \Pi_{T}} \left| \widetilde{S}_{s:T,(C)}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b})\right|. \nonumber
\end{align}
We obtain the result (ii) by applying Lemma \ref{lem:convergence_rate_Stilde_A_C} to each of the three terms in the right hand side.

We will next prove the result (iii). By applying the result (i) sequentially to $$\sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right|,$$ we have
\begin{align*}
    \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right| &\leq o_{p}(n^{-\min\{1/2,\tilde{\tau}_{B}/2\}}) + \sup_{\pi_{T}^a , \pi_{T}^b \in \Pi_{T}} \left| \widetilde{S}_{s:T}^{\underline{a}_{s:T}}(\pi_{T}^a , \pi_{T}^b)\right|.
\end{align*}
Then the result (iii) follows from the result (ii).

As for the result (iv), when $s=t$,
\begin{align*}
     \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} \left|\widehat{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})- \widetilde{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})\right| =  \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right|.
\end{align*}
Then the result (iv) directly follows from the result (iii).
\end{proof}

\bigskip

The following is the proof of Lemma \ref{lem:asymptotic_estimated_policy_difference_function}.\\

\bigskip

\noindent
\textit{Proof of Lemma \ref{lem:asymptotic_estimated_policy_difference_function}.} Lemma \ref{lem:asymptotic_estimated_policy_difference_function} directly follows from Lemma \ref{lem:convergence_rate_Stilde} (iv). $\Box$\\

\bigskip

The following lemma is used in the proof of Lemma Lemma \ref{lem:convergence_rate_Stilde}.
\bigskip

\begin{lemma} \label{lem:convergence_rate_Stilde_A_C}
Suppose that Assumptions \ref{asm:sequential independence}, \ref{asm:bounded outcome}, \ref{asm:overlap}, \ref{asm:rate_of_convergence_backward_Q}, and \ref{asm:bounded entropy} hold. Fix $\underline{a}_{s:T} \in \underline{\MA}_{s:T}$. Let $\widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$, $\widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$, and $\widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$ be defined as (\ref{eq:Stilde_A}), (\ref{eq:Stilde_B_1})-(\ref{eq:Stilde_B_2}), and (\ref{eq:Stilde_C}), respectively. Then, for any integers $s$ and $t$ such that $1\leq s \leq t \leq T$, the following hold:
\begin{align*}
    \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in  \Pi_{t:T}} \left| \widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| &= o_{P}\left(n^{-\min\{1/2,\tilde{\tau}_B/2\}}\right);\\
    \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in  \Pi_{t:T}} \left| \widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| &= o_{P}\left(n^{-1/2}\right);\\
    \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in  \Pi_{t:T}} \left| \widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| &= o_{P}\left(n^{-\tilde{\tau}_B/2}\right).
\end{align*}
\end{lemma}

\bigskip

\begin{proof}
We will prove the results for the case of $s < t$. The results for the case of $s=t$ follow from the same argument. Fix $s$ and $t$, $\underline{a}_{s:T} \in \underline{\MA}_{s:T}$, and $\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}$. Without loss of generality, we suppose that $n>n_0$ where $n_0$ appears in Assumption \ref{asm:rate_of_convergence_backward_Q}.


We consider $\widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$. We decompose $\widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$ as follows:
\begin{align*}
    &\widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right) = \widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right) + \widetilde{S}_{s:t,(A2)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right),
\end{align*}
where
\begin{align*}
\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)&:=\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\cdot\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\\
      &\times\left(\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right);\\
	\widetilde{S}_{s:t,(A2)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)&:=\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\cdot\left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right)\\
 &\times\left(\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right).
\end{align*}
For each fold $k$, define
\begin{align*}
\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)&:=\frac{1}{n}\sum_{i \in I_k}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\cdot\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\\
      &\times\left(\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right).
\end{align*}



Fix $k \in \{1,\ldots,K\}$. We first consider $\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$. Since $\widehat{Q}_{t}^{-k}(\cdot,\underline{a}_{t:T})$ is computed using the data in the rest $K-1$ folds, when the data $\{Z_i : k(i) \neq k\}$ in the rest $K-1$ folds is conditioned,  $\widehat{Q}_{t}^{-k}(\cdot,\underline{a}_{t:T})$ is fixed; hence, $\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$ is a sum of i.i.d. bounded random variables under Assumptions \ref{asm:bounded outcome}, \ref{asm:overlap}, and \ref{asm:rate_of_convergence_backward_Q} (ii).

It follows that 
\begin{align*}
    	&E\left[G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\cdot\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\cdot\left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right) \right. \\ & \left. \times \left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right)\middle| \widehat{Q}_{t}^{-k}\left(\cdot,\underline{a}_{t:T}\right)\right]\\
=&	E\left[G_{\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}(H_{i,t})\cdot\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})} \cdot \left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\right. \\ 
& \left. \times E\left[\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right)\middle| H_{i,t}\right]\middle|\widehat{Q}_{t}^{-k}\left(\cdot,\underline{a}_{t:T}\right) \right]\\
=&	0.
\end{align*}
Hence, $\sup_{\pi_{t:T}^a , \pi_{t:T}^b \in  \Pi_{t:T}} \left| \widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|$ can be written as
\begin{align*}
     	&\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|\\
&=	\frac{1}{K}\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\frac{1}{n/K}\sum_{i \in I_k }\left\{ G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\cdot\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right.\right.\\
	&\times \left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right) \\
	&-E\left[G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\cdot\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right.\\
	&\left.\left.\left.\times\left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right)\middle|\widehat{Q}_{t}^{-k}\left(\cdot,\underline{a}_{t:T}\right) \right]\right\} \right|.
\end{align*}

By applying Lemma \ref{lem:concentration inequality_influence difference function} with setting $i \in I_k$ and 
\begin{align*}
    \Gamma_{i}(a_t)=G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}} \frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right),
\end{align*}
the following holds: $\forall \delta > 0$, with probability at least $1-2\delta$,
\begin{align*}
    	&\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|\\
&\leq	o\left(n^{-1/2}\right)+\left(54.4\kappa\left(\pi_{t:T}\right)+435.2+\sqrt{2\log(1/\delta)}\right) \\
	&\times\left[\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}E\left[\left(G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\right)^{2}\cdot\left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right)^{2}\right.\right.\\
&	\left.\left.\left.\times\left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right)^{2}\right|\widehat{Q}_{t}^{-k}\left(\cdot,\underline{a}_{t:T}\right) \right] \middle/ \left(\frac{n}{K}\right) \right]^{1/2} \\
&\leq	o\left(n^{-1/2}\right)+\sqrt{K} \cdot \left(54.4\kappa\left(\pi_{t:T}\right)+435.2+\sqrt{2\log(1/\delta)}\right)
	\cdot \left(\frac{1}{\eta}\right)^{t-s}\\
	&\times \sqrt{\frac{E\left[\left.\left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^2\right|\widehat{Q}_{t}^{-k}\left(\cdot,\underline{a}_{t:T}\right) \right]}{n}},
\end{align*}
where the last inequality follows from $\left(G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\right)^{2}\leq 1$ a.s. and Assumption \ref{asm:overlap} (overlap condition). From Assumptions \ref{asm:bounded outcome} and \ref{asm:rate_of_convergence_backward_Q} (ii), we have $E\left[\left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^2\right]<\infty$. Hence, Markov's inequality leads to
\begin{align*}
    E\left[\left.\left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^2\right|\widehat{Q}_{t}^{-k}\left(\cdot,\underline{a}_{t:T}\right) \right] = O_p(1).
\end{align*}
Note also that $\kappa(\Pi_{t:T})<\infty$ according to Lemma \ref{lem:entropy_integral_bound}. Combining these results, we have
\begin{align}
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| = o_p \left(\frac{1}{\sqrt{n}}\right). \label{eq:A1_convergence}
\end{align}
Consequently, 
\begin{align*}
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| \leq  \sum_{k=1}^{K}\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| = o_p \left(\frac{1}{\sqrt{n}}\right).
\end{align*}

We next consider $\widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$ (we will consider $\widetilde{S}_{s:t,(A2)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$ later). We decompose $\widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$ as follows:
\begin{align*}
    \widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)=\sum_{k=1}^{K}\left(\widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)+\widetilde{S}_{s:t,(B2)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right),
\end{align*}
where
\begin{align*}
    \widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)&:=\frac{1}{n}\sum_{i\in I_k}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\left(\widetilde{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\\
    &\times \left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)};\\
    \widetilde{S}_{s:t,(B2)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)&:=\frac{1}{n}\sum_{i \in I_k}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\left(\widetilde{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\\
    &\times \left(\frac{\prod_{\ell=s}^{t}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t}e_{\ell}(H_{i,\ell},a_{\ell})}\right).
\end{align*}
Fix $k \in \{1,\ldots,K\}$. As for $ \widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$,
when $t<T$, note that 
\begin{align}
&E\left[\widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\middle|\{Z_{i}:k(i)\neq k\}\right] \nonumber \\
 	&=E\left[\frac{1}{n/K}\sum_{i \in I_k}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\left(\widetilde{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\right. \nonumber \\
 	&\left.\times\left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\middle|\{Z_{i}:k(i)\neq k\}\right] \nonumber \\
&=	E\left[\frac{1}{n/K}\sum_{i \in I_k}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}E\left[\widetilde{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\middle|H_{i,t},A_{i,t}=a_{t}\right]\right. \nonumber \\
&\left. \times \left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\middle|\{Z_{i}:k(i)\neq k\}\right] \nonumber \\
&=	E\left[\frac{1}{n/K}\sum_{i \in I_k}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\right. \nonumber \\
&\left.\times \left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\middle|\{Z_{i}:k(i)\neq k\}\right] \nonumber \\
&=	0, \nonumber
\end{align}
where the third equality follows from Assumption \ref{asm:sequential independence}. When $t=T$, $$E\left[\widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\middle|\{Z_{i}:k(i)\neq k\}\right]=0$$ also holds by the same argument. Note that conditional on $\{Z_{i}:k(i)\neq k\}$, $\widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$ is a sum of i.i.d. bounded random variables under Assumptions \ref{asm:bounded outcome}, \ref{asm:overlap}, and \ref{asm:rate_of_convergence_backward_Q} (ii), and its condition mean is zero.
Hence, we can apply Lemma \ref{lem:concentration inequality_influence difference function} with setting $i \in I_k$ and 
\begin{align*}
    \Gamma_{i}(\underline{a}_{s:T})&=	G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\left(\widetilde{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\\
    &\times \left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}
\end{align*}
to obtain the following: $\forall \delta > 0$, with probability at least $1-2\delta$,
\begin{align*}
    	&\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in\Pi_{t:T}}\left|\widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| \\
&\leq	o\left(n^{-1/2}\right)+\left(54.4\kappa\left(\pi_{t:T}\right)+435.2+\sqrt{2\log(1/\delta)}\right) \\
&\times	\left[\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in\Pi_{t:T}}E\left[\left(G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\right)^{2}\cdot\left(\widetilde{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}\right.\right.\\
&\left.\left. \left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)^{2}\left(\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right)^{2}\middle|\left\{ Z_{i}:k(i)\neq k\right\} \right]\middle/ \left(\frac{n}{K}\right)\right]^{1/2} \\
&\leq	\left(n^{-1/2}\right)+\sqrt{K}\cdot\left(54.4\kappa\left(\pi_{t:T}\right)+435.2+\sqrt{2\log(1/\delta)}\right)\cdot\left(\frac{2M}{\eta^{T-t}}+\sum_{j=0}^{T-t-1}\frac{M}{\eta^j}\right)\cdot \frac{1}{\eta} \\
&\times	\sqrt{\frac{E\left[\left(\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)^{2}\middle|\left\{ Z_{i}:k(i)\neq k\right\} \right]}{n}},
\end{align*}
where the last inequality follows from $\left(G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\right)^{2}\leq 1$ a.s. and Assumptions \ref{asm:bounded outcome} and \ref{asm:overlap}. From Assumptions \ref{asm:overlap} and \ref{asm:rate_of_convergence_backward_Q} (ii), we have
\begin{align*}
    E\left[\left(\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)^{2}\right] < \infty.
\end{align*}
Hence, Markov's inequality leads to
\begin{align*}
    E\left[\left(\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)^{2}\middle|\left\{ Z_{i}:k(i)\neq k\right\} \right] = O_p(1).
\end{align*}
Note also that $\kappa(\Pi_{t:T})<\infty$ according to Lemma \ref{lem:entropy_integral_bound}. Combining these results, we have
\begin{align}
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| = o_p \left(\frac{1}{\sqrt{n}}\right).  \label{eq:B1_convergence}
\end{align}
By applying the same argument to derive (\ref{eq:B1_convergence}), we also obtain 
\begin{align*}
   \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(B2)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| = o_p \left(\frac{1}{\sqrt{n}}\right).
\end{align*}
Consequently, 
\begin{align*}
    &\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| \\
    &\leq  \sum_{k=1}^{K}\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|
     + \sum_{k=1}^{K}\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(B2)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|\\
    &= o_p \left(\frac{1}{\sqrt{n}}\right).
\end{align*}

We next consider to bound $\sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|$ from above. It follows that 
\begin{align*}
    		&\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|\\
	&=\frac{1}{n}\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\sum_{i=1}^{n}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\cdot\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}\right.\\
	&\times\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left.\cdot\left(\frac{1}{\hat{e}_{t}^{-k(i)}\left(H_{i,t},a_{t}\right)}-\frac{1}{e_{t}\left(H_{i,t},a_{t}\right)}\right)\cdot1\left\{ A_{i,t}=a_{t}\right\} \right|\\
	&\leq\frac{1}{n}\sum_{i=1}^{n}\left|Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right|\cdot \left|\frac{1}{\prod_{\ell=s}^{t}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{1}{e_{t}\left(H_{i,t},a_{t}\right)\cdot\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}\right|\\
	&\leq\frac{1}{n}\sum_{i=1}^{n}\left|Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right| \cdot \left|\frac{1}{\prod_{\ell=s}^{t}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t}e_{\ell}(H_{i,\ell},a_{\ell})}\right|\\
	&+\frac{1}{n}\sum_{i=1}^{n}\left|Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right|\cdot\left|\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right|\left(\frac{1}{e_t(H_{i,t},a_t)}\right)\\
	&\leq\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}}\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(\frac{1}{\prod_{\ell=s}^{t}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t}e_{\ell}(H_{i,\ell},a_{\ell})}\right)^{2}}\\
	&+\left(\frac{1}{\eta}\right)\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}}\\ &\times \sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right)^{2}}
\end{align*}
where the last inequality follows from Cauchy-Schwartz inequality and Assumption \ref{asm:overlap} (overlap condition). Taking expectation of both sides yields:
\begin{align*}
 E\left[\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|\right] 
 &\leq	E\left[\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}}\right. \\
&	\left.\times\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(\frac{1}{\prod_{\ell=s}^{t}\hat{e}_{\ell}^{-k(i)}\left(H_{i,\ell},a_{\ell}\right)}-\frac{1}{\prod_{\ell=s}^{t}e_{\ell}\left(H_{i,\ell},a_{\ell}\right)}\right)^{2}}\right] \\
& + \eta^{-1}E\left[\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}}\right. \\
&	\left.\times\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}\left(H_{i,\ell},a_{\ell}\right)}-\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}\left(H_{i,\ell},a_{\ell}\right)}\right)^{2}}\right] \\
 &\leq	\sqrt{\frac{1}{n}\sum_{i=1}^{n}E\left[\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}\right]} \\
&	\times\sqrt{\frac{1}{n}\sum_{i=1}^{n}E\left[\left(\frac{1}{\prod_{\ell=s}^{t}\hat{e}_{\ell}^{-k(i)}\left(H_{i,\ell},a_{\ell}\right)}-\frac{1}{\prod_{\ell=s}^{t}e_{\ell}\left(H_{i,\ell},a_{\ell}\right)}\right)^{2}\right]} \\
&+ \eta^{-1}\sqrt{\frac{1}{n}\sum_{i=1}^{n}E\left[\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}\right]} \\
&	\times\sqrt{\frac{1}{n}\sum_{i=1}^{n}E\left[\left(\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}\left(H_{i,\ell},a_{\ell}\right)}-\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}\left(H_{i,\ell},a_{\ell}\right)}\right)^{2}\right]} \\
 &=	o\left(n^{-\tilde{\tau}_{B}/2}\right),
\end{align*}
where the second inequality follows from Cauchy-Schwartz inequality and the last line follows from Assumption \ref{asm:rate_of_convergence_backward_Q} (i). Then applying Markov's inequality leads to
\begin{align}
    \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| &= o_{P}\left(n^{-\tilde{\tau}_B/2}\right). \label{eq:S_C_convergence}
\end{align}


Now let us consider $\widetilde{S}_{s:t,(A2)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$. Note that 
\begin{align*}
    	&\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(A2)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|\\
    	&\leq\frac{1}{n}\sum_{i=1}^{n}\left|\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right|\cdot\left|\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right|\\
    	&\times \left|1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right|\\
	&\leq\left(\frac{1}{\eta}\right)\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right)^{2}}\\
	&\times \sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}},
\end{align*}
where the last inequality follows from Assumption \ref{asm:overlap} (overlap condition) and Cauchy-Schwartz inequality. Then, by applying the same argument to derive (\ref{eq:S_C_convergence}), we obtain
\begin{align*}
    \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(A2)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| &= o_{P}\left(n^{-\tilde{\tau}_B/2}\right).
\end{align*}
 Combining this result with (\ref{eq:A1_convergence}), we obtain 
 \begin{align*}
     \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|  
     &\leq \sum_{k=1}^{K} \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| \\
     &+ \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(A2)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| \\
     &= o_{P}\left(n^{-\min\{1/2,\tilde{\tau}_B/2\}}\right).
 \end{align*}
Consequently, we have shown the result (\ref{eq:sequential_bound}).
 
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Proof of Theorem \ref{thm:main_theorem_backward_Q}}\label{seq:main_proof_Q}

%\begin{comment}
This appendix presents the proof of Theorem \ref{thm:main_theorem_backward_Q}.  The proof follows a similar argument to that of the proof of Theorem \ref{thm:main_theorem_backward}. We consider to derive asymptotic upper bound on $R(\check{\pi})$.

Given the estimated DTR $\check{\pi}$, for any $\pi_t \in \Pi_{t}$, we define $R_{t}^{\check{\pi}_{t:T}}(\pi_t) \equiv V_{t}(\pi_t,\check{\pi}_{(t+1):T}) - V_{t}(\check{\pi}_{t:T})$ for $t=1,\ldots,T$. Then, applying Lemma \ref{lem:helpful_lemma_general} with $\pi = \check{\pi}$, we obtain
\begin{align}
    R(\check{\pi}) \leq R_{1}^{\check{\pi}_{1:T}}(\pi_{1}^{\ast}) + \sum_{t=2}^{T} \frac{2^{t-2}}{\eta^{t-1}} R_{t}^{\check{\pi}_{t:T}}(\pi_{t}^{\ast}). \label{eq:decomposition_result_Q}
\end{align}
The result (\ref{eq:decomposition_result_Q}) enables us to evaluate $R(\check{\pi})$ through evaluating $R_{t}^{\check{\pi}_{t:T}}(\pi_{t}^{\ast})$ for each $t$.

Given a fixed DTR $\pi=(\pi_1,\ldots,\pi_T)$, let us define
\begin{align*}
\Gamma_{i,T}^{\dag}(a_{T}) &\equiv \frac{ Y_{i} - Q_{T}(H_{i,T},A_{i,T}) }{e_{T}(H_{i,T},A_{i,T})}\cdot 1\{A_{i,T}=a_T\} + Q_{T}(H_{i,T},a_{T}),\\
\widehat{\Gamma}_{i,T}^{\dag}(a_T) &\equiv \frac{ Y_{i}- \widehat{Q}_{T}^{-k(i)}(H_{i,T},A_{i,T}) }{\hat{e}_{T}^{-k(i)}(H_{i,T},A_{i,T})}\cdot1\{A_{i,T} = a_T\} + 
    \widehat{Q}_{T}^{-k(i)}\left(H_{i,T},a_{T}\right),\\
\Gamma_{i,T-1}^{\dag,\pi_T}(a_{T-1}) &\equiv \frac{ \Gamma_{i,T}^{\dag}(\pi_{T}(H_{i,T})) - Q_{T-1}^{\pi_{T}}(H_{i,T-1},A_{i,T-1}) }{e_{T-1}(H_{i,T-1},A_{i,T-1})}\cdot 1\{A_{i,T-1}=a_{T-1}\} \\
&+ Q_{T-1}^{\pi_{T}}(H_{i,T-1},A_{i,T-1}),\\
\widehat{\Gamma}_{i,T-1}^{\dag,\pi_T}(a_{T-1}) &\equiv \frac{\widehat{\Gamma}_{i,T}^{\dag}(\pi_T(H_{i,T}))- \widehat{Q}_{T-1}^{\pi_{T},-k(i)}(H_{i,T-1},A_{i,T-1}) }{\hat{e}_{T-1}^{-k(i)}(H_{i,T-1},A_{i,T-1})}\cdot1\{A_{i,T-1} = a_{T-1}\} \\
&+    \widehat{Q}_{T-1}^{\pi_{T},-k(i)}\left(H_{i,T-1},a_{T-1}\right),
\end{align*}
and, recursively for $t=T-2,\ldots,1$,
\begin{align*}
\Gamma_{i,t}^{\dag,\pi_{(t+1):T}}(a_{t}) &\equiv \frac{ \Gamma_{i,t+1}^{\dag,\pi_{(t+2):T}}(\pi_{t+1}(H_{i,t+1})) - Q_{t}^{\pi_{(t+1):T}}(H_{i,t},A_{i,t}) }{e_{t}(H_{i,t},A_{i,t})}\cdot 1\{A_{i,t}=a_{t}\} \\
&+ Q_{t}^{\pi_{(t+1):T}}(H_{i,t},a_{t}),\\
\widehat{\Gamma}_{i,t}^{\dag,\pi_{(t+1):T}}(a_{t}) &\equiv \frac{ \widehat{\Gamma}_{i,t+1}^{\dag,\pi_{(t+2):T}}(\pi_{t+1}(H_{i,t+1})) - \widehat{Q}_{t}^{\pi_{(t+1):T},-k(i)}(H_{i,t},A_{i,t}) }{\hat{e}_{t}^{-k(i)}(H_{i,t},A_{i,t})}\cdot 1\{A_{i,t}=a_{t}\} \\
&+ \widehat{Q}_{t}^{\pi_{(t+1):T},-k(i)}(H_{i,t},a_{t}).
\end{align*}
Note that $(1/n)\sum_{i=1}^{n}\Gamma_{i,t}^{\dag,\pi_{(t+1):T}}\left(a_t\right)$ is an oracle estimate of the action value function $V_{t}^{\pi_{(t+1):T}}(a_t)$ with oracle access to $\{Q_{s}^{\pi_{(s+1):T}}(\cdot,\cdot)\}_{s=t,\ldots,T}$ and $\{e_{s}(\cdot,\cdot)\}_{s=t,\ldots,T}$.
%\footnote{Note that $V_{t}^{\pi_{(t+1):T}}(a_t)$ is different from the optimal action value $E\left[Q_{t}(H_t,a_t)\right]$ unless $\pi_{(t+1):T} \neq \pi_{(t+1):T}^{\ast}$. Hence, $(1/n)\sum_{i=1}^{n}\Gamma_{i,t}^{\pi_{(t+1):T}}\left(a_t\right)$ is not a doubly-robust estimator of the optimal value function $E\left[Q_{t}(H_t,a_t)\right]$.} 

Following the discussion in Appendix \ref{app:main_proof}, we define the oracle influence difference function $\widetilde{\Delta}_{t}^{\dag}(\cdot;\cdot):\Pi_{t:T} \times \Pi_{t:T} \rightarrow \Real$, and the estimated policy value difference function $\widehat{\Delta}_{t}^{\dag}(\cdot;\cdot):\Pi_{t} \times \Pi_{t:T} \rightarrow \Real$, respectively, as follows: For $\pi_{t:T}^{a}=(\pi_{t}^{a},\ldots,\pi_{T}^{a})$ and $\pi_{t:T}^{b}=(\pi_{t}^{b},\ldots,\pi_{T}^{b})$ in $\Pi_{t:T}$,
\begin{align}
 &\widetilde{\Delta}_{T}^{\dag}(\pi_{T}^{a};\pi_{T}^{b})\equiv \frac{1}{n}\sum_{i=1}^{n} \Gamma_{i,T}^{\dag}\left(\pi_{T}^{a}(H_{i,T})\right) - \frac{1}{n}\sum_{i=1}^{n} \Gamma_{i,T}^{\dag}\left(\pi_{T}^{b}(H_{i,T})\right), \nonumber\\
  &\widehat{\Delta}_{T}^{\dag}(\pi_{T}^{a};\pi_{T}^{b}) \equiv  \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,T}^{\dag}\left(\pi_{T}^{a}(H_{i,T})\right) -  \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,T}^{\dag}\left(\pi_{T}^{b}(H_{i,T})\right), \nonumber \\
 &\widetilde{\Delta}_{t}^{\dag}(\pi_{t:T}^{a};\pi_{t:T}^{b}) \equiv \frac{1}{n}\sum_{i=1}^{n} \Gamma_{i,t}^{\dag,\pi_{(t+1):T}^{a}}\left(\pi_{t}^{a}(H_{i,t})\right) - \frac{1}{n}\sum_{i=1}^{n} \Gamma_{i,t}^{\dag,\pi_{(t+1):T}^{b}}\left(\pi_{t}^{b}(H_{i,t})\right), \nonumber\\
 & \widehat{\Delta}_{t}^{\dag}(\pi_{t:T}^{a};\pi_{t:T}^{b}) \equiv  \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\dag,\pi_{(t+1):T}^{a}}\left(\pi_{t}^{a}(H_{i,t})\right) -  \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\dag,\pi_{(t+1):T}^{b}}\left(\pi_{t}^{b}(H_{i,t})\right) \nonumber
\end{align}
for $t=1,\ldots,T-1$.
Note that $\widetilde{\Delta}_{T}^{\dag}(\pi_{t:T}^{a};\pi_{t:T}^{b})$ is an unbiased estimator of the policy value difference function $\Delta_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})$. From the definitions, $R_{t}^{\check{\pi}_{t:T}}(\pi_{t}^{\ast}) = \Delta_{t}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right)$.

%Noting that Lemma \ref{lem:helpful_lemma} enables us to evaluate $R(\check{\pi})$ through evaluating $\Delta_{t}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right)$ for each $t$, 
In what follows, we evaluate $R_{t}^{\check{\pi}_{t:T}}(\pi_{t}^{\ast})$ for each $t$. A standard argument of the statistical learning theory (\cite{Lugosi_2002}) gives
\begin{align}
    R_{t}^{\check{\pi}_{t:T}}(\pi_{t}^{\ast})  
    &=\Delta_{t}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right) 
    %& = \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}\left(\pi_{t}^{\ast}(H_{i,t})\right) - \frac{1}{n}\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\check{\pi}_{(t+1):T}}\left(\check{\pi}_{t}(H_{i,t})\right) \\
    %&+ \Delta_{t}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right) - \widehat{\Delta}_{T}^{\dag}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right)\\
    \leq \Delta_{t}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right) - \widehat{\Delta}_{T}^{\dag}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right) \nonumber \\
    &\leq 
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} |\Delta_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b}) - \widehat{\Delta}_{T}^{\dag}(\pi_{t:T}^{a};\pi_{t:T}^{b})| \nonumber \\
    & \leq  \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} |\Delta_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b}) - \widetilde{\Delta}_{T}^{\dag}(\pi_{t:T}^{a};\pi_{t:T}^{b})| 
    + \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_t} |\widehat{\Delta}_{T}^{\dag}(\pi_{t:T}^{a};\pi_{t:T}^{b}) - \widetilde{\Delta}_{T}^{\dag}(\pi_{t:T}^{a};\pi_{t:T}^{b})|, \label{eq:standard_inequality_Q}
\end{align}
where the first inequality follows because $\check{\pi}_{t}$ maximizes  $(1/n)\sum_{i=1}^{n} \widehat{\Gamma}_{i,t}^{\dag,\check{\pi}_{(t+1):T}}\left(\pi_{t}(H_{i,t})\right)$ over $\Pi_t$; hence, $\widehat{\Delta}_{T}^{\dag}\left(\pi_{t}^{\ast},\check{\pi}_{(t+1):T};\check{\pi}_{t:T}\right) \leq 0$. 

We can evaluate $R_{t}^{\check{\pi}_{t:T}}(\pi_{t}^{\ast})$ through evaluating $\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} |\Delta_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b}) - \widetilde{\Delta}_{T}^{\dag}(\pi_{t:T}^{a};\pi_{t:T}^{b})|$ and $\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} |\widehat{\Delta}_{T}^{\dag}(\pi_{t:T}^{a};\pi_{t:T}^{b}) - \widetilde{\Delta}_{T}^{\dag}(\pi_{t:T}^{a};\pi_{t:T}^{b})|$. As for the former, we apply the uniform concentration result of \citeauthor{zhou2022offline} (\citeyear{zhou2022offline}, Lemma 2) for the oracle influence difference function to obtain the following lemma.

\bigskip
\begin{lemma}\label{lem:bound_influence_difference_function_Q}
Suppose that Assumptions \ref{asm:sequential independence}, \ref{asm:bounded outcome}, \ref{asm:overlap}, and \ref{asm:bounded entropy} hold. Then for any stage $t$ and $\delta \in (0,1)$, with probability at least $1-2\delta$, the following holds:
\begin{align}
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} \left|\widetilde{\Delta}_{T}^{\dag}(\pi_{t:T}^{a};\pi_{t:T}^{b})-\Delta_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})\right| &\leq \left(54.4 \sqrt{2}\kappa(\Pi_{t:T}) + 435.2 + \sqrt{2 \log \frac{1}{\delta}}\right)\sqrt{\frac{V_{t:T}^{\ast}}{n}} \nonumber \\
    &+ o\left(\frac{1}{\sqrt{n}}\right), \label{eq:bound_influence_difference_function}
\end{align}
where 
    $V_{t:T}^{\ast} := \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}}E
    \left[\left(\Gamma_{i}^{\dag,\pi_{(t+1):T}^{a}}(\pi_{t}^{a}(H_{i,t})) - \Gamma_{i}^{\dag,\pi_{(t+1):T}^{b}}(\pi_{t}^{b}(H_{i,t})) \right)^2\right] < \infty$.
\end{lemma}
\bigskip

\begin{proof}
The result follows by the same argument to the proof of Lemma \ref{lem:bound_influence_difference_function}.
\end{proof}

\bigskip

As for $\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} |\widehat{\Delta}_{T}^{\dag}(\pi_{t:T}^{a},\pi_{t:T}^{b}) - \widetilde{\Delta}_{T}^{\dag}(\pi_{t:T}^{a},\pi_{t:T}^{b})|$, by the similar argument in the proof of Lemma \ref{lem:asymptotic_estimated_policy_difference_function}, we can obtain the following lemma. 

\bigskip

\begin{lemma}\label{lem:asymptotic_estimated_policy_difference_function_Q}
Suppose that Assumptions \ref{asm:sequential independence}, \ref{asm:bounded outcome}, \ref{asm:overlap}, and \ref{asm:rate_of_convergence_backward_Q} hold. Then, for any stage $t$, the following holds:
\begin{align*}
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} \left|\widehat{\Delta}_{T}^{\dag}(\pi_{t:T}^{a};\pi_{t:T}^{b})- \widetilde{\Delta}_{T}^{\dag}(\pi_{t:T}^{a};\pi_{t:T}^{b})\right| = o_{p}(n^{-\min\{1/2,\tilde{\tau}_{B}/2\}}).
\end{align*}
\end{lemma}

\bigskip

\begin{proof}
See Appendix \ref{app:proofs_of_main_lemmas}.
\end{proof}

\bigskip

Combing the inequality (\ref{eq:standard_inequality_Q}) with Lemmas \ref{lem:bound_influence_difference_function_Q} and \ref{lem:asymptotic_estimated_policy_difference_function_Q}, we obtain 
\begin{align}
    R_{t}^{\check{\pi}_{t:T}}(\pi_{t}^{\ast}) = O_{p}\left(\kappa(\Pi_{t:T}) \cdot n^{-1/2}\right) + o_{p} (n^{-\min\{1/2,\tilde{\tau}_{B}/2\}}) \label{eq:result_deviation_regret}
\end{align}
for all $t=1,\ldots,T$. This result eventually shows Theorem \ref{thm:main_theorem_backward_Q} through (\ref{eq:decomposition_result_Q}).$\Box$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Proof of Lemma \ref{lem:asymptotic_estimated_policy_difference_function_Q}}\label{app:proofs_of_main_lemmas_Q}

We provide the proofs of Lemma \ref{lem:asymptotic_estimated_policy_difference_function_Q} in this section. 
Fix $t \in \{1,\ldots,T\}$ and $\pi_{t:T} \in \Pi_{t:T}$. For any $\underline{a}_{t:T} \in \underline{\MA}_{t:T}$, let $G_{i,\pi_{t:T}}^{\underline{a}_{t:T}} := \prod_{s=t}^{T}1\{\pi_{s}(H_{i,s})=a_s\}$.
Define recursively 
\begin{align*}
\Gamma_{i,T}^{\dagger}\left(a_{T}\right)	&\equiv\frac{Y_{i}-Q_{T}(H_{i,T},a_{T})}{e_{T}(H_{i,T},a_{T})}\cdot1\{A_{i,T}=a_{T}\}+Q_{T}(H_{i,T},a_{T}),\\
	\widehat{\Gamma}_{i,T}^{\dagger}\left(a_{T}\right)&\equiv\frac{Y_{i}-\widehat{Q}_{T}^{-k(i)}(H_{i,T},a_{T})}{\hat{e}_{T}^{-k(i)}(H_{i,T},a_{T})}\cdot1\{A_{i,T}=a_{T}\}+\widehat{Q}_{T}^{-k(i)}\left(H_{i,T},a_{T}\right),\\
	\Gamma_{i,T-1}^{\dagger,\pi_{T}}\left(a_{T-1}\right)&\equiv\frac{\Gamma_{i,T}^{\dagger}\left(\pi_{T}\left(H_{i,T}\right)\right)-Q_{T-1}^{\pi_{T}}(H_{i,T-1},a_{T-1})}{e_{T-1}(H_{i,T-1},a_{T-1})}\cdot1\{A_{i,T-1}=a_{T-1}\}\\
	&+Q_{T-1}^{\pi_{T}}(H_{i,T-1},a_{T-1}),\\
	\widehat{\Gamma}_{i,T-1}^{\dagger,\pi_{T}}\left(a_{T-1}\right)&\equiv\frac{\widehat{\Gamma}_{i,T}^{\dagger}\left(\pi_{T}\left(H_{i,T}\right)\right)-\widehat{Q}_{T-1}^{\pi_{T},-k(i)}(H_{i,T-1},a_{T-1})}{\hat{e}_{t}^{-k(i)}(H_{i,T-1},a_{T-1})}\cdot1\{A_{i,T-1}=a_{T-1}\}\\
	&+\widehat{Q}_{T-1}^{\pi_{T},-k(i)}\left(H_{i,T-1},a_{T-1}\right),\\
	\Gamma_{i,t}^{\dagger,\pi_{(t+1):T}}\left(a_{t}\right)&\equiv\frac{\Gamma_{i,t+1}^{\dagger,\pi_{(t+2):T}}\left(\pi_{t+1}\left(H_{i,t+1}\right)\right)-Q_{t}^{\pi_{(t+1):T}}(H_{i,t},a_{t})}{e_{t}(H_{i,t},a_{t})}\cdot1\{A_{i,t}=a_{t}\}\\
	&+Q_{t}^{\pi_{(t+1):T}}(H_{i,t},a_{t}),\\
	\widehat{\Gamma}_{i,t}^{\dagger,\pi_{(t+1):T}}\left(a_{t}\right)&\equiv\frac{\widehat{\Gamma}_{i,t+1}^{\dagger,\pi_{(t+2):T}}\left(\pi_{t+1}\left(H_{i,t+1}\right)\right)-\widehat{Q}_{t}^{\pi_{(t+1):T},-k(i)}(H_{i,t},a_{t})}{\hat{e}_{t}^{-k(i)}(H_{i,t},a_{t})}\cdot1\{A_{i,t}=a_{t}\}\\
	&+\widehat{Q}_{t}^{\pi_{(t+1):T},-k(i)}\left(H_{i,t},a_{t}\right),
\end{align*}
for $t=T-2,\ldots,1$.

We also define $\widehat{\Delta}_{t}^{\underline{a}_{t:T}}(\cdot,\cdot):\Pi_{t:T}\times \Pi_{t:T} \rightarrow \Real$ and $\widetilde{\Delta}_{t}^{\underline{a}_{t:T}}(\cdot,\cdot):\Pi_{t:T}\times \Pi_{t:T} \rightarrow \Real$ as follows:
\begin{align*}
   \widehat{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})&\equiv\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^{a}}^{\underline{a}_{t:T}}\Gamma_{i,t}^{\dagger,\pi_{t:T}^{a}}-\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^{b}}^{\underline{a}_{t:T}}\Gamma_{i,t}^{\dagger,\pi_{t:T}^{b}};\\
   \widetilde{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})&\equiv\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^{a}}^{\underline{a}_{t:T}}\Gamma_{i,t}^{\dagger,\pi_{t:T}^{a}}-\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^{b}}^{\underline{a}_{t:T}}\Gamma_{i,t}^{\dagger,\pi_{t:T}^{b}}.
\end{align*}
Note that $\widehat{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b})$ and $\widetilde{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b})$ can be decomposed as 
\begin{align*}
    \widehat{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b}) &= \sum_{\underline{a}_{t:T} \in \underline{\MA}_{t:T}} \widehat{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b});\\ \widetilde{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b}) &= \sum_{\underline{a}_{t:T} \in \underline{\MA}_{t:T}} \widetilde{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}).
\end{align*}
 Hence, 
\begin{align*}
    \widehat{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b}) - \widetilde{\Delta}_{t}(\pi_{t:T}^{a},\pi_{t:T}^{b}) = \sum_{\underline{a}_{t:T} \in \underline{\MA}_{t:T}} \left(\widehat{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) - \widetilde{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right).
\end{align*}
We will evaluate $\widehat{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) - \widetilde{\Delta}_{t}^{\underline{a}_{t:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})$ for each $t$ and $\underline{a}_{t:T} \in \underline{\MA}_{t:T}$ in the following lemma.


\bigskip


\begin{lemma} \label{lem:convergence_rate_Stilde}
Suppose that Assumptions \ref{asm:sequential independence}, \ref{asm:bounded outcome}, \ref{asm:overlap}, \ref{asm:rate_of_convergence_backward_Q}, and \ref{asm:bounded entropy} hold. For any integers $s$ and $t$ such that $1\leq s \leq t \leq T$, sequences of policies $\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}$, and treatments $\underline{a}_{s:T} \in \underline{\MA}_{s:T}$, let 
\begin{align*}
    \widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) := \frac{1}{n}\sum_{i=1}^{n}&G_{\pi_{t:T}^a,\pi_{t:T}^b}^{\underline{a}_{s:T}}(H_{i,s}) \cdot \frac{\prod_{\ell = s}^{t-1}1\{ A_{i,\ell} = a_{\ell}\} }{\prod_{\ell = s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell}) } 
    \cdot \left(\widehat{\Gamma}_{i,t}^{\underline{a}_{t:T}} - \widetilde{\Gamma}_{i,t}^{\underline{a}_{t:T}}\right),
\end{align*}
where we suppose that $(\prod_{\ell = s}^{t-1}1\{ A_{i,\ell} = a_{\ell}\})/(\prod_{\ell = s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})) = 1$ when $s=t$.
Then the following holds: 
\begin{itemize}
    \item[(i)] For any integers $s$ and $t$ such that $1\leq s \leq t < T$,,
\begin{align}
        \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right| 
        & \leq  o_{p}(n^{-\min\{1/2,\tilde{\tau}_{B}/2\}})  \nonumber\\
        &+ \sup_{\pi_{(t+1):T}^a , \pi_{(t+1):T}^b \in \Pi_{(t+1):T}} \left| \widetilde{S}_{s:(t+1)}^{\underline{a}_{s:T}}(\pi_{(t+1):T}^a; \pi_{(t+1):T}^b)\right|; \label{eq:sequential_bound}
\end{align}
\item[(ii)] For any integer $s$ such that $1\leq s \leq T$,
\begin{align}
        \sup_{\pi_{T}^a , \pi_{T}^b \in \Pi_{T}} \left| \widetilde{S}_{s:T}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b})\right| 
        & =  o_{p}(n^{-\min\{1/2,\tilde{\tau}_{B}/2\}}); \nonumber
\end{align}
\item[(iii)] For any integers $s$ and $t$ such that $1\leq s \leq t \leq T$,
\begin{align*}
      \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{T}} \left|\widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right| = o_{p}(n^{-\min\{1/2,\tilde{\tau}_{B}/2\}});
\end{align*}
\item[(iv)]
For any stage $t$,
\begin{align*}
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} \left|\widehat{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})- \widetilde{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})\right| = o_{p}(n^{-\min\{1/2,\tilde{\tau}_{B}/2\}}).
\end{align*}
%\begin{align}
%      \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{T}} \left|\widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right| = o_{p}(n^{-\min\{1/2,\tilde{\tau}_{B}/2\}}).
%\end{align}
\end{itemize}
\end{lemma}

\bigskip

\begin{proof}

For any integers $s$ and $t$ such that $1\leq s\leq t \leq T$, define
\begin{align}
\widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})&:=\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}　\nonumber \\
&\times\left(\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right); \label{eq:Stilde_A} \\
\widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})&:=\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})} \nonumber \\
	&\times\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right) \left(\frac{1\left\{ A_{i,t}=a_{t}\right\} }{\hat{e}_{t}^{-k(i)}\left(H_{i,t},a_{t}\right)}-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right).\label{eq:Stilde_C}
\end{align}
We also define $\widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})$ as follows: When $t <T$,
\begin{align}
	\widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})&:=\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})} \nonumber \\
	&\times\left(\widetilde{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(\frac{1\left\{ A_{i,t}=a_{t}\right\} }{\hat{e}_{t}^{-k(i)}\left(H_{i,t},a_{t}\right)}-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right); \label{eq:Stilde_B_1}
\end{align}
when $t=T$,
\begin{align}
	\widetilde{S}_{s:T,(B)}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b})&:=\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{T}^{a},\pi_{T}^{b}}^{\underline{a}_{s:T}}\frac{\prod_{\ell=s}^{T-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{T-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})} \nonumber \\
	&\times\left(Y_{i}-Q_{T}\left(H_{i,T},a_{T}\right)\right)\left(\frac{1\left\{ A_{i,T}=a_{T}\right\} }{\hat{e}_{T}^{-k(i)}\left(H_{i,T},a_{T}\right)}-\frac{1\left\{ A_{i,T}=a_{T}\right\} }{e_{T}\left(H_{i,T},a_{T}\right)}\right).  \label{eq:Stilde_B_2}
\end{align}

We will first prove the result (i). Fix $s$ and $t$ such that $1\leq s\leq t <T$. We also fix $\underline{a}_{s:T} \in \underline{\MA}_{s:T}$. 
We can decompose $\widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})$ as
\begin{align*}
    &\widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) \\
    &= \widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) + \widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) + \widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}) + \widetilde{S}_{s:(t+1)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b}).
\end{align*}
Hence, $\sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right|$ is bounded from above as
\begin{align}
        &\sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right| \nonumber \\
        & \leq  \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right|  + \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right| \nonumber \\
        &+ \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right|  + \sup_{\pi_{(t+1):T}^a , \pi_{(t+1):T}^b \in \Pi_{(t+1):T}} \left| \widetilde{S}_{s:(t+1)}^{\underline{a}_{s:T}}(\pi_{(t+1):T}^a , \pi_{(t+1):T}^b)\right|. \nonumber
\end{align}
Then applying Lemma \ref{lem:convergence_rate_Stilde_A_C} presented below to each of the first three terms in the right-hand side leads to the result (i).

We will subsequently prove the result (ii). When $t=T$, $\widetilde{S}_{s:T}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b})$ can be decomposed as
\begin{align*}
    &\widetilde{S}_{s:T}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b}) = \widetilde{S}_{s:T,(A)}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b}) + \widetilde{S}_{s:T,(B)}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b}) + \widetilde{S}_{s:T,(C)}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b}).
\end{align*}
Hence,
\begin{align}
        &\sup_{\pi_{T}^a , \pi_{T}^b \in \Pi_{T}} \left| \widetilde{S}_{s:T}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b})\right| \nonumber \\
        & \leq  \sup_{\pi_{T}^a , \pi_{T}^b \in \Pi_{T}} \left| \widetilde{S}_{s:T,(A)}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b})\right|  + \sup_{\pi_{T}^a , \pi_{T}^b \in \Pi_{T}} \left| \widetilde{S}_{s:T,(B)}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b})\right| 
        &+ \sup_{\pi_{T}^a , \pi_{T}^b \in \Pi_{T}} \left| \widetilde{S}_{s:T,(C)}^{\underline{a}_{s:T}}(\pi_{T}^{a},\pi_{T}^{b})\right|. \nonumber
\end{align}
We obtain the result (ii) by applying Lemma \ref{lem:convergence_rate_Stilde_A_C} to each of the three terms in the right hand side.

We will next prove the result (iii). By applying the result (i) sequentially to $$\sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right|,$$ we have
\begin{align*}
    \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right| &\leq o_{p}(n^{-\min\{1/2,\tilde{\tau}_{B}/2\}}) + \sup_{\pi_{T}^a , \pi_{T}^b \in \Pi_{T}} \left| \widetilde{S}_{s:T}^{\underline{a}_{s:T}}(\pi_{T}^a , \pi_{T}^b)\right|.
\end{align*}
Then the result (iii) follows from the result (ii).

As for the result (iv), when $s=t$,
\begin{align*}
     \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}} \left|\widehat{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})- \widetilde{\Delta}_{t}(\pi_{t:T}^{a};\pi_{t:T}^{b})\right| =  \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t}^{\underline{a}_{s:T}}(\pi_{t:T}^{a},\pi_{t:T}^{b})\right|.
\end{align*}
Then the result (iv) directly follows from the result (iii).
\end{proof}

\bigskip

The following is the proof of Lemma \ref{lem:asymptotic_estimated_policy_difference_function}.

\bigskip

\noindent
\textit{Proof of Lemma \ref{lem:asymptotic_estimated_policy_difference_function}.} Lemma \ref{lem:asymptotic_estimated_policy_difference_function} directly follows from Lemma \ref{lem:convergence_rate_Stilde} (iv). $\Box$

\bigskip

The following lemma is used in the proof of Lemma Lemma \ref{lem:convergence_rate_Stilde}.
\bigskip

\begin{lemma} \label{lem:convergence_rate_Stilde_A_C}
Suppose that Assumptions \ref{asm:sequential independence}, \ref{asm:bounded outcome}, \ref{asm:overlap}, \ref{asm:rate_of_convergence_backward_Q}, and \ref{asm:bounded entropy} hold. Fix $\underline{a}_{s:T} \in \underline{\MA}_{s:T}$. Let $\widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$, $\widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$, and $\widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$ be defined as (\ref{eq:Stilde_A}), (\ref{eq:Stilde_B_1})-(\ref{eq:Stilde_B_2}), and (\ref{eq:Stilde_C}), respectively. Then, for any integers $s$ and $t$ such that $1\leq s \leq t \leq T$, the following hold:
\begin{align*}
    \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in  \Pi_{t:T}} \left| \widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| &= o_{P}\left(n^{-\min\{1/2,\tilde{\tau}_B/2\}}\right);\\
    \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in  \Pi_{t:T}} \left| \widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| &= o_{P}\left(n^{-1/2}\right);\\
    \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in  \Pi_{t:T}} \left| \widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| &= o_{P}\left(n^{-\tilde{\tau}_B/2}\right).
\end{align*}
\end{lemma}

\bigskip

\begin{proof}
We will prove the results for the case of $s < t$. The results for the case of $s=t$ follow from the same argument. Fix $s$ and $t$, $\underline{a}_{s:T} \in \underline{\MA}_{s:T}$, and $\pi_{t:T}^{a},\pi_{t:T}^{b} \in \Pi_{t:T}$. Without loss of generality, we suppose that $n>n_0$ where $n_0$ appears in Assumption \ref{asm:rate_of_convergence_backward_Q}.


We consider $\widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$. We decompose $\widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$ as follows:
\begin{align*}
    &\widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right) = \widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right) + \widetilde{S}_{s:t,(A2)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right),
\end{align*}
where
\begin{align*}
\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)&:=\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\cdot\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\\
      &\times\left(\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right);\\
	\widetilde{S}_{s:t,(A2)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)&:=\frac{1}{n}\sum_{i=1}^{n}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\cdot\left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right)\\
 &\times\left(\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right).
\end{align*}
For each fold $k$, define
\begin{align*}
\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)&:=\frac{1}{n}\sum_{i \in I_k}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\cdot\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\\
      &\times\left(\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right).
\end{align*}



Fix $k \in \{1,\ldots,K\}$. We first consider $\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$. Since $\widehat{Q}_{t}^{-k}(\cdot,\underline{a}_{t:T})$ is computed using the data in the rest $K-1$ folds, when the data $\{Z_i : k(i) \neq k\}$ in the rest $K-1$ folds is conditioned,  $\widehat{Q}_{t}^{-k}(\cdot,\underline{a}_{t:T})$ is fixed; hence, $\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$ is a sum of i.i.d. bounded random variables under Assumptions \ref{asm:bounded outcome}, \ref{asm:overlap}, and \ref{asm:rate_of_convergence_backward_Q} (ii).

It follows that 
\begin{align*}
    	&E\left[G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\cdot\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\cdot\left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right) \right. \\ & \left. \times \left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right)\middle| \widehat{Q}_{t}^{-k}\left(\cdot,\underline{a}_{t:T}\right)\right]\\
=&	E\left[G_{\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}(H_{i,t})\cdot\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})} \cdot \left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\right. \\ 
& \left. \times E\left[\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right)\middle| H_{i,t}\right]\middle|\widehat{Q}_{t}^{-k}\left(\cdot,\underline{a}_{t:T}\right) \right]\\
=&	0.
\end{align*}
Hence, $\sup_{\pi_{t:T}^a , \pi_{t:T}^b \in  \Pi_{t:T}} \left| \widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|$ can be written as
\begin{align*}
     	&\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|\\
&=	\frac{1}{K}\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\frac{1}{n/K}\sum_{i \in I_k }\left\{ G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\cdot\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right.\right.\\
	&\times \left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right) \\
	&-E\left[G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\cdot\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right.\\
	&\left.\left.\left.\times\left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right)\middle|\widehat{Q}_{t}^{-k}\left(\cdot,\underline{a}_{t:T}\right) \right]\right\} \right|.
\end{align*}

By applying Lemma \ref{lem:concentration inequality_influence difference function} with setting $i \in I_k$ and 
\begin{align*}
    \Gamma_{i}(a_t)=G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}} \frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right),
\end{align*}
the following holds: $\forall \delta > 0$, with probability at least $1-2\delta$,
\begin{align*}
    	&\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|\\
&\leq	o\left(n^{-1/2}\right)+\left(54.4\kappa\left(\pi_{t:T}\right)+435.2+\sqrt{2\log(1/\delta)}\right) \\
	&\times\left[\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}E\left[\left(G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\right)^{2}\cdot\left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right)^{2}\right.\right.\\
&	\left.\left.\left.\times\left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}\left(1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right)^{2}\right|\widehat{Q}_{t}^{-k}\left(\cdot,\underline{a}_{t:T}\right) \right] \middle/ \left(\frac{n}{K}\right) \right]^{1/2} \\
&\leq	o\left(n^{-1/2}\right)+\sqrt{K} \cdot \left(54.4\kappa\left(\pi_{t:T}\right)+435.2+\sqrt{2\log(1/\delta)}\right)
	\cdot \left(\frac{1}{\eta}\right)^{t-s}\\
	&\times \sqrt{\frac{E\left[\left.\left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^2\right|\widehat{Q}_{t}^{-k}\left(\cdot,\underline{a}_{t:T}\right) \right]}{n}},
\end{align*}
where the last inequality follows from $\left(G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\right)^{2}\leq 1$ a.s. and Assumption \ref{asm:overlap} (overlap condition). From Assumptions \ref{asm:bounded outcome} and \ref{asm:rate_of_convergence_backward_Q} (ii), we have $E\left[\left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^2\right]<\infty$. Hence, Markov's inequality leads to
\begin{align*}
    E\left[\left.\left(\widehat{Q}_{t}^{-k}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^2\right|\widehat{Q}_{t}^{-k}\left(\cdot,\underline{a}_{t:T}\right) \right] = O_p(1).
\end{align*}
Note also that $\kappa(\Pi_{t:T})<\infty$ according to Lemma \ref{lem:entropy_integral_bound}. Combining these results, we have
\begin{align}
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| = o_p \left(\frac{1}{\sqrt{n}}\right). \label{eq:A1_convergence}
\end{align}
Consequently, 
\begin{align*}
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| \leq  \sum_{k=1}^{K}\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| = o_p \left(\frac{1}{\sqrt{n}}\right).
\end{align*}

We next consider $\widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$ (we will consider $\widetilde{S}_{s:t,(A2)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$ later). We decompose $\widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$ as follows:
\begin{align*}
    \widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)=\sum_{k=1}^{K}\left(\widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)+\widetilde{S}_{s:t,(B2)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right),
\end{align*}
where
\begin{align*}
    \widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)&:=\frac{1}{n}\sum_{i\in I_k}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\left(\widetilde{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\\
    &\times \left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)};\\
    \widetilde{S}_{s:t,(B2)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)&:=\frac{1}{n}\sum_{i \in I_k}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\left(\widetilde{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\\
    &\times \left(\frac{\prod_{\ell=s}^{t}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t}e_{\ell}(H_{i,\ell},a_{\ell})}\right).
\end{align*}
Fix $k \in \{1,\ldots,K\}$. As for $ \widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$,
when $t<T$, note that 
\begin{align}
&E\left[\widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\middle|\{Z_{i}:k(i)\neq k\}\right] \nonumber \\
 	&=E\left[\frac{1}{n/K}\sum_{i \in I_k}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\left(\widetilde{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\right. \nonumber \\
 	&\left.\times\left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\middle|\{Z_{i}:k(i)\neq k\}\right] \nonumber \\
&=	E\left[\frac{1}{n/K}\sum_{i \in I_k}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}E\left[\widetilde{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\middle|H_{i,t},A_{i,t}=a_{t}\right]\right. \nonumber \\
&\left. \times \left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\middle|\{Z_{i}:k(i)\neq k\}\right] \nonumber \\
&=	E\left[\frac{1}{n/K}\sum_{i \in I_k}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\right. \nonumber \\
&\left.\times \left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\middle|\{Z_{i}:k(i)\neq k\}\right] \nonumber \\
&=	0, \nonumber
\end{align}
where the third equality follows from Assumption \ref{asm:sequential independence}. When $t=T$, $$E\left[\widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\middle|\{Z_{i}:k(i)\neq k\}\right]=0$$ also holds by the same argument. Note that conditional on $\{Z_{i}:k(i)\neq k\}$, $\widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$ is a sum of i.i.d. bounded random variables under Assumptions \ref{asm:bounded outcome}, \ref{asm:overlap}, and \ref{asm:rate_of_convergence_backward_Q} (ii), and its condition mean is zero.
Hence, we can apply Lemma \ref{lem:concentration inequality_influence difference function} with setting $i \in I_k$ and 
\begin{align*}
    \Gamma_{i}(\underline{a}_{s:T})&=	G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\left(\widetilde{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\\
    &\times \left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}
\end{align*}
to obtain the following: $\forall \delta > 0$, with probability at least $1-2\delta$,
\begin{align*}
    	&\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in\Pi_{t:T}}\left|\widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| \\
&\leq	o\left(n^{-1/2}\right)+\left(54.4\kappa\left(\pi_{t:T}\right)+435.2+\sqrt{2\log(1/\delta)}\right) \\
&\times	\left[\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in\Pi_{t:T}}E\left[\left(G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\right)^{2}\cdot\left(\widetilde{\Gamma}_{i,t+1}^{\underline{a}_{(t+1):T}}-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}\right.\right.\\
&\left.\left. \left(\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)^{2}\left(\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right)^{2}\middle|\left\{ Z_{i}:k(i)\neq k\right\} \right]\middle/ \left(\frac{n}{K}\right)\right]^{1/2} \\
&\leq	\left(n^{-1/2}\right)+\sqrt{K}\cdot\left(54.4\kappa\left(\pi_{t:T}\right)+435.2+\sqrt{2\log(1/\delta)}\right)\cdot\left(\frac{2M}{\eta^{T-t}}+\sum_{j=0}^{T-t-1}\frac{M}{\eta^j}\right)\cdot \frac{1}{\eta} \\
&\times	\sqrt{\frac{E\left[\left(\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)^{2}\middle|\left\{ Z_{i}:k(i)\neq k\right\} \right]}{n}},
\end{align*}
where the last inequality follows from $\left(G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\right)^{2}\leq 1$ a.s. and Assumptions \ref{asm:bounded outcome} and \ref{asm:overlap}. From Assumptions \ref{asm:overlap} and \ref{asm:rate_of_convergence_backward_Q} (ii), we have
\begin{align*}
    E\left[\left(\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)^{2}\right] < \infty.
\end{align*}
Hence, Markov's inequality leads to
\begin{align*}
    E\left[\left(\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k}(H_{i,\ell},a_{\ell})}\right)^{2}\middle|\left\{ Z_{i}:k(i)\neq k\right\} \right] = O_p(1).
\end{align*}
Note also that $\kappa(\Pi_{t:T})<\infty$ according to Lemma \ref{lem:entropy_integral_bound}. Combining these results, we have
\begin{align}
    \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| = o_p \left(\frac{1}{\sqrt{n}}\right).  \label{eq:B1_convergence}
\end{align}
By applying the same argument to derive (\ref{eq:B1_convergence}), we also obtain 
\begin{align*}
   \sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(B2)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| = o_p \left(\frac{1}{\sqrt{n}}\right).
\end{align*}
Consequently, 
\begin{align*}
    &\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(B)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| \\
    &\leq  \sum_{k=1}^{K}\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(B1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|
     + \sum_{k=1}^{K}\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(B2)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|\\
    &= o_p \left(\frac{1}{\sqrt{n}}\right).
\end{align*}

We next consider to bound $\sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|$ from above. It follows that 
\begin{align*}
    		&\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|\\
	&=\frac{1}{n}\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\sum_{i=1}^{n}G_{i,\pi_{t:T}^{a},\pi_{t:T}^{b}}^{\underline{a}_{s:T}}\cdot\frac{\prod_{\ell=s}^{t-1}1\{A_{i,\ell}=a_{\ell}\}}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}\right.\\
	&\times\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right)\left.\cdot\left(\frac{1}{\hat{e}_{t}^{-k(i)}\left(H_{i,t},a_{t}\right)}-\frac{1}{e_{t}\left(H_{i,t},a_{t}\right)}\right)\cdot1\left\{ A_{i,t}=a_{t}\right\} \right|\\
	&\leq\frac{1}{n}\sum_{i=1}^{n}\left|Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right|\cdot \left|\frac{1}{\prod_{\ell=s}^{t}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{1}{e_{t}\left(H_{i,t},a_{t}\right)\cdot\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}\right|\\
	&\leq\frac{1}{n}\sum_{i=1}^{n}\left|Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right| \cdot \left|\frac{1}{\prod_{\ell=s}^{t}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t}e_{\ell}(H_{i,\ell},a_{\ell})}\right|\\
	&+\frac{1}{n}\sum_{i=1}^{n}\left|Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right|\cdot\left|\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right|\left(\frac{1}{e_t(H_{i,t},a_t)}\right)\\
	&\leq\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}}\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(\frac{1}{\prod_{\ell=s}^{t}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t}e_{\ell}(H_{i,\ell},a_{\ell})}\right)^{2}}\\
	&+\left(\frac{1}{\eta}\right)\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}}\\ &\times \sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right)^{2}}
\end{align*}
where the last inequality follows from Cauchy-Schwartz inequality and Assumption \ref{asm:overlap} (overlap condition). Taking expectation of both sides yields:
\begin{align*}
 E\left[\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|\right] 
 &\leq	E\left[\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}}\right. \\
&	\left.\times\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(\frac{1}{\prod_{\ell=s}^{t}\hat{e}_{\ell}^{-k(i)}\left(H_{i,\ell},a_{\ell}\right)}-\frac{1}{\prod_{\ell=s}^{t}e_{\ell}\left(H_{i,\ell},a_{\ell}\right)}\right)^{2}}\right] \\
& + \eta^{-1}E\left[\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}}\right. \\
&	\left.\times\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}\left(H_{i,\ell},a_{\ell}\right)}-\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}\left(H_{i,\ell},a_{\ell}\right)}\right)^{2}}\right] \\
 &\leq	\sqrt{\frac{1}{n}\sum_{i=1}^{n}E\left[\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}\right]} \\
&	\times\sqrt{\frac{1}{n}\sum_{i=1}^{n}E\left[\left(\frac{1}{\prod_{\ell=s}^{t}\hat{e}_{\ell}^{-k(i)}\left(H_{i,\ell},a_{\ell}\right)}-\frac{1}{\prod_{\ell=s}^{t}e_{\ell}\left(H_{i,\ell},a_{\ell}\right)}\right)^{2}\right]} \\
&+ \eta^{-1}\sqrt{\frac{1}{n}\sum_{i=1}^{n}E\left[\left(Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)-\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}\right]} \\
&	\times\sqrt{\frac{1}{n}\sum_{i=1}^{n}E\left[\left(\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}\left(H_{i,\ell},a_{\ell}\right)}-\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}\left(H_{i,\ell},a_{\ell}\right)}\right)^{2}\right]} \\
 &=	o\left(n^{-\tilde{\tau}_{B}/2}\right),
\end{align*}
where the second inequality follows from Cauchy-Schwartz inequality and the last line follows from Assumption \ref{asm:rate_of_convergence_backward_Q} (i). Then applying Markov's inequality leads to
\begin{align}
    \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(C)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| &= o_{P}\left(n^{-\tilde{\tau}_B/2}\right). \label{eq:S_C_convergence}
\end{align}


Now let us consider $\widetilde{S}_{s:t,(A2)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)$. Note that 
\begin{align*}
    	&\sup_{\pi_{t:T}^{a},\pi_{t:T}^{b}\in \Pi_{t:T}}\left|\widetilde{S}_{s:t,(A2)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|\\
    	&\leq\frac{1}{n}\sum_{i=1}^{n}\left|\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right|\cdot\left|\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right|\\
    	&\times \left|1-\frac{1\left\{ A_{i,t}=a_{t}\right\} }{e_{t}\left(H_{i,t},a_{t}\right)}\right|\\
	&\leq\left(\frac{1}{\eta}\right)\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(\frac{1}{\prod_{\ell=s}^{t-1}\hat{e}_{\ell}^{-k(i)}(H_{i,\ell},a_{\ell})}-\frac{1}{\prod_{\ell=s}^{t-1}e_{\ell}(H_{i,\ell},a_{\ell})}\right)^{2}}\\
	&\times \sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(\widehat{Q}_{t}^{-k(i)}\left(H_{i,t},\underline{a}_{t:T}\right)-Q_{t}\left(H_{i,t},\underline{a}_{t:T}\right)\right)^{2}},
\end{align*}
where the last inequality follows from Assumption \ref{asm:overlap} (overlap condition) and Cauchy-Schwartz inequality. Then, by applying the same argument to derive (\ref{eq:S_C_convergence}), we obtain
\begin{align*}
    \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(A2)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| &= o_{P}\left(n^{-\tilde{\tau}_B/2}\right).
\end{align*}
 Combining this result with (\ref{eq:A1_convergence}), we obtain 
 \begin{align*}
     \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(A)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right|  
     &\leq \sum_{k=1}^{K} \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(A1)}^{\underline{a}_{s:T},k}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| \\
     &+ \sup_{\pi_{t:T}^a , \pi_{t:T}^b \in \Pi_{t:T}} \left| \widetilde{S}_{s:t,(A2)}^{\underline{a}_{s:T}}\left(\pi_{t:T}^{a},\pi_{t:T}^{b}\right)\right| \\
     &= o_{P}\left(n^{-\min\{1/2,\tilde{\tau}_B/2\}}\right).
 \end{align*}
Consequently, we have shown the result (\ref{eq:sequential_bound}).
 
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{ecta}
\bibliography{ref_DTR,ref_surrogate_loss}


\end{document}