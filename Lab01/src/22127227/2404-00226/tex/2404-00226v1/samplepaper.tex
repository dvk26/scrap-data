% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{bbding}
\usepackage{cite}
\usepackage[colorlinks=true]{hyperref}
\hypersetup{
    linkcolor=red, 
    urlcolor=green,
}
\renewcommand{\labelitemi}{$\bullet$}
% \usepackage{showframe}

% Used for displaying a sample Figure If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Design as Desired: Utilizing Visual Question Answering for Multimodal Pre-training}

\author{Anonymous}
\institute{Anonymous Organization\\
\email{**@******.***}}
%
\titlerunning{Design as Desired: Utilizing VQA for Multimodal Pre-training}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{Tongkun Su\inst{1,2} \and
% Jun Li\inst{3}\thanks{The first two authors contribute equally to this work.} \and
% Xi Zhang\inst{1,2} \and Baoliang Zhao\inst{1} \and Hao Chen\inst{5} \and Qiong Wang\inst{1,6(}\Envelope\inst{)} \and Faqin Lv\inst{4(}\Envelope\inst{)} \and Pheng-Ann Heng\inst{6}, Yin Hu\inst{1}}
% % %
% % % First names are abbreviated in the running head.
% % % If there are more than two authors, 'et al.' is used.
% % %
% \institute{Shenzhen Institute of Advanced Technology, Chinese Academy of Science\\
% \email{\{tk.su, wangqiong\}@siat.ac.cn}\\
% \and
% University of Chinese Academy of Science\\
% \and
% Technical University of Munich\\
% \email{june.li@tum.de}
% \and
% % Helmholtz Center Munich\\
% % \and
% Southern Medical University\\
% \email{lvjin8912@163.com}
% \and
% The Hong Kong University of Science and Technology\\
% \and
% The Chinese University of Hong Kong
% }
%
\maketitle              % typeset the header of the contribution

\begin{abstract}

Multimodal pre-training demonstrates its potential in the medical domain, which learns medical visual representations from paired medical reports. However, many pre-training tasks require extra annotations from clinicians, and most of them fail to explicitly guide the model to learn the desired features of different pathologies. To the best of our knowledge, we are the first to utilize Visual Question Answering (VQA) for multimodal pre-training to guide the framework focusing on targeted pathological features. In this work, we leverage descriptions in medical reports to design multi-granular question-answer pairs associated with different diseases, which assist the framework in pre-training without requiring extra annotations from experts. We also propose a novel pre-training framework with a quasi-textual feature transformer, a module designed to transform visual features into a quasi-textual space closer to the textual domain via a contrastive learning strategy. This narrows the vision-language gap and facilitates modality alignment. Our framework is applied to four downstream tasks: report generation, classification, segmentation, and detection across five datasets. Extensive experiments demonstrate the superiority of our framework compared to other state-of-the-art methods. Our code will be released upon acceptance. 

% \keywords{Multimodal Pre-training \and Visual Question Answering \and Report Generation \and Contrastive Learning}
\keywords{Multimodal Pre-training \and Visual Question Answering}

\begin{figure}[h!]
    \centering
    % \includegraphics[width=0.85\textwidth]{Figure/VQA example.pdf}
    \includegraphics[width=0.9\textwidth]{Figure/ultrasound_intro.pdf}
    \caption{Overview of the VQA design. Left: the original report. Right: question and answer pairs. We design three levels of VQA to enable the model to capture multi-granular features. Numerical values are replaced with a special token [size].}
    \label{VQA}
\end{figure}
\end{abstract}

\section{Introduction}
% In recent years, multitask learning received great attention, especially in multimodal pre-training. This approach requires the model to tackle several vision-language tasks simultaneously during pre-training, enabling the model to capture multi-granular features and enhance performance across different downstream tasks. Generally, there are two main streams in multitask learning for pre-training: supervised and self-supervised. Supervised multitask learning enhances the model with auxiliary supervised tasks. Yan \textit{et al.} \cite{ClinicalBERT} introduce a classification task during pre-training to assist the model in distinguishing different diseases, improving the accuracy of report generation during fine-tuning. 
% However, the drawback is supervised method needs extra annotations from clinicians, which is often time-consuming and cost-prohibitive in the medical domain. In contrast to supervised methods, self-supervised (SSL) tasks, like contrastive learning \cite{ConVIRT,GLoRIA,MGCA,CMITM} and mask modeling \cite{CMITM,MRM}, require no additional labeled data and implicitly enhance the model to understand semantic information through modality alignment and reconstruction. 
% However, SSL methods often struggle to direct the model to focus on the desired target features during the pre-training.

In recent years, multimodal pre-training has received significant attention. In clinical settings, there is a vast amount of medical images and text reports stored in the database, making them easily available for pre-training tasks. Multimodal pre-training aims to learn generalized representations through the inherent relationships between image-report pairs to benefit various downstream tasks, particularly where annotated data is limited. Generally, there are two mainstream methods in multimodal pre-training: contrastive learning  \cite{MedCLIP,MGCA,GLoRIA,CMITM} and cross-modal reconstruction \cite{MedIM,MRM,CMITM}. The first method aims to learn discriminative representations by pulling closer the representations from paired images and reports while pushing away the unpaired ones. The second method generates one modality from the other, which assists the framework in understanding the relationship between two modalities. However, these methods have some drawbacks. On the one hand, some methods \cite{MedCLIP,MedIM} require extra annotations from clinicians, which is often time-consuming and cost-prohibitive. On the other hand, unlike supervised learning, where labels guide the model to learn the features of interest, self-supervised methods do not explicitly direct the model to focus on the specific features associated with different diseases.

% Second paragraph,
Therefore, our objective is to explore an effective medical multimodal pre-training task that can guide the model to focus on the desired features of different pathologies without the need for further annotations by clinicians. Our attention turns to VQA, an essential task in cross-modal generation which requires the model to understand both visual and textual knowledge\cite{CAT-ViL,WSDAN,MedVQA,MedVQALLM}. Clinicians just need to provide the pathologies they care about in the reports, and we can design different questions according to their instructions. By answering the questions during pre-training, the framework will attempt to focus on different levels of information according to the questions. Figure \ref{VQA} illustrates the main idea of question-answer design. We design VQA tasks at three levels of granularity to guide the model in learning essential details in the image and text.

Additionally, we propose a Quasi-textual Feature Transformer (QFT) module with a contrastive learning strategy to help the framework bridge the gap between image and text. This challenge stems from the fact that discriminative pathological features and tokens often occupy only a small fraction of the medical images and reports, making it difficult for the model to learn their associations effectively. Inspired by QFormer \cite{BLIP2}, we propose a QFT module, which utilizes contrastive learning to convert visual features into a quasi-textual domain that is closer to the textual domain. This transformation narrows the distribution gap between two modalities and improves the model's visual understanding capabilities. Moreover, we construct an ultrasound dataset with different organs for pre-training, comprising 10,720 ultrasound images and 5,360 reports. We transfer our model to four downstream tasks: report generation, classification, detection and segmentation. Extensive experiments demonstrate the superior performance of our framework compared to other State-Of-The-Art (SOTA) methods. Overall, our main contributions can be summarized as:  
\begin{itemize}
    \item To the best of our knowledge, we are the first to utilize VQA for multimodal pre-training in the medical field to assist the framework in focusing on the desired pathological features without extra expert annotationss.
    \item We propose a  QFT module with a contrastive learning strategy, which aligns the visual features into a quasi-textual domain to narrow the modality gap and facilitate modality alignment.
    \item Our approach demonstrates significant improvement in four downstream tasks: report generation, classification, detection and segmentation.
\end{itemize}

% \section{Related Work}
% \subsubsection{Report Generation}
% Unlike image captioning, medical report generation needs to generate longer text and requires a stronger understanding of fine-grained visual features. Currently, there are two main design approaches for medical report generation systems: one is to utilize additional labels for multi-task learning\cite{CNN-LSTM,TriNet,SGF}, and the other is to mimic the workflow of medical professionals\cite{R2Gen,SGF}.

% \subsubsection{Medical Multimodal Pretraining}
% Medical images and reports are the most common data in medical domain, providing the fundamentals for multimodal pretraining. Unlike common multimodal pretraining, medical multimodal pretraining focus on pretraining a visual foundation model. The prototype of the well-known CLIP model\cite{CLIP}, ConVIRT\cite{ConVIRT}, was proposed in the medical domain. Through image-text contrastive learning, this model achieved SOTA performance on few-shot classification tasks. Following ConVIRT, GLoRIA\cite{GLoRIA} and MGCA\cite{MGCA} emerged, shifting the attention to extracting multi-granularity visual features. However, they all rely on extensive image-text data. This raises new questions: can these methods be
% effective in small-scale pretraining and if not, how can we pretrain a model with
% limited data?

% \section{Overview of the Multi-granular VQA}
% We design VQA tasks at three granularities. In coarse-grained VQA task, we require the model to generate complete reports. Due to the data bias caused by structured templates in medical reports, when answering these questions, the model mainly focuses on the report formats while overlooking some details hidden in the templates, and thus fails to capture fine-grained features. In medium-grained VQA, we require the model to generate clinical descriptions for different anatomical regions, which enables the model to learn basic anatomical divisions. In fine-grained VQA, we design a series of questions based on the report, targeting subtle but crucial features in the images.  Given that nodule recognition is a focal point in ultrasound, a large portion of our designed questions are related to nodules. The original text is in Chinese and was translated with ChatGPT \cite{ChatGPT}. We exhibit more details in supplementary materials.

\section{Methods}
\label{methods}
% In this section, we first introduce our proposed MGLM method. Following this, we introduce the design principles and details of our proposed report generation architecture based on QFE.

% \subsection{Overview of the Multi-granularity VQA pipline.}
% In other to implement VQA, we manually annotate a VQA dataset based on medical reports. It's worth noting that this requires no expert knowledge. We divide the designed VQA tasks into three grains: coarse, medium, and fine, to accommodate the model’s ability from weak to strong. Specifically, we regard report generation as coarse-grained VQA task. This task enables the model to understand the structure of a medical report and specialized terminology. This task also serves as the foundation for report generation. For medium-granularity, we require the model to generate the description according to different anatomical regions based on the prompt of each region. This task enables the model to understand the spatial relationships in medical images, such as left lobe, right side and so on. For fine-grained VQA, we design a series of questions based on the reports. Answering these questions requires the model to accurately capture the fine-grained visual details. Given that nodule recognition is a focal point in medical domain, a large portion of our designed questions are relevant to nodules. The questions we designed are illustrated in Figure\ref{VQA}. The coarse, medium, and fine-grained tasks correspond to three levels of difficulty. In the coarse-grained VQA, subtle but critical information about disease is hidden in the report templates and thus, the model spends a large portion of training time learning the report format while overlooking this information. In medium-grained tasks, the model needs a basic perception of the images to identify various regions of the organs. The fine-grained tasks demand a comprehensive understanding of the image details for accurately localizing key features.

% We manually annotate a VQA dataset based on the report's contents. We design VQA tasks at three granularities: coarse, medium, and fine, corresponding to the granularity of visual features needed to answer the questions. By answering these questions, the model is expected to learn how to extract multi-granular visual features. Specifically, in coarse-grained VQA task, we require the model to generate complete reports. This task also serves as the foundation of report generation. Due to the data bias caused by structured templates in medical reports, when answering these questions, the model mainly focuses on the report formats while overlooking some details hidden in the templates, and thus fails to capture fine-grained features. In medium-grained VQA, we require the model to generate clinical descriptions for different anatomical regions, which enables the model to learn basic anatomical divisions. In fine-grained VQA, we design a series of questions based on the report, targeting subtle but crucial features in the images.  Given that nodule recognition is a focal point in ultrasound, a large portion of our designed questions are related to nodules. Examples of VQA are illustrated in Figure \ref{VQA} and supplementary materials.

% \begin{table}[H]
% \centering
% \caption{The questions we design in MGLM. The original data is in Chinese and we have translated then into English here.}
% \label{MGLM}
% \begin{tabular}{c|c|c}
% \hline
% Organ                    & Grain                   & Question(Prompt)                               \\ \hline
% \multirow{8}{*}{Breast}  & Coarse                  & Generate ultrasound report:                    \\ \cline{2-3} 
%                          & \multirow{3}{*}{Medium} & Describe left breast:                          \\
%                          &                         & Describe right breast:                         \\
%                          &                         & Describe axillary:                             \\ \cline{2-3} 
%                          & \multirow{4}{*}{Fine}   & Are there any nodules in the images?           \\
%                          &                         & Where are the nodules located in the images?   \\
%                          &                         & Are there dilated mammary ducts in the images? \\
%                          &                         & Where are the dilated mammary ducts located?   \\ \hline
% \multirow{8}{*}{Thyroid} & Coarse                  & Generate ultrasound report:                    \\ \cline{2-3} 
%                          & \multirow{3}{*}{Medium} & Describe left robe:                            \\
%                          &                         & Describe right robe:                           \\
%                          &                         & Describe thyroid overview:                     \\ \cline{2-3} 
%                          & \multirow{4}{*}{Fine}   & How is the size and shape of the thyroid?      \\
%                          &                         & Are there any nodules in the images?           \\
%                          &                         & Where are the nodules located in the image?    \\
%                          &                         & How is the echogenicity of the thyroid?        \\ \hline
% \end{tabular}
% \end{table}

% \textbf{Coarse-Grained Language Modeling}
% We regard report generation as coarse-grained language modeling task. This task enables the model to understand the structure of a medical report and specialized terminology. This task
% also serves as the foundation for report generation.

% \textbf{Medium-Grained Language Modeling}
% We require the model to generate the description according to different anatomical regions based on the prompt of each region. This task enables the model to understand the spatial relationships in medical images, such as left lobe, right side and so on.

% \textbf{Fine-Grained Language Modeling}
% We design a series of questions based on the reports. Answering these questions requires a comprehensive understanding of medical images. Given that nodule recognition is a focal point in medical domain, a large portion of our designed questions are relevant to nodules.

% \begin{figure}[]
%     \centering
%     \includegraphics[width=\textwidth]{Figure/VQA example.pdf}
%     \caption{Example of Multi-granularity language modeling tasks. The original text is in Chinese. We display the text translated using ChatGPT for the purpose of saving space in the Figure The same applies to the following Figure <Empty> means that this question and answer will not be used as a sample in language modeling.}
%     \label{Example}
% \end{figure}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}



% The coarse, medium, and fine-grained language modeling tasks correspond to three levels of difficulty. In the coarse-grained language modeling task, subtle but critical information about disease is hidden in the report templates and thus, the model spends a large portion of training time learning the report format while overlooking this information. In the medium-grained task, the model needs a basic perception of the images to identify various regions of the organs. The fine-grained tasks demand a comprehensive understanding of the image details
% for accurately localizing key features.

% In the diffusion model, CLIP is chosen as the text encoder to map text to a quasi-image domain, improving the model understanding of the text. In the research on generating text from images, similar attempts are relatively scarce.

\begin{figure}[t!]
    \centering
    \label{QFT}
    \includegraphics[width=\textwidth]{Figure/QFE.pdf}
    \caption{Overview of our framework. The image and text features extracted by the visual and text encoders are aligned by the quasi-textual Feature transformer module with two contrastive learning tasks. Then the quasi-textual features are fed to the text generator to generate answers based on the questions during pre-training. TE domain, VE domain, QFT domain denote the latent space of global textual features $T_i$, global visual features $V_i$ and quasi-textual features $Q_i$, respectively.}
\end{figure}

\subsubsection{Overview.}
We aim to conduct multimodal pre-training through VQA, enabling the model to learn multi-granular pathological information, and thereby enhancing the performance across various downstream tasks. We employ an encoder-decoder architecture as the backbone of our model. Given a mini-batch of images and their corresponding question-answer pairs with the batch size of $B$, we encode the images using the visual encoder (\textit{e.g.}, ViT \cite{ViT} or ResNet \cite{ResNet}) to obtain global features $V_i$ and patch features $v_i, i\in B$. Subsequently, the text generator (\textit{e.g.}, GPT \cite{GPT}) generates answers based on the obtained image features and questions. Inspired by QFormer \cite{BLIP2}, we propose a Quasi-textual Feature Transformer (QFT) module on top of the general encoder-decoder framework to facilitate modality alignment. Our model also includes a text encoder (\textit{e.g.}, BERT \cite{BERT}), which encodes reports into global features $T_i$ and token features $t_i$. Our framework is illustrated in Figure \ref{QFT}.

\subsubsection{Quasi-Textual Feature Transformer.}
\label{QFT section}
In the medical field, discriminative pathological features generally occupy only a small fraction of the medical images, which poses a challenge for modality alignment. To narrow the gap between image and text, we propose a Quasi-textual Feature Transformer (QFT) module with a contrastive learning strategy. The backbone of QFT is a multi-layer bidirectional transformer decoder \cite{transformer}. It takes $m$ trainable queries as input. The queries interact with patch features $v_i$ through the cross-attention mechanism. To enforce the queries to extract quasi-textual features $Q_i$ that are closer to the textual domain, we implement a Q-Contrastive Learning (QCL) for the QFT module.
% train the QFT such that the queries can learn to extract quasi-textual features $Q_i$ that are closer to the textual domain from $v_i$, we implement a Q-Contrastive Learning (QCL) process:
% In order to train the QFT such that the queries can learn to extract quasi-textual features $Q_i$ that are closer to the textual domain from $v_i$, we implement a Q-Contrastive Learning (QCL) process:
%Unlike common multimodal contrastive learning that the features from two modalities are $B \times H$, $q_i$ in Q-contrastive learning is $B \times Q \times H$, where $Q$ is the number of queries. 
We calculate the pairwise similarity between global textual features $T_i$ and all $m$ tokens in the output $Q_i$ from the QFT module, selecting the highest score in the similarity matrix as the final score, which can be formulated as $s_q(Q_i, T_i) = \max_{0\leq l \leq m} s(Q_i[l], T_i)$, where $s(\cdot, \cdot)$ computes the cosine similarity, $Q_i[l]$ is one of the output token in $Q_i$. Then we use this similarity score to calculate the InfoNCE loss \cite{InfoNCE}. By minimizing this loss, the model will align the latent space of $Q_i$ with that of $T_i$ together. Given that medical images of the same organ share similar features \cite{SGF}, leading to significant visual redundancy, we introduce a bottleneck design into our model. we set the $m$ much smaller than the number of visual tokens $v_i$. This bottleneck requires the QFT module to compress visual information and extract the most robust information, while also reducing the computational cost of the text generator.

% \begin{equation}
%     l_{qcl, i}^{v2t} = -log\frac{exp(s_q(q_i, T_i)/\tau_q)}{\sum_{j=1}^B exp(s_q(q_i, T_j)/\tau_q)}
% \end{equation}
% \begin{equation}
%     l_{qcl, i}^{t2v} = -log\frac{exp(s_q(q_i, T_i)/\tau_q)}{\sum_{j=1}^B exp(s_q(\hat{t_j}, T_i)/\tau_q)}\\
% \end{equation}
% \begin{equation}
%     L_{qcl} = E_{i\in B}\left[-\left(log\frac{exp(s_q(q_i, T_i)/\tau_q)}{\sum_{j=1}^B exp(s_q(q_i, T_j)/\tau_q)} + log\frac{exp(s_q(q_i, T_i)/\tau_q)}{\sum_{j=1}^B exp(s_q(\hat{t_j}, T_i)/\tau_q)}\right) / 2\right]
% \end{equation}
% \begin{equation}
%     s_q(\hat{t}, T) = \max_{0\leq j \leq Q} s(\hat{t}[j], T)
% \end{equation}


% As QFT is initialized with a language model, it can hardly understand visual features at the beginning of training, indicating that the initial QFT domain is meaningless. Directly mapping TE and QFT domain together would lead to converging at a region which is also unreasonable. 

% Additionally, we introduce vision-language Contrastive Learning (CL) to aligns the VE and the TE domains together, and so does the QFT domain, ultimately maintaining more visual features.  aligns the VE and the TE domains together, and so does the QFT domain, ultimately maintaining more visual features. Directly pulling the QFT domain and the TE domain together would ignore the important visual information in the quasi-textual feature. This is may becasue .....

Additionally, we introduce the vision-language Contrastive Learning (CL) to enhance the model's visual perception. As mentioned in \cite{MME}, many QFormer-based models perform poorly on visual perception tasks, especially in position recognition, which is vital in the medical domain. We assume that this phenomenon is because these models all apply a pre-training process similar to QCL. Although QCL facilitates alignment between different modalities, directly pulling $Q_i$ to the textual domain will lead to a significant loss of fine-grained visual information. Thus we use contrastive learning as a constraint to further align the latent space of visual features $V_i$ and that of textual features $T_i$ together, ultimately maintaining more visual features in $Q_i$. Moreover, following \cite{ALBEF}, we introduce three buffers with a buffer size of $N$ for QCL and CL. We store the global features $T_i$, $V_i$ and quasi-textual features $Q_i$ from the most recent batches as negative samples for the current batch. Thus, each batch has $N+B$ negative samples. The QCL and CL loss can be formulated as below, where $\tau_q$ and $\tau_c$ are the temperature parameter.
% \begin{equation}
%     l_{cl, i}^{v2t} = -log\frac{exp(s(V_i, T_i)/\tau_q)}{\sum_{j=1}^B exp(s(V_i, T_j)/\tau_c)}
% \end{equation}
% \begin{equation}
%     l_{cl, i}^{t2v} = -log\frac{exp(s(V_i, T_i)/\tau_q)}{\sum_{j=1}^B exp(s(V_j, T_i)/\tau_c)}\\
% \end{equation}

{\small
\begin{equation}
     \mathcal L_{qcl} = -\frac{1}{2B}\sum_i^B\left[log\frac{\exp(s_q(Q_i, T_i)/\tau_q)}{\sum_{j=1}^{N+B} \exp(s_q(Q_i, T_j)/\tau_q)} + log\frac{\exp(s_q(Q_i, T_i)/\tau_q)}{\sum_{j=1}^{N+B} \exp(s_q(Q_j, T_i)/\tau_q)}\right]
\end{equation}
\begin{equation}
    \mathcal L_{cl} = -\frac{1}{2B}\sum_i^B\left[log\frac{\exp(s(V_i, T_i)/\tau_c)}{\sum_{j=1}^{N+B} \exp(s(V_i, T_j)/\tau_c)} + log\frac{\exp(s(V_i, T_i)/\tau_c)}{\sum_{j=1}^{N+B} \exp(s(V_j, T_i)/\tau_c)}\right]
\end{equation}}

\subsubsection{Vision Question Answering.}
According to the data we gathered from the hospital, each report is associated with two images. Thus, we encode two images separately with a shared visual encoder and the QFT module. We take the average of two global visual features $V_i$ for CL, the average of two quasi-textual features $Q_i$ for QCL, and the concatenation of $Q_i$ for generation. For each pair of medical images, we randomly sample three question-answer pairs from coarse, medium, and fine-grained VQA tasks respectively. We then use the questions as prompts to guide the generation. Given the question $p_i$ and ground truth answer $y_i = \{y_{i, 1}, y_{i, 2}, ... y_{i, L}\}$ composed of $L$ tokens, the language modeling loss can be formulated as:
\begin{equation}
    l_{clm, mlm, flm} = \frac{1}{BL}\sum_i^B\sum_j^L \left[-log(p(y_{i,j}|y_{i,<j}, p_i, q_i)) \right]
\end{equation}
\begin{equation}
    \mathcal L_{lm} = \lambda_c l_{clm} + \lambda_m l_{mlm} + \lambda_f l_{flm}
\end{equation}
where $l_{clm, mlm, flm}$ represent language modeling loss of coarse, medium and fine-grained VQA. $\lambda_c$, $\lambda_m$, $\lambda_f$ are hyperparameters. We apply ViT-B to initialize the visual encoder, the first six layers of ERNIE\cite{ERNIE-health-zh} to initialize text encoder, and GPT2-small \cite{GPT2,UER,Tencentpretrain} to initialize the text generator. For the QFT module, we initialize it with the first three layers of GPT2-small to equip it with the same knowledge as the text generator. We weight CL with $\lambda$ to balance the constraint. Our loss function is shown as below:
\begin{equation}
    \mathcal L = \lambda \mathcal L_{cl} + \mathcal L_{qcl} + \mathcal L_{lm}
\end{equation}

\section{Experiments}
\subsubsection{Dataset.}
We construct a dataset for experiments with 10,720 ultrasound images and 5,360 Chinese reports for breast and thyroid. The dataset is divided into training, validation, and test sets in a ratio of 7:1:2. The training and validation sets are used for pre-training, while the test set is used for downstream report generation. For three visual recognition tasks, we evaluate the effectiveness of pre-training on three public datasets: BUSI \cite{BUSI}, AUITD \cite{AUITD}, and DDTI \cite{DDTI}. 
\subsubsection{VQA Design.}
Figure \ref{VQA} shows the overview of our VQA design (the texts are translated from Chinese to English by ChatGPT \cite{ChatGPT}). In coarse-grained VQA, we require the model to generate complete reports. During this process, the model will focus on the report formats and writing styles. However, the medical reports are usually very long and templated, so coarse-grained VQA is not enough to highlight the details in the reports. Therefore, we further design medium and fine-grained questions. In medium-grained VQA, we require the model to generate clinical descriptions for various anatomical regions, enabling the model to effectively differentiate anatomical structures. In fine-grained VQA, we design a series of questions based on the report, targeting subtle but crucial pathological visual features.  Given that nodule recognition is the most important in ultrasound, our designed questions are mainly focusing on the presence and location of the nodules. Finally, we obtain a VQA dataset with 23,572 question-answer pairs. More examples are presented in the supplementary material.


\subsubsection{Framework Setup.}
Considering that report generation is a generative task, while visual recognition is a discriminative task, we separately train the framework with two settings for different downstream tasks. For the report generation, since we utilize it as coarse-grained VQA in pre-training, we directly utilize the prompt in coarse-grained to require the model to generate reports. We set $\lambda_{c} = 9$, $\lambda_{m} = 1$, and $\lambda_f = 3$ to emphasize coarse-grained VQA. We use BLEU scores \cite{BLEU}, METEOR \cite{METEOR} and ROUGE-L \cite{ROUGE-L} to evaluate report quality. For the other three visual recognition tasks, to guide the model focusing on the pathological features emphasized in the middle and fine-grained VQA, we set $\lambda_{c} = 1$, $\lambda_{m} = 3$, and $\lambda_f = 9$. We transfer the pre-trained visual encoder into different downstream backbones (\textit{e.g.}, YOLOv3 \cite{YOLOv3} for detection) and only fine-tune the other parts of the backbone. We compare accuracy (ACC) for classification, average precision (AP) for detection and DICE score for segmentation. We set $\lambda = 1$ in both settings. All baselines are retrained on our ultrasound dataset. More framework setup details are illustrated in supplementary materials. 

% Our settings are detailed in supplementary materials. In multimodal pretraining, we transfer the pretrained VE into different downstream tasks and only fine-tune the other parts of the backbone. For report generation, we emphasize coarse-grained VQA, since it's the base of report generation, by setting $\lambda_{c} = 9$, $\lambda_{m} = 1$, and $\lambda_f = 3$. In multimodal pretraining, to alleviate the impact of redundancy from templates and strengthen the guidance of VQA, we set $\lambda_{c} = 1$, $\lambda_{m} = 3$, and $\lambda_f = 9$. All baselines are retrained on our dataset.




% \subsubsection{Setting}
% Following \cite{MGCA}, we train our framework for 50 epochs with a batch size of 25. The AdamW optimizer\cite{AdamW} is applied with a learning rate of 2e-5 and a weight decay of 0.05. Additionally, we incorporate a linear warmup strategy in combination with a cosine annealing scheduler. Initially, the learning rate is set to 1e-8, and a warmup period of 20 epochs is applied. Given that report generation is our primary task, we emphasize the importance of the coarse-grained language modeling task by setting the weights as $\lambda_{c} = 9$, $\lambda_{m} = 1$, and $\lambda_f = 3$.


% \begin{table}[ht!]
% \scriptsize
% \caption{The downstream performance of multimodal pretraining. We only report DET results on ResNet since we use YOLOv3 \cite{YOLOv3} as our backbone, which is based on CNN. Even with limited pretraining data, our method achieves SOTA performance across various tasks, while other methods exhibit notable shortcomings.}
% \centering
% \begin{tabular}{p{1.9cm}>{\centering\arraybackslash}m{1.4cm}>{\centering\arraybackslash}m{1.4cm}>{\centering\arraybackslash}m{1.4cm}>{\centering\arraybackslash}m{1.4cm}>{\centering\arraybackslash}m{1.4cm}>{\centering\arraybackslash}m{1.4cm}}
% \hline
% \multirow{2}{*}{Method} & \multicolumn{2}{c}{CLS (AUC\%$\uparrow$)} & \multicolumn{2}{c}{DET (AP\%$\uparrow$)} & \multicolumn{2}{c}{SEG (DICE\%$\uparrow$)} \\
%                    & BUSI           & AUITD          & BUSI           & DDTI           & BUSI           & DDTI          \\ \hline
% Random(ViT)        & 56.4          & 81.3          & -              & -              & 38.1          & 58.1         \\
% Random(Res)        & 61.5          & 81.3          & 51.5          & 13.9          & 58.0          & 64.7         \\
% ImageNet(ViT)      & 84.5          & 82.5          & -              & -              & 63.9          & 61.5         \\
% ImageNet(Res)      & 82.9          & 82.2          & \textbf{66.7} & 50.0          & 49.0          & 61.1         \\
% GloRIA(Res)        & 85.5          & 80.2          & 54.9          & 21.1          & 63.7          & 63.8         \\
% MGCA(ViT)          & 82.9          & 80.2          & -              & -              & 61.2          & 68.7         \\
% MGCA(Res)          & 82.9          & 82.2          & 55.5          & 10.5          & 59.2          & 68.8         \\
% MRM(ViT)           & 69.2          & 81.3          & -              & -              & 61.1          & \textbf{73.1} \\ \hline
% \textbf{Ours(ViT)} & \textbf{88.9} & \textbf{83.3} & -              & -              & 63.5          & 70.2         \\
% \textbf{Ours(Res)} & 84.6          & 82.2          & 62.1          & \textbf{57.9} & \textbf{65.6} & 70.4         \\ \hline
% \end{tabular}
% \label{MMP}
% \end{table}

% \begin{figure}[htpb]
%     \centering
%     \includegraphics[width=\textwidth]{Figure/Results.pdf}
%     \caption{Caption}
%     \label{results}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[scale=0.4]{Figure/Ablation studies.pdf}
%     \caption{Caption}
%     \label{ablation studies}
% \end{figure}

% \begin{table}[htbp]
% \centering
% \caption{Ablation studies of our methods.REG and LOC represent the accuracy in recognizing and locating nodules, respectively.}
% \begin{tabular}{c|cc|cccccc|cc}
% \hline
% Dataset &
%   QFE &
%   MGLM &
%   \begin{tabular}[c]{@{}c@{}}BLUE\\ -1\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}BLUE\\ -2\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}BLUE\\ -3\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}BLUE\\ -4\end{tabular} &
%   METEOR &
%   \begin{tabular}[c]{@{}c@{}}ROUGE\\ -L\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}REC\\ (\%)\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}LOC\\ (\%)\end{tabular} \\ \hline
% \multirow{3}{*}{Breast}  &   &   &                &                &                &                &                &                & -           & -           \\
%                          & \checkmark &   & 0.705          & 0.671          & 0.640          & 0.614          & 0.426          & 0.758          & 91          & \textbf{85} \\
%                          & \checkmark & \checkmark & \textbf{0.734} & \textbf{0.696} & \textbf{0.663} & \textbf{0.636} & \textbf{0.441} & \textbf{0.764} & \textbf{95} & 84          \\ \hline
% \multirow{3}{*}{Thyroid} &   &   &                &                &                &                &                &                & -           & -           \\
%                          & \checkmark &   & \textbf{0.769} & \textbf{0.725} & \textbf{0.685} & \textbf{0.650} & \textbf{0.453} & \textbf{0.767} & 95          & 70          \\
%                          & \checkmark & \checkmark & 0.756          & 0.714          & 0.676          & 0.643          & 0.446          & 0.763          & \textbf{96} & \textbf{70} \\ \hline
% \end{tabular}
% \end{table}

% \begin{figure}[h]
%     \centering
%     \includegraphics[scale = 0.3]{Figure/Multimodal pretraining.pdf}
%     \caption{The performance of multimodal pretraining methods on downstream tasks. CLS, DET, and SEG represent classification, object detection, and semantic segmentation tasks, respectively. Acc denotes classification accuracy, AP denotes average precision, and DICE represents the Dice score.}
%     \label{Multimodal pretraining}
% \end{figure}
\section{Results and Analysis}

\begin{figure}[t!]
    \centering
    \includegraphics[scale=0.31]{Figure/Multimodal pretraining.pdf}
    \caption{Performance on visual recognition tasks. We compare our method with GLoRIA \cite{GLoRIA}, MGCA\cite{MGCA} and MRM \cite{MRM}.  We only report detection results on ResNet since we use YOLOv3\cite{YOLOv3} as our backbone, which is based on the convolutional neural network. Despite being pre-trained on a relatively small dataset, our method demonstrates balanced and nearly the best performance across various tasks, while other methods exhibit some shortcomings. Numerical details are in the supplementary materials.}
    \label{multimodal}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[scale=0.33]{Figure/Vision visualization.pdf}
    \caption{Examples of visual recognition tasks. Our method achieves superior performance in both classification, detection, and segmentation compared to GLoRIA.
    % Benefiting from our multi-granular VQA, our method identifies pathological features more precisely compared to GLoRIA. 
    }
    \label{vision visualization}
\end{figure}

\subsubsection{Visual recognition.}

% \subsubsection{Setting}
% To demonstrate the effectiveness of our approach on different backbones and compare it with other methods, we also present the results using ResNet50 as the VE. For ViT-B, to avoid the model converging to a trivial solution, we shorten the training epoch to 30. For ResNet50, we maintain training for 50 epochs. In the pretraining task, the coarse-grained language modeling task is no longer the primary focus of the model. Instead, the medium and fine-grained language modeling tasks will play a more significant role. Thus we set $\lambda_{c} = 1$, $\lambda_{m} = 3$, and $\lambda_f = 9$. The rest of the training configurations are the same as the report generation section.

In this section, we compare our model's performance in three visual recognition tasks with that of other SOTA multimodal pre-training methods in Figure \ref{multimodal}. We observe that most of the previous SOTA methods struggle to achieve balanced performance across various downstream tasks when pre-trained on our dataset which is relatively smaller than their pre-training dataset. For instance, MRM \cite{MRM} achieves excellent segmentation performance but performs poorly in classification. Our approach achieves competitive performance across all tasks. We believe that this is because VQA enhances the utilization efficiency of images. Additionally, our multi-granular VQA enables the model to learn to extract multi-granular features, thus benefiting various downstream tasks. Figure \ref{vision visualization} visualizes some of the examples. Compared to GLoRIA \cite{GLoRIA}, our method achieves more precise recognition and less misidentification.

\begin{table}[t!]
% \scriptsize
\centering
\caption{Performance of report generation. Our method shows the best in most cases compared to other SOTA methods. B1 to B4 represent BLEU-1 to BLEU-4, while MR and RL represent METEOR and ROUGE-L, respectively.}
\label{report generation}
\begin{tabular}{>{\centering\arraybackslash}m{1.1cm}|p{2.2cm}>{\centering\arraybackslash}m{1.3cm}>{\centering\arraybackslash}m{1.3cm}>{\centering\arraybackslash}m{1.3cm}>{\centering\arraybackslash}m{1.3cm}>{\centering\arraybackslash}m{1.4cm}>{\centering\arraybackslash}m{1.5cm}}
\hline
\multicolumn{1}{c|}{Dataset} & Method        & B1$\uparrow$          & B2$\uparrow$          & B3$\uparrow$          & B4$\uparrow$          & MR$\uparrow$          & RL$\uparrow$         \\ \hline
\multirow{5}{*}{Breast}  
                         & TriNet\cite{TriNet}  & 0.693 & 0.594 & 0.533 & 0.478 & 0.439 & 0.742 \\
                         & R2Gen\cite{R2Gen}   & 0.663 & 0.611 & 0.572 & 0.541 & 0.411 & 0.685 \\
                         & TF\cite{transformer}      & 0.699 & 0.653 & 0.619 & 0.590 & 0.437 & 0.757 \\ 
                         & R2GenRL\cite{R2GenRL}      & 0.616 & 0.528 & 0.464 & 0.414 & 0.470 & 0.599 \\
                         & DeltaNet\cite{DeltaNet}      & 0.716 & 0.661 & 0.628 & 0.598 & \textbf{0.517} & 0.758 \\ \cline{2-8} 
                             % & \textbf{QFE} & 0.728 & 0.692 & 0.660 & 0.632 & 0.437 & 0.767 \\
                             & \textbf{Ours} & \textbf{0.730} & \textbf{0.695} & \textbf{0.665} & \textbf{0.639} & 0.442 & \textbf{0.774} \\ \hline
\multirow{5}{*}{Thyroid} 
                         & TriNet\cite{TriNet}  & 0.645 & 0.510 & 0.421 & 0.345 & 0.409 & 0.678 \\
                         & R2Gen\cite{R2Gen}   & 0.578 & 0.532 & 0.492 & 0.457 & 0.369 & 0.664 \\
                         & TF\cite{transformer}      & 0.709 & 0.642 & 0.585 & 0.538 & 0.425 & 0.701 \\
                         & R2GenRL\cite{R2GenRL}      & 0.672 & 0.595 & 0.531 & 0.479 & \textbf{0.500} & 0.651 \\
                         & DeltaNet\cite{DeltaNet}& 0.610 & 0.559 & 0.515 & 0.479 & 0.443 & 0.686\\ \cline{2-8} 
                             % & \textbf{QFE} & \textbf{0.769}  & \textbf{0.726}  & \textbf{0.687}  & \textbf{0.653}  & \textbf{0.455}  & \textbf{0.760}  \\ 
                             & \textbf{Ours} & \textbf{0.755}  & \textbf{0.712}  & \textbf{0.674}  & \textbf{0.640}  & 0.444  & \textbf{0.761}  \\ \hline
\end{tabular}
\end{table}

\subsubsection{Report Generation.}
We compare our method with other SOTA report generation methods in Table \ref{report generation}. Our method achieves the best performance in most cases. Specifically, our method outperforms the suboptimal model (DeltaNet \cite{DeltaNet} and TF \cite{transformer}) by 6.856\% on the breast dataset and by 18.95\% on the thyroid dataset in BLEU-4. We assume that the benefit arises from the design of medium and fine-grained VQA which enhances the precision of nodule recognition, and the QFT module contributes to generating more accurate reports. Additionally, we observe that our method does not achieve the highest METEOR score, which is more related to the correctness of token order. However, we believe that the order for describing different anatomical regions in the medical report is less crucial. Our method pays more attention to pathological features rather than the order of each region, resulting in higher BLEU and ROUGE-L scores.

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=\textwidth]{Figure/Report+ablation.pdf}
%     \caption{(a)Ablation studies of RG. (b)Ablation studies of MMP. (c)Examples of RG.}
%     \label{report+ablation} 
% \end{figure}

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=\textwidth]{Figure/Generated report.pdf}
%     \caption{Examples of report generation. R2Gen and DeltaNet only recognize nodules on one side of the thyroid, while our method accurately identifies nodules on both sides.}
%     \label{RG example} 
% \end{figure}

\begin{table}[t!]
% \scriptsize
\caption{Results of ablation studies. P and R denote precision and recall for nodule recognition in reports. The introduction of QFT improves the quality of report generation, while the addition of VQA reduces the misidentification rate of nodules. }
\centering
\begin{tabular}{>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{0.6cm}>{\centering\arraybackslash}m{0.5cm}>{\centering\arraybackslash}m{0.7cm}|>{\centering\arraybackslash}m{1.1cm}>{\centering\arraybackslash}m{1.1cm}>{\centering\arraybackslash}m{1.1cm}>{\centering\arraybackslash}m{1.1cm}>{\centering\arraybackslash}m{1.3cm}>{\centering\arraybackslash}m{1.2cm}>{\centering\arraybackslash}m{0.6cm}>{\centering\arraybackslash}m{0.6cm}}
\hline
\begin{tabular}[c]{@{}c@{}}Multi\\Images\end{tabular} &QCL &CL &VQA &
  \begin{tabular}[c]{@{}c@{}}B1$\uparrow$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}B2$\uparrow$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}B3$\uparrow$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}B4$\uparrow$\end{tabular} &MR$\uparrow$ &
  \begin{tabular}[c]{@{}c@{}}RL$\uparrow$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}P$\uparrow$\\ (\%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}R$\uparrow$\\ (\%)\end{tabular} \\ \hline
  &   &   &   & 0.652          & 0.619          & 0.591          & 0.568          & 0.396          & 0.717          & -             & -            \\
\checkmark &   &   &   & 0.703          & 0.662          & 0.627          & 0.598          & 0.416          & 0.743          & -             & -   \\
\checkmark & \checkmark &   &   & 0.678          & 0.641          & 0.609          & 0.582          & 0.407          & 0.722          & -             & -            \\
\checkmark & \checkmark & \checkmark &   & 0.728          & 0.692          & 0.660          & 0.632          & 0.437          & 0.767          & 85.0            & 94.0           \\
\checkmark & \checkmark & \checkmark & \checkmark & \textbf{0.730} & \textbf{0.695} & \textbf{0.665} & \textbf{0.639} & \textbf{0.442} & \textbf{0.774} & \textbf{87.5} & \textbf{100} \\ \hline
\end{tabular}
\label{RG ablation}
\end{table}

\subsubsection{Ablation Study.}
In this section, we show the ablation study of our framework on the breast report generation task in Table \ref{RG ablation}. Firstly, although only using the most relevant image is a common approach in previous research \cite{CNN-LSTM,R2Gen,TriNet}, we observe that using multiple images as input improves the quality of the generated reports (0.717 vs 0.743). As mentioned in Sec. \ref{QFT section}, adding QCL results in a trivial solution (0.722), while the addition of CL leads to a notable increase (0.767), demonstrating that the constraint of CL can effectively reduce the loss of image information during QCL. The addition of VQA further improves the performance (0.767 vs 0.774). Besides, we conduct further analysis to evaluate the impact of VQA: we randomly sample 100 reports and calculate the precision and recall for nodule recognition. Specifically, we regard it as a binary classification task, where reports mentioning the presence of nodules are labeled as 1, and otherwise as 0. We observe a significant improvement (85.0\% vs 87.5\% on precision, 94.0\% vs 100\% on recall). This is crucial in the medical field as it can reduce misdiagnosis. 

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% \begin{table}[]
% \centering
% \caption{The performance of multimodal pretraining methods on downstream tasks. CLS, DET, and SEG represent classification, object detection, and semantic segmentation tasks, respectively. IMP represents the improvement relative to ImageNet pretraining. Acc denotes classification accuracy, AP denotes average precision, and DICE represents the Dice score.}
% \begin{tabular}{lccccccc}
% \hline
%  &
%   \multicolumn{2}{c}{\ \ \ CLS(AUC\%)\ \ \ } &
%   \multicolumn{2}{c}{\ \ \ DET(AP\%)\ \ \ } &
%   \multicolumn{2}{c}{\ \ \ SEG(DICE\%)\ \ \ } &
%    \\
% \multirow{-2}{*}{Method} &
%   BUSI &
%   AUITD &
%   BUSI &
%   DDTI &
%   BUSI &
%   DDTI &
%   \multirow{-2}{*}{\ \ \ IMP(\%)\ \ \ } \\ \hline
% Random(ViT)   & 56.41 & 81.34 & -              & -     & 38.12 & 58.13 & {\color[HTML]{FE0000} 20.11↓} \\
% Random(Res)   & 61.54 & 81.34 & 51.48          & 13.86 & 58.02 & 64.74 & {\color[HTML]{FE0000} 16.27↓} \\
% ImageNet(ViT) & 84.52 & 82.45 & -              & -     & 63.92 & 61.50 & 0                             \\
% ImageNet(Res) & 82.91 & 82.17 & \textbf{66.74} & 50.00 & 49.06 & 61.06 & 0                             \\
% GloRIA(Res)   & 85.47 & 80.22 & 54.86          & 21.09 & 63.69 & 63.84 & {\color[HTML]{FE0000} 6.76↓}  \\
% MGCA(ViT)     & 82.91 & 80.22 & -              & -     & 61.15 & 68.66 & {\color[HTML]{32CB00} 0.67↑}  \\
% MGCA(Res)     & 82.91 & 82.17 & 55.47          & 10.51 & 59.21 & 68.80 & {\color[HTML]{FE0000} 10.42↓} \\ \hline
% \textbf{Ours(ViT)} &
%   \textbf{88.89} &
%   \textbf{83.29} &
%   - &
%   - &
%   63.53 &
%   70.24 &
%   {\color[HTML]{32CB00} \textbf{4.99↑}} \\
% \textbf{Ours(Res)} &
%   84.62 &
%   82.17 &
%   62.08 &
%   \textbf{57.93} &
%   \textbf{65.57} &
%   \textbf{70.35} &
%   {\color[HTML]{32CB00} \textbf{9.97↑}} \\ \hline
% \end{tabular}
% \end{table}

% \begin{table}[ht]
% \centering
% \caption{Ablation studies of multimodal pretraining.}
% \begin{tabular}{cccc|cc}
% \hline
% CL &
%   \begin{tabular}[c]{@{}c@{}}MGLM\\ (c)\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}MGLM\\ (m)\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}MGLM\\ (f)\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}CLS\\ (AUC\%)\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}SEG\\ (DICE\%)\end{tabular} \\ \hline
% \checkmark &   &   &   & 86.32          & 67.64          \\
% \checkmark & \checkmark &   &   & 84.61          & 71.59          \\
% \checkmark & \checkmark & \checkmark &   & 84.62          & \textbf{72.76} \\
% \checkmark & \checkmark & \checkmark & \checkmark & \textbf{88.89} & 70.24          \\ \hline
% \end{tabular}
% \end{table}

% We show VQA ablation on BUSI CLS and DDTI SEG with ViT base in Figure\ref{report+ablation}(b), corresponding to global and local understanding, respectively. We observe that incorporating coarse and medium-grained VQA could enhance SEG capacity(67.6 vs 71.6 vs 72.8) but lead to a decline in CLS(86.3 vs 84.6 vs 84.6). Fine-grained VQA improves CLS(88.9) but reduces SEG performance(70.2). We hypothesize there is a trade-off between extracting global and local features. All three granularities of VQA enhance VE's understanding of local features, leading to performance improvement in SEG. However, the performance gains in SEG by incorporating coarse and medium-grained VQA come at the cost of a decline in CLS. On the other hand, the nodule recognition tasks in fine-grained VQA are essentially a binary CLS, which means that the model has faced CLS tasks during the pretraining, so the addition of fine-grained tasks enhances CLS performance but comes with a decline in SEG performance.

\section{Conclusion}
In this paper, we take the lead in exploring the potential of VQA in pre-training. We design different levels VQA targeting vital pathological features according to the description in the medical report without any extra annotations from clinicians. We also propose a pre-training framework with QFT, a module used to narrow the vision-language gap with a contrastive learning strategy. We demonstrate the effectiveness of our approach in report generation and three visual recognition tasks. Experimental results indicate that VQA guides the model focusing on desired pathological features, demonstrating the potential of VQA in pre-training. This work is an initial exploration. We will further investigate how to design reasonable questions and how to efficiently utilize VQA in pre-training. 

\subsubsection{Acknowledgement.} Thanks for anonymous organization, and **** number.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{refs.bib}

% \ \ \ \ \textbf{BUSI} Breast UltraSound Image dataset consists of a total of 780 images with an average size of 500×500 pixels. These images are classified into three categories: normal, benign, and malignant. For each image in the dataset, a corresponding ground truth mask is available, offering pixel-level annotationss for the segmentation task. This dataset is utilized for all three downstream tasks. For object detection, we utilize the provided ground truth masks to create bounding box annotationss. More specifically, we apply the circumscribed rectangle of the masked region in the ground truth mask as the bounding box.

% \textbf{AUITD} Algerian Ultrasound Images Thyroid Dataset contains 3538 thyroid images. These images are categorized into three groups: normal, benign, and malignant. In our study, we specifically utilize this dataset for the classification task.

% \textbf{DDTI} Digital Database Thyroid Image dataset includes 134 ultrasound images along with their respective masks. In our experiment, we preprocess this dataset using the method introduced in previous work []. We employ this dataset for both object detection and segmentation tasks.

% \section*{B. SOTA Method in Report Generation and Multimodal Pretraining}

% \subsection*{B.1. Report Generation}
% \ \ \ \ \textbf{CNN-RNN} This approach initially employs a CNN model to extract visual features from ultrasound images. It then applies hierarchical LSTM decoding to generate reports. Since we don't have the classification data mentioned in that method, we remove the classification branch and train the model only for the report generation task.

% \textbf{TriNet} In this approach, two branches are designed to align visual and textual features. It's worth noticing that one branch in the original method requires medical subject headings, which are lacking in our dataset. Consequently, we remove this branch from our comparison.

% \textbf{R2Gen} This method introduces a memory-driven unit to integrate memory into the transformer, aiming at improving the performance of radiology report generation.

% \textbf{TF} This method follows the conventional Transformer encoder-decoder framework. Visual features are extracted from the image using a CNN and subsequently fed into the Transformer to generate textual reports.

% \subsection*{B.2. Multimodal Pretraining}
% \ \ \ \ \textbf{GLoRIA} This approach builds upon ConVIRT's global feature alignment and extends it with local feature alignment: utilizing weighted visual local features to reconstruct textual local.

% \textbf{MGCA} This method extends upon GLoRIA by reconstructing visual local features using textual local features. Furthermore, researchers use the clustering result of one modality as classification labels for the other modality.

% \section*{C. Visualization of Report Generation}
% \subsection*{C.1. Comparision with Other SOTA Methods}
% Fig[] compares the reports generated by our model with other SOTA methods. It can be seen that the reports generated by R2Gen basically do not understand the content of the image, resulting in many errors. The reports generated by TriNet demonstrate a better understanding of the images but suffer from language issues. The reports generated by QFE+MGLM are closer to the ground truth and accurately locate the position of nodules. However, an existing drawback is that our method still has a limited understanding of CDFI.

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{Figure/Generated report.pdf}
%     \caption{Comparision in report generation.}
%     \label{Generated report}
% \end{figure}

% \subsection*{Example of Multi-granularity Language Modeling}
% \section{D. Visualization of Multimodal Pretraining}
% Figure[] visualizes the performance differences between our model and MGCA in downstream tasks. It is evident that our model predicts more accurately in segmentation and object detection. In classification, our model's heatmap accurately identifies the lesion location, and our ROC curve completely envelops MGCA's ROC curve.

\end{document}
